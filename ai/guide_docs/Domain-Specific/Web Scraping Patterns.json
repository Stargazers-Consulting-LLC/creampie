{
  "ai_metadata": {
    "purpose": "",
    "last_updated": "",
    "template_version": "2.1",
    "ai_tool_compatibility": "",
    "ai_processing_level": "High",
    "required_context": "Web scraping, HTML parsing, async programming, data processing",
    "validation_required": "Yes",
    "code_generation": "Supported",
    "cross_references": [
      "../Core%20Principles.json",
      "../Language-Specific/Python%20Style%20Guide.json",
      "../Language-Specific/FastAPI%20Development%20Guide.json",
      "../../project_context/Common%20Patterns.json",
      "../../features/summaries/[COMPLETED]-stock_data_processing_pipeline_summary.json"
    ],
    "maintenance": ""
  },
  "file_info": {
    "file_path": "guide_docs/Domain-Specific/Web Scraping Patterns.md",
    "original_format": "markdown",
    "converted_at": "2025-06-18T19:14:30.261386",
    "file_size": 26299,
    "line_count": 720,
    "optimized_at": "2025-06-18T19:19:47.751591",
    "optimization_version": "1.0"
  },
  "content": {
    "sections": [
      {
        "level": 1,
        "title": "Web Scraping Patterns",
        "content": "> This guide provides comprehensive patterns and best practices for web scraping and data extraction. Use these patterns to create reliable, efficient web scraping solutions.",
        "subsections": []
      },
      {
        "level": 2,
        "title": "AI Metadata",
        "content": "**Template Version:** 2.1\n**AI Processing Level:** High\n**Required Context:** Web scraping, HTML parsing, async programming, data processing\n**Validation Required:** Yes\n**Code Generation:** Supported\n\n**Dependencies:**\n- `../Core%20Principles.json.replace(\".json\", \".json\")` - Decision-making frameworks\n- `../Language-Specific/Python%20Style%20Guide.json.replace(\".json\", \".json\")` - Python implementation patterns\n- `../Language-Specific/FastAPI%20Development%20Guide.json.replace(\".json\", \".json\")` - Background task patterns\n- `../../project_context/Common%20Patterns.json.replace(\".json\", \".json\")` - Project-specific patterns\n- `../../features/summaries/[COMPLETED]-stock_data_processing_pipeline_summary.json.replace(\".json\", \".json\")` - Stock data implementation\n\n**Validation Rules:**\n- All HTML parsing must use semantic selectors and proper error handling\n- Background tasks must include proper lifecycle management\n- Data ordering must be explicit and consistent\n- Error handling must distinguish between critical and recoverable errors\n- All selectors must be documented with reasoning",
        "subsections": []
      },
      {
        "level": 2,
        "title": "Overview",
        "content": "**Document Purpose:** Web scraping and data processing patterns specific to the CreamPie project\n**Scope:** HTML parsing, background tasks, data ordering, error handling, and file processing\n**Target Users:** AI assistants and developers implementing web scraping functionality\n**Last Updated:** Current\n\n**AI Context:** This guide provides the foundational patterns for all web scraping and data processing in the project. It ensures robust, maintainable, and error-resistant scraping implementations.",
        "subsections": []
      },
      {
        "level": 2,
        "title": "1. HTML Structure Handling",
        "content": "",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Best Practices",
        "content": "- **Use proper semantic HTML elements**: Leverage `<thead>`, `<tbody>` when parsing tables\n- **Write robust parsers**: Handle standard HTML table structures correctly\n- **Avoid assumptions about structure**: Don't assume HTML will always have a specific format\n- **Use semantic selectors**: Prefer direct, semantic selectors (e.g., `.table`) over complex ones\n\nThese practices will inform all HTML parsing implementation throughout the project.\n\nAll HTML parsing must follow these practices and include proper error handling.",
        "subsections": []
      },
      {
        "level": 3,
        "title": "CSS Selector Design",
        "content": "- **Keep selectors simple and specific**: Avoid overly complex selectors that are fragile\n- **Use direct, semantic selectors**: Prefer `.table` over `.gridLayout > div:nth-child(2)`\n- **Make selectors resilient**: Choose selectors that are less likely to break with HTML changes\n- **Document selector assumptions**: Explain why specific selectors were chosen\n\nThis selector strategy will inform all CSS selector design and documentation.\n\nAll selectors must be documented with reasoning and tested for resilience.",
        "subsections": []
      },
      {
        "level": 3,
        "title": "HTML Parsing Patterns",
        "content": "```python\nfrom bs4 import BeautifulSoup\nimport logging\nfrom typing import List, Dict, Optional\n\nlogger = logging.getLogger(__name__)\n\ndef parse_stock_table(html_content: str) -> List[Dict[str, str]]:\n    \"\"\"Parse stock data from HTML table.\n\n    Args:\n        html_content: Raw HTML content to parse\n\n    Returns:\n        List of dictionaries containing parsed stock data\n\n    Raises:\n        ValueError: If HTML structure is invalid\n        Exception: For other parsing errors\n    \"\"\"\n    try:\n        soup = BeautifulSoup(html_content, 'html.parser')\n\n        # Use semantic selectors - prefer direct table selection\n        table = soup.find('table', class_='stock-table')\n        if not table:\n            logger.warning(\"Stock table not found in HTML\")\n            return []\n\n        # Parse table structure with proper error handling\n        rows = table.find_all('tr')\n        if len(rows) < 2:  # Need header + at least one data row\n            logger.warning(\"Insufficient table rows found\")\n            return []\n\n        # Extract headers with validation\n        header_row = rows[0]\n        headers = []\n        for th in header_row.find_all('th'):\n            header_text = th.get_text(strip=True)\n            if header_text:  # Only include non-empty headers\n                headers.append(header_text)\n\n        if not headers:\n            logger.error(\"No valid headers found in table\")\n            return []\n\n        # Parse data rows with validation\n        data = []\n        for row in rows[1:]:\n            cells = row.find_all('td')\n            if len(cells) == len(headers):\n                row_data = {}\n                for header, cell in zip(headers, cells):\n                    cell_text = cell.get_text(strip=True)\n                    row_data[header] = cell_text\n                data.append(row_data)\n            else:\n                logger.warning(f\"Row has {len(cells)} cells, expected {len(headers)}\")\n\n        logger.info(f\"Successfully parsed {len(data)} rows from stock table\")\n        return data\n\n    except Exception as e:\n        logger.error(f\"Error parsing HTML: {e}\")\n        raise ValueError(f\"Failed to parse HTML table: {e}\")\n```\n\nThis parsing pattern will inform all HTML table parsing implementation with proper error handling and logging.\n\nAll HTML parsing must include comprehensive error handling, logging, and data validation.",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Robust Selector Patterns",
        "content": "```python\ndef find_table_with_fallback(soup: BeautifulSoup, selectors: List[str]) -> Optional[BeautifulSoup]:\n    \"\"\"Find table using multiple selector fallbacks.\n\n    Args:\n        soup: BeautifulSoup object\n        selectors: List of CSS selectors to try in order\n\n    Returns:\n        Table element if found, None otherwise\n    \"\"\"\n    for selector in selectors:\n        table = soup.select_one(selector)\n        if table:\n            logger.info(f\"Found table using selector: {selector}\")\n            return table\n\n    logger.warning(f\"No table found with any selector: {selectors}\")\n    return None",
        "subsections": []
      },
      {
        "level": 1,
        "title": "Usage example",
        "content": "def parse_stock_data_robust(html_content: str) -> List[Dict[str, str]]:\n    \"\"\"Parse stock data with multiple selector fallbacks.\"\"\"\n    soup = BeautifulSoup(html_content, 'html.parser')\n\n    # Try multiple selectors in order of preference\n    selectors = [\n        'table.stock-table',\n        'table.data-table',\n        'table',\n        '.stock-data table'\n    ]\n\n    table = find_table_with_fallback(soup, selectors)\n    if not table:\n        return []\n\n    # Continue with parsing logic...\n    return parse_table_data(table)\n```\n\nThis fallback pattern will inform robust selector implementation for handling HTML structure variations.\n\nAll selector implementations must include fallback strategies and proper logging.",
        "subsections": []
      },
      {
        "level": 2,
        "title": "2. Background Task Integration",
        "content": "",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Task Organization",
        "content": "- **Keep tasks independent and focused**: Separate different operations into independent tasks\n- **Use single responsibility principle**: Each task should have one clear purpose\n- **Implement proper task lifecycle**: Create tasks with clear start/stop conditions\n\nThis task organization will inform all background task implementation and lifecycle management.\n\nAll background tasks must follow single responsibility principle and include proper lifecycle management.",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Task Implementation Patterns",
        "content": "```python\nimport asyncio\nimport logging\nfrom typing import Optional\nfrom datetime import datetime\n\nlogger = logging.getLogger(__name__)\n\nclass TaskLifecycleManager:\n    \"\"\"Manages background task lifecycle and error handling.\"\"\"\n\n    def __init__(self, task_name: str):\n        self.task_name = task_name\n        self.is_running = False\n        self.last_run = None\n        self.error_count = 0\n        self.max_errors = 3\n\n    async def run_with_lifecycle(self, task_func, *args, **kwargs):\n        \"\"\"Run task with proper lifecycle management.\"\"\"\n        self.is_running = True\n        self.last_run = datetime.utcnow()\n\n        try:\n            logger.info(f\"Starting {self.task_name}\")\n            await task_func(*args, **kwargs)\n            self.error_count = 0  # Reset error count on success\n            logger.info(f\"Completed {self.task_name}\")\n\n        except Exception as e:\n            self.error_count += 1\n            logger.error(f\"Error in {self.task_name}: {e}\")\n\n            if self.error_count >= self.max_errors:\n                logger.critical(f\"Too many errors in {self.task_name}, stopping task\")\n                self.is_running = False\n                return False\n\n        return True",
        "subsections": []
      },
      {
        "level": 1,
        "title": "Good - independent, focused tasks with lifecycle management",
        "content": "async def run_periodic_updates() -> None:\n    \"\"\"Run periodic updates of tracked stocks.\"\"\"\n    lifecycle_manager = TaskLifecycleManager(\"stock_updates\")\n\n    while lifecycle_manager.is_running:\n        try:\n            success = await lifecycle_manager.run_with_lifecycle(update_all_tracked_stocks)\n            if not success:\n                break\n\n        except Exception as e:\n            logger.error(f\"Critical error in stock updates: {e}\")\n            break\n\n        await asyncio.sleep(RETRIEVAL_INTERVAL_SECONDS)\n\nasync def run_periodic_file_processing() -> None:\n    \"\"\"Run periodic processing of raw HTML files.\"\"\"\n    lifecycle_manager = TaskLifecycleManager(\"file_processing\")\n\n    while lifecycle_manager.is_running:\n        try:\n            success = await lifecycle_manager.run_with_lifecycle(process_raw_files_task)\n            if not success:\n                break\n\n        except RuntimeError as e:\n            logger.critical(f\"Critical error in file processing: {e}\")\n            break\n        except Exception as e:\n            logger.error(f\"Error in file processing: {e}\")\n            # Continue on non-critical errors\n\n        await asyncio.sleep(PROCESSING_INTERVAL_SECONDS)\n```\n\nThis lifecycle management pattern will inform all background task implementation with proper error handling and monitoring.\n\nAll background tasks must include lifecycle management and proper error handling strategies.",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Error Handling Strategy",
        "content": "- **Distinguish error types**: Use different exception types for different error categories\n- **Implement appropriate responses**: Handle critical errors (stop task) and recoverable errors (continue) differently\n- **Use proper logging levels**: Log critical errors with `logger.critical()` and regular errors with `logger.error()`\n- **Provide clear error context**: Include relevant information in error messages\n\nThis error handling strategy will inform all background task error handling implementation.\n\nAll background tasks must implement proper error categorization and appropriate responses.",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Task Health Monitoring",
        "content": "```python\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nfrom typing import Dict, Any\n\n@dataclass\nclass TaskHealth:\n    \"\"\"Task health monitoring data.\"\"\"\n    task_name: str\n    is_running: bool\n    last_run: Optional[datetime]\n    error_count: int\n    success_count: int\n    avg_duration: float\n\n    def is_healthy(self) -> bool:\n        \"\"\"Check if task is healthy.\"\"\"\n        if not self.is_running:\n            return False\n\n        # Check if task has run recently (within expected interval)\n        if self.last_run and datetime.utcnow() - self.last_run > timedelta(minutes=30):\n            return False\n\n        # Check error rate\n        total_runs = self.success_count + self.error_count\n        if total_runs > 0 and self.error_count / total_runs > 0.5:\n            return False\n\n        return True\n\nclass TaskMonitor:\n    \"\"\"Monitor background task health.\"\"\"\n\n    def __init__(self):\n        self.tasks: Dict[str, TaskHealth] = {}\n\n    def register_task(self, task_name: str) -> TaskHealth:\n        \"\"\"Register a new task for monitoring.\"\"\"\n        health = TaskHealth(\n            task_name=task_name,\n            is_running=True,\n            last_run=None,\n            error_count=0,\n            success_count=0,\n            avg_duration=0.0\n        )\n        self.tasks[task_name] = health\n        return health\n\n    def update_task_health(self, task_name: str, success: bool, duration: float):\n        \"\"\"Update task health metrics.\"\"\"\n        if task_name not in self.tasks:\n            return\n\n        health = self.tasks[task_name]\n        health.last_run = datetime.utcnow()\n\n        if success:\n            health.success_count += 1\n        else:\n            health.error_count += 1\n\n        # Update average duration\n        total_runs = health.success_count + health.error_count\n        health.avg_duration = (health.avg_duration * (total_runs - 1) + duration) / total_runs\n\n    def get_unhealthy_tasks(self) -> List[str]:\n        \"\"\"Get list of unhealthy tasks.\"\"\"\n        return [name for name, health in self.tasks.items() if not health.is_healthy()]\n```\n\nThis monitoring pattern will inform all background task health monitoring implementation.\n\nAll background tasks must include health monitoring and metrics collection.",
        "subsections": []
      },
      {
        "level": 2,
        "title": "3. Data Ordering and Time Series",
        "content": "",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Best Practices",
        "content": "- **Make data ordering explicit**: When dealing with time-series data, always consider the order\n- **Add explicit sorting**: Use `sort_values(\"date\")` to ensure consistent data order\n- **Document ordering requirements**: Explain why specific ordering is important\n- **Test ordering edge cases**: Verify behavior with different data orderings\n\nThese practices will inform all time series data processing implementation.\n\nAll time series data must include explicit ordering and proper validation.",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Time Series Data Patterns",
        "content": "```python\nimport pandas as pd\nfrom datetime import datetime\nfrom typing import List, Dict, Any\n\ndef process_time_series_data(data: List[Dict[str, Any]]) -> pd.DataFrame:\n    \"\"\"Process time series data with proper ordering and validation.\n\n    Args:\n        data: List of dictionaries containing time series data\n\n    Returns:\n        Processed DataFrame with proper time ordering\n\n    Raises:\n        ValueError: If data is invalid or missing required fields\n    \"\"\"\n    if not data:\n        raise ValueError(\"No data provided for processing\")\n\n    df = pd.DataFrame(data)\n\n    # Validate required columns\n    required_columns = ['date', 'symbol']\n    missing_columns = [col for col in required_columns if col not in df.columns]\n    if missing_columns:\n        raise ValueError(f\"Missing required columns: {missing_columns}\")\n\n    # Ensure date column exists and is properly formatted\n    try:\n        df['date'] = pd.to_datetime(df['date'])\n    except Exception as e:\n        raise ValueError(f\"Invalid date format: {e}\")\n\n    # Validate date range\n    current_time = pd.Timestamp.now()\n    future_dates = df[df['date'] > current_time]\n    if not future_dates.empty:\n        logger.warning(f\"Found {len(future_dates)} records with future dates\")\n        df = df[df['date'] <= current_time]\n\n    # Sort by date to ensure chronological order\n    df = df.sort_values('date')\n\n    # Reset index after sorting\n    df = df.reset_index(drop=True)\n\n    # Validate data integrity\n    if df.empty:\n        raise ValueError(\"No valid data after processing\")\n\n    # Check for duplicate date-symbol combinations\n    duplicates = df.duplicated(subset=['date', 'symbol']).sum()\n    if duplicates > 0:\n        logger.warning(f\"Found {duplicates} duplicate date-symbol combinations\")\n        df = df.drop_duplicates(subset=['date', 'symbol'], keep='last')\n\n    logger.info(f\"Processed {len(df)} time series records\")\n    return df\n```\n\nThis time series pattern will inform all time series data processing with proper validation and ordering.\n\nAll time series processing must include date validation, ordering, and duplicate handling.",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Data Validation Patterns",
        "content": "```python\nfrom pydantic import BaseModel, validator, Field\nfrom datetime import datetime\nfrom typing import List, Optional\nfrom enum import Enum\n\nclass DataQuality(Enum):\n    \"\"\"Data quality levels.\"\"\"\n    HIGH = \"high\"\n    MEDIUM = \"medium\"\n    LOW = \"low\"\n\nclass StockDataPoint(BaseModel):\n    \"\"\"Validated stock data point.\"\"\"\n    symbol: str = Field(..., min_length=1, max_length=10, regex=r'^[A-Z]+$')\n    date: datetime\n    price: float = Field(..., gt=0)\n    volume: Optional[int] = Field(None, ge=0)\n    quality: DataQuality = DataQuality.MEDIUM\n\n    @validator('price')\n    def validate_price(cls, v):\n        if v <= 0:\n            raise ValueError('Price must be positive')\n        if v > 1000000:  # Reasonable upper limit\n            raise ValueError('Price seems unreasonably high')\n        return v\n\n    @validator('date')\n    def validate_date(cls, v):\n        if v > datetime.now():\n            raise ValueError('Date cannot be in the future')\n        if v < datetime(1990, 1, 1):  # Reasonable lower limit\n            raise ValueError('Date seems unreasonably old')\n        return v\n\n    @validator('symbol')\n    def validate_symbol(cls, v):\n        return v.upper()\n\nclass StockDataValidator:\n    \"\"\"Validate stock data quality and consistency.\"\"\"\n\n    def __init__(self):\n        self.validation_errors = []\n\n    def validate_dataset(self, data: List[Dict[str, Any]]) -> List[StockDataPoint]:\n        \"\"\"Validate entire dataset.\"\"\"\n        validated_data = []\n\n        for i, item in enumerate(data):\n            try:\n                validated_item = StockDataPoint(**item)\n                validated_data.append(validated_item)\n            except Exception as e:\n                self.validation_errors.append(f\"Row {i}: {e}\")\n                logger.warning(f\"Validation error in row {i}: {e}\")\n\n        if self.validation_errors:\n            logger.warning(f\"Found {len(self.validation_errors)} validation errors\")\n\n        return validated_data\n\n    def get_validation_report(self) -> Dict[str, Any]:\n        \"\"\"Get validation report.\"\"\"\n        return {\n            \"total_errors\": len(self.validation_errors),\n            \"errors\": self.validation_errors,\n            \"error_rate\": len(self.validation_errors) / max(len(self.validation_errors), 1)\n        }\n```\n\nThis validation pattern will inform all data validation implementation with comprehensive error reporting.\n\nAll data validation must include comprehensive error reporting and quality assessment.",
        "subsections": []
      },
      {
        "level": 2,
        "title": "4. File Processing Patterns",
        "content": "",
        "subsections": []
      },
      {
        "level": 3,
        "title": "File Organization",
        "content": "```python\nimport os\nfrom pathlib import Path\nfrom datetime import datetime\nfrom typing import List, Optional\n\nclass FileProcessor:\n    \"\"\"Manages file processing with proper organization and error handling.\"\"\"\n\n    def __init__(self, base_path: str):\n        self.base_path = Path(base_path)\n        self.raw_path = self.base_path / \"raw_responses\"\n        self.parsed_path = self.base_path / \"parsed_responses\"\n        self.processed_path = self.base_path / \"processed_data\"\n\n        # Ensure directories exist\n        for path in [self.raw_path, self.parsed_path, self.processed_path]:\n            path.mkdir(parents=True, exist_ok=True)\n\n    def get_raw_files(self, pattern: str = \"*.html\") -> List[Path]:\n        \"\"\"Get list of raw files to process.\"\"\"\n        return list(self.raw_path.glob(pattern))\n\n    def move_to_processed(self, file_path: Path, success: bool = True) -> None:\n        \"\"\"Move file to processed directory with status indication.\"\"\"\n        if not file_path.exists():\n            logger.warning(f\"File not found: {file_path}\")\n            return\n\n        # Create processed subdirectory based on success\n        status_dir = \"success\" if success else \"failed\"\n        processed_dir = self.processed_path / status_dir\n        processed_dir.mkdir(exist_ok=True)\n\n        # Move file with timestamp\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        new_name = f\"{file_path.stem}_{timestamp}{file_path.suffix}\"\n        new_path = processed_dir / new_name\n\n        try:\n            file_path.rename(new_path)\n            logger.info(f\"Moved {file_path} to {new_path}\")\n        except Exception as e:\n            logger.error(f\"Failed to move {file_path}: {e}\")\n\n    def save_parsed_data(self, data: List[Dict[str, Any]], filename: str) -> None:\n        \"\"\"Save parsed data to file.\"\"\"\n        import json\n\n        file_path = self.parsed_path / f\"{filename}.json\"\n        try:\n            with open(file_path, 'w') as f:\n                json.dump(data, f, indent=2, default=str)\n            logger.info(f\"Saved parsed data to {file_path}\")\n        except Exception as e:\n            logger.error(f\"Failed to save parsed data: {e}\")\n```\n\nThis file processing pattern will inform all file organization and processing implementation.\n\nAll file processing must include proper organization, error handling, and status tracking.",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Batch Processing",
        "content": "```python\nimport asyncio\nfrom typing import List, Callable, Any\n\nclass BatchProcessor:\n    \"\"\"Process files in batches with proper error handling.\"\"\"\n\n    def __init__(self, batch_size: int = 10, max_workers: int = 5):\n        self.batch_size = batch_size\n        self.max_workers = max_workers\n        self.processed_count = 0\n        self.error_count = 0\n\n    async def process_batch(\n        self,\n        files: List[Path],\n        processor_func: Callable[[Path], Any]\n    ) -> Dict[str, Any]:\n        \"\"\"Process a batch of files.\"\"\"\n        results = {\n            \"processed\": 0,\n            \"errors\": 0,\n            \"errors_details\": []\n        }\n\n        # Process files with semaphore to limit concurrency\n        semaphore = asyncio.Semaphore(self.max_workers)\n\n        async def process_single_file(file_path: Path):\n            async with semaphore:\n                try:\n                    await processor_func(file_path)\n                    results[\"processed\"] += 1\n                    self.processed_count += 1\n                except Exception as e:\n                    results[\"errors\"] += 1\n                    results[\"errors_details\"].append(f\"{file_path}: {e}\")\n                    self.error_count += 1\n                    logger.error(f\"Error processing {file_path}: {e}\")\n\n        # Create tasks for all files\n        tasks = [process_single_file(file) for file in files]\n\n        # Execute tasks\n        await asyncio.gather(*tasks, return_exceptions=True)\n\n        return results\n\n    async def process_all_files(\n        self,\n        file_list: List[Path],\n        processor_func: Callable[[Path], Any]\n    ) -> Dict[str, Any]:\n        \"\"\"Process all files in batches.\"\"\"\n        total_files = len(file_list)\n        logger.info(f\"Starting batch processing of {total_files} files\")\n\n        all_results = {\n            \"total_files\": total_files,\n            \"total_processed\": 0,\n            \"total_errors\": 0,\n            \"batch_results\": []\n        }\n\n        # Process in batches\n        for i in range(0, total_files, self.batch_size):\n            batch = file_list[i:i + self.batch_size]\n            batch_num = i // self.batch_size + 1\n\n            logger.info(f\"Processing batch {batch_num} ({len(batch)} files)\")\n\n            batch_results = await self.process_batch(batch, processor_func)\n            all_results[\"batch_results\"].append(batch_results)\n            all_results[\"total_processed\"] += batch_results[\"processed\"]\n            all_results[\"total_errors\"] += batch_results[\"errors\"]\n\n        logger.info(f\"Batch processing completed: {all_results['total_processed']} processed, {all_results['total_errors']} errors\")\n        return all_results\n```\n\nThis batch processing pattern will inform all batch file processing implementation with proper concurrency control.\n\nAll batch processing must include proper concurrency control, error handling, and progress tracking.",
        "subsections": []
      },
      {
        "level": 2,
        "title": "Implementation Guidelines",
        "content": "",
        "subsections": []
      },
      {
        "level": 3,
        "title": "For AI Assistants",
        "content": "1. **Follow these patterns** for all web scraping implementation\n2. **Use semantic selectors** and document selector reasoning\n3. **Implement proper error handling** with error categorization\n4. **Include comprehensive logging** at all stages\n5. **Validate data quality** and handle edge cases\n6. **Use background tasks** for long-running operations\n7. **Implement proper file organization** and status tracking\n8. **Test with various HTML structures** and error conditions",
        "subsections": []
      },
      {
        "level": 3,
        "title": "For Human Developers",
        "content": "1. **Reference these patterns** when implementing web scraping\n2. **Use robust selectors** that can handle HTML changes\n3. **Implement comprehensive error handling** and logging\n4. **Validate data quality** before processing\n5. **Use background tasks** for performance\n6. **Organize files properly** with clear status tracking\n7. **Test thoroughly** with various scenarios",
        "subsections": []
      },
      {
        "level": 2,
        "title": "Quality Assurance",
        "content": "",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Scraping Quality Standards",
        "content": "- All selectors must be documented with reasoning\n- Error handling must be comprehensive and categorized\n- Data validation must include quality assessment\n- File processing must include proper organization",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Performance Standards",
        "content": "- Batch processing must use appropriate concurrency limits\n- Background tasks must not block main application flow\n- File operations must be efficient and non-blocking\n- Memory usage must be optimized for large datasets",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Reliability Standards",
        "content": "- Scraping must handle HTML structure changes gracefully\n- Error recovery must be implemented for transient failures\n- Data integrity must be maintained throughout processing\n- File status must be properly tracked and reported",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Monitoring Standards",
        "content": "- Task health must be monitored continuously\n- Error rates must be tracked and reported\n- Performance metrics must be collected\n- Data quality metrics must be assessed\n\n---\n\n**AI Quality Checklist**: Before implementing web scraping functionality, ensure:\n- [x] Selectors are semantic and well-documented\n- [x] Error handling is comprehensive and categorized\n- [x] Data validation includes quality assessment\n- [x] Background tasks include proper lifecycle management\n- [x] File processing includes proper organization\n- [x] Batch processing includes concurrency control\n- [x] Monitoring and health checks are implemented\n- [x] Testing covers various scenarios and edge cases",
        "subsections": []
      }
    ],
    "code_blocks": [
      {
        "language": "python",
        "code": "from bs4 import BeautifulSoup\nimport logging\nfrom typing import List, Dict, Optional\n\nlogger = logging.getLogger(__name__)\n\ndef parse_stock_table(html_content: str) -> List[Dict[str, str]]:\n    \"\"\"Parse stock data from HTML table.\n\n    Args:\n        html_content: Raw HTML content to parse\n\n    Returns:\n        List of dictionaries containing parsed stock data\n\n    Raises:\n        ValueError: If HTML structure is invalid\n        Exception: For other parsing errors\n    \"\"\"\n    try:\n        soup = BeautifulSoup(html_content, 'html.parser')\n\n        # Use semantic selectors - prefer direct table selection\n        table = soup.find('table', class_='stock-table')\n        if not table:\n            logger.warning(\"Stock table not found in HTML\")\n            return []\n\n        # Parse table structure with proper error handling\n        rows = table.find_all('tr')\n        if len(rows) < 2:  # Need header + at least one data row\n            logger.warning(\"Insufficient table rows found\")\n            return []\n\n        # Extract headers with validation\n        header_row = rows[0]\n        headers = []\n        for th in header_row.find_all('th'):\n            header_text = th.get_text(strip=True)\n            if header_text:  # Only include non-empty headers\n                headers.append(header_text)\n\n        if not headers:\n            logger.error(\"No valid headers found in table\")\n            return []\n\n        # Parse data rows with validation\n        data = []\n        for row in rows[1:]:\n            cells = row.find_all('td')\n            if len(cells) == len(headers):\n                row_data = {}\n                for header, cell in zip(headers, cells):\n                    cell_text = cell.get_text(strip=True)\n                    row_data[header] = cell_text\n                data.append(row_data)\n            else:\n                logger.warning(f\"Row has {len(cells)} cells, expected {len(headers)}\")\n\n        logger.info(f\"Successfully parsed {len(data)} rows from stock table\")\n        return data\n\n    except Exception as e:\n        logger.error(f\"Error parsing HTML: {e}\")\n        raise ValueError(f\"Failed to parse HTML table: {e}\")"
      },
      {
        "language": "python",
        "code": "def find_table_with_fallback(soup: BeautifulSoup, selectors: List[str]) -> Optional[BeautifulSoup]:\n    \"\"\"Find table using multiple selector fallbacks.\n\n    Args:\n        soup: BeautifulSoup object\n        selectors: List of CSS selectors to try in order\n\n    Returns:\n        Table element if found, None otherwise\n    \"\"\"\n    for selector in selectors:\n        table = soup.select_one(selector)\n        if table:\n            logger.info(f\"Found table using selector: {selector}\")\n            return table\n\n    logger.warning(f\"No table found with any selector: {selectors}\")\n    return None\n\n# Usage example\ndef parse_stock_data_robust(html_content: str) -> List[Dict[str, str]]:\n    \"\"\"Parse stock data with multiple selector fallbacks.\"\"\"\n    soup = BeautifulSoup(html_content, 'html.parser')\n\n    # Try multiple selectors in order of preference\n    selectors = [\n        'table.stock-table',\n        'table.data-table',\n        'table',\n        '.stock-data table'\n    ]\n\n    table = find_table_with_fallback(soup, selectors)\n    if not table:\n        return []\n\n    # Continue with parsing logic...\n    return parse_table_data(table)"
      },
      {
        "language": "python",
        "code": "import asyncio\nimport logging\nfrom typing import Optional\nfrom datetime import datetime\n\nlogger = logging.getLogger(__name__)\n\nclass TaskLifecycleManager:\n    \"\"\"Manages background task lifecycle and error handling.\"\"\"\n\n    def __init__(self, task_name: str):\n        self.task_name = task_name\n        self.is_running = False\n        self.last_run = None\n        self.error_count = 0\n        self.max_errors = 3\n\n    async def run_with_lifecycle(self, task_func, *args, **kwargs):\n        \"\"\"Run task with proper lifecycle management.\"\"\"\n        self.is_running = True\n        self.last_run = datetime.utcnow()\n\n        try:\n            logger.info(f\"Starting {self.task_name}\")\n            await task_func(*args, **kwargs)\n            self.error_count = 0  # Reset error count on success\n            logger.info(f\"Completed {self.task_name}\")\n\n        except Exception as e:\n            self.error_count += 1\n            logger.error(f\"Error in {self.task_name}: {e}\")\n\n            if self.error_count >= self.max_errors:\n                logger.critical(f\"Too many errors in {self.task_name}, stopping task\")\n                self.is_running = False\n                return False\n\n        return True\n\n# Good - independent, focused tasks with lifecycle management\nasync def run_periodic_updates() -> None:\n    \"\"\"Run periodic updates of tracked stocks.\"\"\"\n    lifecycle_manager = TaskLifecycleManager(\"stock_updates\")\n\n    while lifecycle_manager.is_running:\n        try:\n            success = await lifecycle_manager.run_with_lifecycle(update_all_tracked_stocks)\n            if not success:\n                break\n\n        except Exception as e:\n            logger.error(f\"Critical error in stock updates: {e}\")\n            break\n\n        await asyncio.sleep(RETRIEVAL_INTERVAL_SECONDS)\n\nasync def run_periodic_file_processing() -> None:\n    \"\"\"Run periodic processing of raw HTML files.\"\"\"\n    lifecycle_manager = TaskLifecycleManager(\"file_processing\")\n\n    while lifecycle_manager.is_running:\n        try:\n            success = await lifecycle_manager.run_with_lifecycle(process_raw_files_task)\n            if not success:\n                break\n\n        except RuntimeError as e:\n            logger.critical(f\"Critical error in file processing: {e}\")\n            break\n        except Exception as e:\n            logger.error(f\"Error in file processing: {e}\")\n            # Continue on non-critical errors\n\n        await asyncio.sleep(PROCESSING_INTERVAL_SECONDS)"
      },
      {
        "language": "python",
        "code": "from dataclasses import dataclass\nfrom datetime import datetime, timedelta\nfrom typing import Dict, Any\n\n@dataclass\nclass TaskHealth:\n    \"\"\"Task health monitoring data.\"\"\"\n    task_name: str\n    is_running: bool\n    last_run: Optional[datetime]\n    error_count: int\n    success_count: int\n    avg_duration: float\n\n    def is_healthy(self) -> bool:\n        \"\"\"Check if task is healthy.\"\"\"\n        if not self.is_running:\n            return False\n\n        # Check if task has run recently (within expected interval)\n        if self.last_run and datetime.utcnow() - self.last_run > timedelta(minutes=30):\n            return False\n\n        # Check error rate\n        total_runs = self.success_count + self.error_count\n        if total_runs > 0 and self.error_count / total_runs > 0.5:\n            return False\n\n        return True\n\nclass TaskMonitor:\n    \"\"\"Monitor background task health.\"\"\"\n\n    def __init__(self):\n        self.tasks: Dict[str, TaskHealth] = {}\n\n    def register_task(self, task_name: str) -> TaskHealth:\n        \"\"\"Register a new task for monitoring.\"\"\"\n        health = TaskHealth(\n            task_name=task_name,\n            is_running=True,\n            last_run=None,\n            error_count=0,\n            success_count=0,\n            avg_duration=0.0\n        )\n        self.tasks[task_name] = health\n        return health\n\n    def update_task_health(self, task_name: str, success: bool, duration: float):\n        \"\"\"Update task health metrics.\"\"\"\n        if task_name not in self.tasks:\n            return\n\n        health = self.tasks[task_name]\n        health.last_run = datetime.utcnow()\n\n        if success:\n            health.success_count += 1\n        else:\n            health.error_count += 1\n\n        # Update average duration\n        total_runs = health.success_count + health.error_count\n        health.avg_duration = (health.avg_duration * (total_runs - 1) + duration) / total_runs\n\n    def get_unhealthy_tasks(self) -> List[str]:\n        \"\"\"Get list of unhealthy tasks.\"\"\"\n        return [name for name, health in self.tasks.items() if not health.is_healthy()]"
      },
      {
        "language": "python",
        "code": "import pandas as pd\nfrom datetime import datetime\nfrom typing import List, Dict, Any\n\ndef process_time_series_data(data: List[Dict[str, Any]]) -> pd.DataFrame:\n    \"\"\"Process time series data with proper ordering and validation.\n\n    Args:\n        data: List of dictionaries containing time series data\n\n    Returns:\n        Processed DataFrame with proper time ordering\n\n    Raises:\n        ValueError: If data is invalid or missing required fields\n    \"\"\"\n    if not data:\n        raise ValueError(\"No data provided for processing\")\n\n    df = pd.DataFrame(data)\n\n    # Validate required columns\n    required_columns = ['date', 'symbol']\n    missing_columns = [col for col in required_columns if col not in df.columns]\n    if missing_columns:\n        raise ValueError(f\"Missing required columns: {missing_columns}\")\n\n    # Ensure date column exists and is properly formatted\n    try:\n        df['date'] = pd.to_datetime(df['date'])\n    except Exception as e:\n        raise ValueError(f\"Invalid date format: {e}\")\n\n    # Validate date range\n    current_time = pd.Timestamp.now()\n    future_dates = df[df['date'] > current_time]\n    if not future_dates.empty:\n        logger.warning(f\"Found {len(future_dates)} records with future dates\")\n        df = df[df['date'] <= current_time]\n\n    # Sort by date to ensure chronological order\n    df = df.sort_values('date')\n\n    # Reset index after sorting\n    df = df.reset_index(drop=True)\n\n    # Validate data integrity\n    if df.empty:\n        raise ValueError(\"No valid data after processing\")\n\n    # Check for duplicate date-symbol combinations\n    duplicates = df.duplicated(subset=['date', 'symbol']).sum()\n    if duplicates > 0:\n        logger.warning(f\"Found {duplicates} duplicate date-symbol combinations\")\n        df = df.drop_duplicates(subset=['date', 'symbol'], keep='last')\n\n    logger.info(f\"Processed {len(df)} time series records\")\n    return df"
      },
      {
        "language": "python",
        "code": "from pydantic import BaseModel, validator, Field\nfrom datetime import datetime\nfrom typing import List, Optional\nfrom enum import Enum\n\nclass DataQuality(Enum):\n    \"\"\"Data quality levels.\"\"\"\n    HIGH = \"high\"\n    MEDIUM = \"medium\"\n    LOW = \"low\"\n\nclass StockDataPoint(BaseModel):\n    \"\"\"Validated stock data point.\"\"\"\n    symbol: str = Field(..., min_length=1, max_length=10, regex=r'^[A-Z]+$')\n    date: datetime\n    price: float = Field(..., gt=0)\n    volume: Optional[int] = Field(None, ge=0)\n    quality: DataQuality = DataQuality.MEDIUM\n\n    @validator('price')\n    def validate_price(cls, v):\n        if v <= 0:\n            raise ValueError('Price must be positive')\n        if v > 1000000:  # Reasonable upper limit\n            raise ValueError('Price seems unreasonably high')\n        return v\n\n    @validator('date')\n    def validate_date(cls, v):\n        if v > datetime.now():\n            raise ValueError('Date cannot be in the future')\n        if v < datetime(1990, 1, 1):  # Reasonable lower limit\n            raise ValueError('Date seems unreasonably old')\n        return v\n\n    @validator('symbol')\n    def validate_symbol(cls, v):\n        return v.upper()\n\nclass StockDataValidator:\n    \"\"\"Validate stock data quality and consistency.\"\"\"\n\n    def __init__(self):\n        self.validation_errors = []\n\n    def validate_dataset(self, data: List[Dict[str, Any]]) -> List[StockDataPoint]:\n        \"\"\"Validate entire dataset.\"\"\"\n        validated_data = []\n\n        for i, item in enumerate(data):\n            try:\n                validated_item = StockDataPoint(**item)\n                validated_data.append(validated_item)\n            except Exception as e:\n                self.validation_errors.append(f\"Row {i}: {e}\")\n                logger.warning(f\"Validation error in row {i}: {e}\")\n\n        if self.validation_errors:\n            logger.warning(f\"Found {len(self.validation_errors)} validation errors\")\n\n        return validated_data\n\n    def get_validation_report(self) -> Dict[str, Any]:\n        \"\"\"Get validation report.\"\"\"\n        return {\n            \"total_errors\": len(self.validation_errors),\n            \"errors\": self.validation_errors,\n            \"error_rate\": len(self.validation_errors) / max(len(self.validation_errors), 1)\n        }"
      },
      {
        "language": "python",
        "code": "import os\nfrom pathlib import Path\nfrom datetime import datetime\nfrom typing import List, Optional\n\nclass FileProcessor:\n    \"\"\"Manages file processing with proper organization and error handling.\"\"\"\n\n    def __init__(self, base_path: str):\n        self.base_path = Path(base_path)\n        self.raw_path = self.base_path / \"raw_responses\"\n        self.parsed_path = self.base_path / \"parsed_responses\"\n        self.processed_path = self.base_path / \"processed_data\"\n\n        # Ensure directories exist\n        for path in [self.raw_path, self.parsed_path, self.processed_path]:\n            path.mkdir(parents=True, exist_ok=True)\n\n    def get_raw_files(self, pattern: str = \"*.html\") -> List[Path]:\n        \"\"\"Get list of raw files to process.\"\"\"\n        return list(self.raw_path.glob(pattern))\n\n    def move_to_processed(self, file_path: Path, success: bool = True) -> None:\n        \"\"\"Move file to processed directory with status indication.\"\"\"\n        if not file_path.exists():\n            logger.warning(f\"File not found: {file_path}\")\n            return\n\n        # Create processed subdirectory based on success\n        status_dir = \"success\" if success else \"failed\"\n        processed_dir = self.processed_path / status_dir\n        processed_dir.mkdir(exist_ok=True)\n\n        # Move file with timestamp\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        new_name = f\"{file_path.stem}_{timestamp}{file_path.suffix}\"\n        new_path = processed_dir / new_name\n\n        try:\n            file_path.rename(new_path)\n            logger.info(f\"Moved {file_path} to {new_path}\")\n        except Exception as e:\n            logger.error(f\"Failed to move {file_path}: {e}\")\n\n    def save_parsed_data(self, data: List[Dict[str, Any]], filename: str) -> None:\n        \"\"\"Save parsed data to file.\"\"\"\n        import json\n\n        file_path = self.parsed_path / f\"{filename}.json\"\n        try:\n            with open(file_path, 'w') as f:\n                json.dump(data, f, indent=2, default=str)\n            logger.info(f\"Saved parsed data to {file_path}\")\n        except Exception as e:\n            logger.error(f\"Failed to save parsed data: {e}\")"
      },
      {
        "language": "python",
        "code": "import asyncio\nfrom typing import List, Callable, Any\n\nclass BatchProcessor:\n    \"\"\"Process files in batches with proper error handling.\"\"\"\n\n    def __init__(self, batch_size: int = 10, max_workers: int = 5):\n        self.batch_size = batch_size\n        self.max_workers = max_workers\n        self.processed_count = 0\n        self.error_count = 0\n\n    async def process_batch(\n        self,\n        files: List[Path],\n        processor_func: Callable[[Path], Any]\n    ) -> Dict[str, Any]:\n        \"\"\"Process a batch of files.\"\"\"\n        results = {\n            \"processed\": 0,\n            \"errors\": 0,\n            \"errors_details\": []\n        }\n\n        # Process files with semaphore to limit concurrency\n        semaphore = asyncio.Semaphore(self.max_workers)\n\n        async def process_single_file(file_path: Path):\n            async with semaphore:\n                try:\n                    await processor_func(file_path)\n                    results[\"processed\"] += 1\n                    self.processed_count += 1\n                except Exception as e:\n                    results[\"errors\"] += 1\n                    results[\"errors_details\"].append(f\"{file_path}: {e}\")\n                    self.error_count += 1\n                    logger.error(f\"Error processing {file_path}: {e}\")\n\n        # Create tasks for all files\n        tasks = [process_single_file(file) for file in files]\n\n        # Execute tasks\n        await asyncio.gather(*tasks, return_exceptions=True)\n\n        return results\n\n    async def process_all_files(\n        self,\n        file_list: List[Path],\n        processor_func: Callable[[Path], Any]\n    ) -> Dict[str, Any]:\n        \"\"\"Process all files in batches.\"\"\"\n        total_files = len(file_list)\n        logger.info(f\"Starting batch processing of {total_files} files\")\n\n        all_results = {\n            \"total_files\": total_files,\n            \"total_processed\": 0,\n            \"total_errors\": 0,\n            \"batch_results\": []\n        }\n\n        # Process in batches\n        for i in range(0, total_files, self.batch_size):\n            batch = file_list[i:i + self.batch_size]\n            batch_num = i // self.batch_size + 1\n\n            logger.info(f\"Processing batch {batch_num} ({len(batch)} files)\")\n\n            batch_results = await self.process_batch(batch, processor_func)\n            all_results[\"batch_results\"].append(batch_results)\n            all_results[\"total_processed\"] += batch_results[\"processed\"]\n            all_results[\"total_errors\"] += batch_results[\"errors\"]\n\n        logger.info(f\"Batch processing completed: {all_results['total_processed']} processed, {all_results['total_errors']} errors\")\n        return all_results"
      }
    ],
    "links": [
      {
        "type": "code_reference",
        "text": "../Core%20Principles.md"
      },
      {
        "type": "code_reference",
        "text": "../Language-Specific/Python%20Style%20Guide.md"
      },
      {
        "type": "code_reference",
        "text": "../Language-Specific/FastAPI%20Development%20Guide.md"
      },
      {
        "type": "code_reference",
        "text": "../../project_context/Common%20Patterns.md"
      },
      {
        "type": "code_reference",
        "text": "../../features/summaries/[COMPLETED]-stock_data_processing_pipeline_summary.md"
      },
      {
        "type": "code_reference",
        "text": ".table"
      },
      {
        "type": "code_reference",
        "text": ".table"
      },
      {
        "type": "code_reference",
        "text": ".gridLayout > div:nth-child(2)"
      },
      {
        "type": "code_reference",
        "text": "python\nfrom bs4 import BeautifulSoup\nimport logging\nfrom typing import List, Dict, Optional\n\nlogger = logging.getLogger(__name__)\n\ndef parse_stock_table(html_content: str) -> List[Dict[str, str]]:\n    \"\"\"Parse stock data from HTML table.\n\n    Args:\n        html_content: Raw HTML content to parse\n\n    Returns:\n        List of dictionaries containing parsed stock data\n\n    Raises:\n        ValueError: If HTML structure is invalid\n        Exception: For other parsing errors\n    \"\"\"\n    try:\n        soup = BeautifulSoup(html_content, 'html.parser')\n\n        # Use semantic selectors - prefer direct table selection\n        table = soup.find('table', class_='stock-table')\n        if not table:\n            logger.warning(\"Stock table not found in HTML\")\n            return []\n\n        # Parse table structure with proper error handling\n        rows = table.find_all('tr')\n        if len(rows) < 2:  # Need header + at least one data row\n            logger.warning(\"Insufficient table rows found\")\n            return []\n\n        # Extract headers with validation\n        header_row = rows[0]\n        headers = []\n        for th in header_row.find_all('th'):\n            header_text = th.get_text(strip=True)\n            if header_text:  # Only include non-empty headers\n                headers.append(header_text)\n\n        if not headers:\n            logger.error(\"No valid headers found in table\")\n            return []\n\n        # Parse data rows with validation\n        data = []\n        for row in rows[1:]:\n            cells = row.find_all('td')\n            if len(cells) == len(headers):\n                row_data = {}\n                for header, cell in zip(headers, cells):\n                    cell_text = cell.get_text(strip=True)\n                    row_data[header] = cell_text\n                data.append(row_data)\n            else:\n                logger.warning(f\"Row has {len(cells)} cells, expected {len(headers)}\")\n\n        logger.info(f\"Successfully parsed {len(data)} rows from stock table\")\n        return data\n\n    except Exception as e:\n        logger.error(f\"Error parsing HTML: {e}\")\n        raise ValueError(f\"Failed to parse HTML table: {e}\")\n"
      },
      {
        "type": "code_reference",
        "text": "\n\nThis parsing pattern will inform all HTML table parsing implementation with proper error handling and logging.\n\nAll HTML parsing must include comprehensive error handling, logging, and data validation.\n\n### Robust Selector Patterns\n"
      },
      {
        "type": "code_reference",
        "text": "python\ndef find_table_with_fallback(soup: BeautifulSoup, selectors: List[str]) -> Optional[BeautifulSoup]:\n    \"\"\"Find table using multiple selector fallbacks.\n\n    Args:\n        soup: BeautifulSoup object\n        selectors: List of CSS selectors to try in order\n\n    Returns:\n        Table element if found, None otherwise\n    \"\"\"\n    for selector in selectors:\n        table = soup.select_one(selector)\n        if table:\n            logger.info(f\"Found table using selector: {selector}\")\n            return table\n\n    logger.warning(f\"No table found with any selector: {selectors}\")\n    return None\n\n# Usage example\ndef parse_stock_data_robust(html_content: str) -> List[Dict[str, str]]:\n    \"\"\"Parse stock data with multiple selector fallbacks.\"\"\"\n    soup = BeautifulSoup(html_content, 'html.parser')\n\n    # Try multiple selectors in order of preference\n    selectors = [\n        'table.stock-table',\n        'table.data-table',\n        'table',\n        '.stock-data table'\n    ]\n\n    table = find_table_with_fallback(soup, selectors)\n    if not table:\n        return []\n\n    # Continue with parsing logic...\n    return parse_table_data(table)\n"
      },
      {
        "type": "code_reference",
        "text": "\n\nThis fallback pattern will inform robust selector implementation for handling HTML structure variations.\n\nAll selector implementations must include fallback strategies and proper logging.\n\n## 2. Background Task Integration\n\n### Task Organization\n- **Keep tasks independent and focused**: Separate different operations into independent tasks\n- **Use single responsibility principle**: Each task should have one clear purpose\n- **Implement proper task lifecycle**: Create tasks with clear start/stop conditions\n\nThis task organization will inform all background task implementation and lifecycle management.\n\nAll background tasks must follow single responsibility principle and include proper lifecycle management.\n\n### Task Implementation Patterns\n"
      },
      {
        "type": "code_reference",
        "text": "python\nimport asyncio\nimport logging\nfrom typing import Optional\nfrom datetime import datetime\n\nlogger = logging.getLogger(__name__)\n\nclass TaskLifecycleManager:\n    \"\"\"Manages background task lifecycle and error handling.\"\"\"\n\n    def __init__(self, task_name: str):\n        self.task_name = task_name\n        self.is_running = False\n        self.last_run = None\n        self.error_count = 0\n        self.max_errors = 3\n\n    async def run_with_lifecycle(self, task_func, *args, **kwargs):\n        \"\"\"Run task with proper lifecycle management.\"\"\"\n        self.is_running = True\n        self.last_run = datetime.utcnow()\n\n        try:\n            logger.info(f\"Starting {self.task_name}\")\n            await task_func(*args, **kwargs)\n            self.error_count = 0  # Reset error count on success\n            logger.info(f\"Completed {self.task_name}\")\n\n        except Exception as e:\n            self.error_count += 1\n            logger.error(f\"Error in {self.task_name}: {e}\")\n\n            if self.error_count >= self.max_errors:\n                logger.critical(f\"Too many errors in {self.task_name}, stopping task\")\n                self.is_running = False\n                return False\n\n        return True\n\n# Good - independent, focused tasks with lifecycle management\nasync def run_periodic_updates() -> None:\n    \"\"\"Run periodic updates of tracked stocks.\"\"\"\n    lifecycle_manager = TaskLifecycleManager(\"stock_updates\")\n\n    while lifecycle_manager.is_running:\n        try:\n            success = await lifecycle_manager.run_with_lifecycle(update_all_tracked_stocks)\n            if not success:\n                break\n\n        except Exception as e:\n            logger.error(f\"Critical error in stock updates: {e}\")\n            break\n\n        await asyncio.sleep(RETRIEVAL_INTERVAL_SECONDS)\n\nasync def run_periodic_file_processing() -> None:\n    \"\"\"Run periodic processing of raw HTML files.\"\"\"\n    lifecycle_manager = TaskLifecycleManager(\"file_processing\")\n\n    while lifecycle_manager.is_running:\n        try:\n            success = await lifecycle_manager.run_with_lifecycle(process_raw_files_task)\n            if not success:\n                break\n\n        except RuntimeError as e:\n            logger.critical(f\"Critical error in file processing: {e}\")\n            break\n        except Exception as e:\n            logger.error(f\"Error in file processing: {e}\")\n            # Continue on non-critical errors\n\n        await asyncio.sleep(PROCESSING_INTERVAL_SECONDS)\n"
      },
      {
        "type": "code_reference",
        "text": "\n\nThis lifecycle management pattern will inform all background task implementation with proper error handling and monitoring.\n\nAll background tasks must include lifecycle management and proper error handling strategies.\n\n### Error Handling Strategy\n- **Distinguish error types**: Use different exception types for different error categories\n- **Implement appropriate responses**: Handle critical errors (stop task) and recoverable errors (continue) differently\n- **Use proper logging levels**: Log critical errors with "
      },
      {
        "type": "code_reference",
        "text": "\n- **Provide clear error context**: Include relevant information in error messages\n\nThis error handling strategy will inform all background task error handling implementation.\n\nAll background tasks must implement proper error categorization and appropriate responses.\n\n### Task Health Monitoring\n"
      },
      {
        "type": "code_reference",
        "text": "python\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nfrom typing import Dict, Any\n\n@dataclass\nclass TaskHealth:\n    \"\"\"Task health monitoring data.\"\"\"\n    task_name: str\n    is_running: bool\n    last_run: Optional[datetime]\n    error_count: int\n    success_count: int\n    avg_duration: float\n\n    def is_healthy(self) -> bool:\n        \"\"\"Check if task is healthy.\"\"\"\n        if not self.is_running:\n            return False\n\n        # Check if task has run recently (within expected interval)\n        if self.last_run and datetime.utcnow() - self.last_run > timedelta(minutes=30):\n            return False\n\n        # Check error rate\n        total_runs = self.success_count + self.error_count\n        if total_runs > 0 and self.error_count / total_runs > 0.5:\n            return False\n\n        return True\n\nclass TaskMonitor:\n    \"\"\"Monitor background task health.\"\"\"\n\n    def __init__(self):\n        self.tasks: Dict[str, TaskHealth] = {}\n\n    def register_task(self, task_name: str) -> TaskHealth:\n        \"\"\"Register a new task for monitoring.\"\"\"\n        health = TaskHealth(\n            task_name=task_name,\n            is_running=True,\n            last_run=None,\n            error_count=0,\n            success_count=0,\n            avg_duration=0.0\n        )\n        self.tasks[task_name] = health\n        return health\n\n    def update_task_health(self, task_name: str, success: bool, duration: float):\n        \"\"\"Update task health metrics.\"\"\"\n        if task_name not in self.tasks:\n            return\n\n        health = self.tasks[task_name]\n        health.last_run = datetime.utcnow()\n\n        if success:\n            health.success_count += 1\n        else:\n            health.error_count += 1\n\n        # Update average duration\n        total_runs = health.success_count + health.error_count\n        health.avg_duration = (health.avg_duration * (total_runs - 1) + duration) / total_runs\n\n    def get_unhealthy_tasks(self) -> List[str]:\n        \"\"\"Get list of unhealthy tasks.\"\"\"\n        return [name for name, health in self.tasks.items() if not health.is_healthy()]\n"
      },
      {
        "type": "code_reference",
        "text": "\n\nThis monitoring pattern will inform all background task health monitoring implementation.\n\nAll background tasks must include health monitoring and metrics collection.\n\n## 3. Data Ordering and Time Series\n\n### Best Practices\n- **Make data ordering explicit**: When dealing with time-series data, always consider the order\n- **Add explicit sorting**: Use "
      },
      {
        "type": "code_reference",
        "text": " to ensure consistent data order\n- **Document ordering requirements**: Explain why specific ordering is important\n- **Test ordering edge cases**: Verify behavior with different data orderings\n\nThese practices will inform all time series data processing implementation.\n\nAll time series data must include explicit ordering and proper validation.\n\n### Time Series Data Patterns\n"
      },
      {
        "type": "code_reference",
        "text": "python\nimport pandas as pd\nfrom datetime import datetime\nfrom typing import List, Dict, Any\n\ndef process_time_series_data(data: List[Dict[str, Any]]) -> pd.DataFrame:\n    \"\"\"Process time series data with proper ordering and validation.\n\n    Args:\n        data: List of dictionaries containing time series data\n\n    Returns:\n        Processed DataFrame with proper time ordering\n\n    Raises:\n        ValueError: If data is invalid or missing required fields\n    \"\"\"\n    if not data:\n        raise ValueError(\"No data provided for processing\")\n\n    df = pd.DataFrame(data)\n\n    # Validate required columns\n    required_columns = ['date', 'symbol']\n    missing_columns = [col for col in required_columns if col not in df.columns]\n    if missing_columns:\n        raise ValueError(f\"Missing required columns: {missing_columns}\")\n\n    # Ensure date column exists and is properly formatted\n    try:\n        df['date'] = pd.to_datetime(df['date'])\n    except Exception as e:\n        raise ValueError(f\"Invalid date format: {e}\")\n\n    # Validate date range\n    current_time = pd.Timestamp.now()\n    future_dates = df[df['date'] > current_time]\n    if not future_dates.empty:\n        logger.warning(f\"Found {len(future_dates)} records with future dates\")\n        df = df[df['date'] <= current_time]\n\n    # Sort by date to ensure chronological order\n    df = df.sort_values('date')\n\n    # Reset index after sorting\n    df = df.reset_index(drop=True)\n\n    # Validate data integrity\n    if df.empty:\n        raise ValueError(\"No valid data after processing\")\n\n    # Check for duplicate date-symbol combinations\n    duplicates = df.duplicated(subset=['date', 'symbol']).sum()\n    if duplicates > 0:\n        logger.warning(f\"Found {duplicates} duplicate date-symbol combinations\")\n        df = df.drop_duplicates(subset=['date', 'symbol'], keep='last')\n\n    logger.info(f\"Processed {len(df)} time series records\")\n    return df\n"
      },
      {
        "type": "code_reference",
        "text": "\n\nThis time series pattern will inform all time series data processing with proper validation and ordering.\n\nAll time series processing must include date validation, ordering, and duplicate handling.\n\n### Data Validation Patterns\n"
      },
      {
        "type": "code_reference",
        "text": "python\nfrom pydantic import BaseModel, validator, Field\nfrom datetime import datetime\nfrom typing import List, Optional\nfrom enum import Enum\n\nclass DataQuality(Enum):\n    \"\"\"Data quality levels.\"\"\"\n    HIGH = \"high\"\n    MEDIUM = \"medium\"\n    LOW = \"low\"\n\nclass StockDataPoint(BaseModel):\n    \"\"\"Validated stock data point.\"\"\"\n    symbol: str = Field(..., min_length=1, max_length=10, regex=r'^[A-Z]+$')\n    date: datetime\n    price: float = Field(..., gt=0)\n    volume: Optional[int] = Field(None, ge=0)\n    quality: DataQuality = DataQuality.MEDIUM\n\n    @validator('price')\n    def validate_price(cls, v):\n        if v <= 0:\n            raise ValueError('Price must be positive')\n        if v > 1000000:  # Reasonable upper limit\n            raise ValueError('Price seems unreasonably high')\n        return v\n\n    @validator('date')\n    def validate_date(cls, v):\n        if v > datetime.now():\n            raise ValueError('Date cannot be in the future')\n        if v < datetime(1990, 1, 1):  # Reasonable lower limit\n            raise ValueError('Date seems unreasonably old')\n        return v\n\n    @validator('symbol')\n    def validate_symbol(cls, v):\n        return v.upper()\n\nclass StockDataValidator:\n    \"\"\"Validate stock data quality and consistency.\"\"\"\n\n    def __init__(self):\n        self.validation_errors = []\n\n    def validate_dataset(self, data: List[Dict[str, Any]]) -> List[StockDataPoint]:\n        \"\"\"Validate entire dataset.\"\"\"\n        validated_data = []\n\n        for i, item in enumerate(data):\n            try:\n                validated_item = StockDataPoint(**item)\n                validated_data.append(validated_item)\n            except Exception as e:\n                self.validation_errors.append(f\"Row {i}: {e}\")\n                logger.warning(f\"Validation error in row {i}: {e}\")\n\n        if self.validation_errors:\n            logger.warning(f\"Found {len(self.validation_errors)} validation errors\")\n\n        return validated_data\n\n    def get_validation_report(self) -> Dict[str, Any]:\n        \"\"\"Get validation report.\"\"\"\n        return {\n            \"total_errors\": len(self.validation_errors),\n            \"errors\": self.validation_errors,\n            \"error_rate\": len(self.validation_errors) / max(len(self.validation_errors), 1)\n        }\n"
      },
      {
        "type": "code_reference",
        "text": "\n\nThis validation pattern will inform all data validation implementation with comprehensive error reporting.\n\nAll data validation must include comprehensive error reporting and quality assessment.\n\n## 4. File Processing Patterns\n\n### File Organization\n"
      },
      {
        "type": "code_reference",
        "text": "python\nimport os\nfrom pathlib import Path\nfrom datetime import datetime\nfrom typing import List, Optional\n\nclass FileProcessor:\n    \"\"\"Manages file processing with proper organization and error handling.\"\"\"\n\n    def __init__(self, base_path: str):\n        self.base_path = Path(base_path)\n        self.raw_path = self.base_path / \"raw_responses\"\n        self.parsed_path = self.base_path / \"parsed_responses\"\n        self.processed_path = self.base_path / \"processed_data\"\n\n        # Ensure directories exist\n        for path in [self.raw_path, self.parsed_path, self.processed_path]:\n            path.mkdir(parents=True, exist_ok=True)\n\n    def get_raw_files(self, pattern: str = \"*.html\") -> List[Path]:\n        \"\"\"Get list of raw files to process.\"\"\"\n        return list(self.raw_path.glob(pattern))\n\n    def move_to_processed(self, file_path: Path, success: bool = True) -> None:\n        \"\"\"Move file to processed directory with status indication.\"\"\"\n        if not file_path.exists():\n            logger.warning(f\"File not found: {file_path}\")\n            return\n\n        # Create processed subdirectory based on success\n        status_dir = \"success\" if success else \"failed\"\n        processed_dir = self.processed_path / status_dir\n        processed_dir.mkdir(exist_ok=True)\n\n        # Move file with timestamp\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        new_name = f\"{file_path.stem}_{timestamp}{file_path.suffix}\"\n        new_path = processed_dir / new_name\n\n        try:\n            file_path.rename(new_path)\n            logger.info(f\"Moved {file_path} to {new_path}\")\n        except Exception as e:\n            logger.error(f\"Failed to move {file_path}: {e}\")\n\n    def save_parsed_data(self, data: List[Dict[str, Any]], filename: str) -> None:\n        \"\"\"Save parsed data to file.\"\"\"\n        import json\n\n        file_path = self.parsed_path / f\"{filename}.json\"\n        try:\n            with open(file_path, 'w') as f:\n                json.dump(data, f, indent=2, default=str)\n            logger.info(f\"Saved parsed data to {file_path}\")\n        except Exception as e:\n            logger.error(f\"Failed to save parsed data: {e}\")\n"
      },
      {
        "type": "code_reference",
        "text": "\n\nThis file processing pattern will inform all file organization and processing implementation.\n\nAll file processing must include proper organization, error handling, and status tracking.\n\n### Batch Processing\n"
      },
      {
        "type": "code_reference",
        "text": "python\nimport asyncio\nfrom typing import List, Callable, Any\n\nclass BatchProcessor:\n    \"\"\"Process files in batches with proper error handling.\"\"\"\n\n    def __init__(self, batch_size: int = 10, max_workers: int = 5):\n        self.batch_size = batch_size\n        self.max_workers = max_workers\n        self.processed_count = 0\n        self.error_count = 0\n\n    async def process_batch(\n        self,\n        files: List[Path],\n        processor_func: Callable[[Path], Any]\n    ) -> Dict[str, Any]:\n        \"\"\"Process a batch of files.\"\"\"\n        results = {\n            \"processed\": 0,\n            \"errors\": 0,\n            \"errors_details\": []\n        }\n\n        # Process files with semaphore to limit concurrency\n        semaphore = asyncio.Semaphore(self.max_workers)\n\n        async def process_single_file(file_path: Path):\n            async with semaphore:\n                try:\n                    await processor_func(file_path)\n                    results[\"processed\"] += 1\n                    self.processed_count += 1\n                except Exception as e:\n                    results[\"errors\"] += 1\n                    results[\"errors_details\"].append(f\"{file_path}: {e}\")\n                    self.error_count += 1\n                    logger.error(f\"Error processing {file_path}: {e}\")\n\n        # Create tasks for all files\n        tasks = [process_single_file(file) for file in files]\n\n        # Execute tasks\n        await asyncio.gather(*tasks, return_exceptions=True)\n\n        return results\n\n    async def process_all_files(\n        self,\n        file_list: List[Path],\n        processor_func: Callable[[Path], Any]\n    ) -> Dict[str, Any]:\n        \"\"\"Process all files in batches.\"\"\"\n        total_files = len(file_list)\n        logger.info(f\"Starting batch processing of {total_files} files\")\n\n        all_results = {\n            \"total_files\": total_files,\n            \"total_processed\": 0,\n            \"total_errors\": 0,\n            \"batch_results\": []\n        }\n\n        # Process in batches\n        for i in range(0, total_files, self.batch_size):\n            batch = file_list[i:i + self.batch_size]\n            batch_num = i // self.batch_size + 1\n\n            logger.info(f\"Processing batch {batch_num} ({len(batch)} files)\")\n\n            batch_results = await self.process_batch(batch, processor_func)\n            all_results[\"batch_results\"].append(batch_results)\n            all_results[\"total_processed\"] += batch_results[\"processed\"]\n            all_results[\"total_errors\"] += batch_results[\"errors\"]\n\n        logger.info(f\"Batch processing completed: {all_results['total_processed']} processed, {all_results['total_errors']} errors\")\n        return all_results\n"
      }
    ],
    "raw_content": "# Web Scraping Patterns\n\n> This guide provides comprehensive patterns and best practices for web scraping and data extraction. Use these patterns to create reliable, efficient web scraping solutions.\n\n## AI Metadata\n\n**Template Version:** 2.1\n**AI Processing Level:** High\n**Required Context:** Web scraping, HTML parsing, async programming, data processing\n**Validation Required:** Yes\n**Code Generation:** Supported\n\n**Dependencies:**\n- `../Core%20Principles.md` - Decision-making frameworks\n- `../Language-Specific/Python%20Style%20Guide.md` - Python implementation patterns\n- `../Language-Specific/FastAPI%20Development%20Guide.md` - Background task patterns\n- `../../project_context/Common%20Patterns.md` - Project-specific patterns\n- `../../features/summaries/[COMPLETED]-stock_data_processing_pipeline_summary.md` - Stock data implementation\n\n**Validation Rules:**\n- All HTML parsing must use semantic selectors and proper error handling\n- Background tasks must include proper lifecycle management\n- Data ordering must be explicit and consistent\n- Error handling must distinguish between critical and recoverable errors\n- All selectors must be documented with reasoning\n\n## Overview\n\n**Document Purpose:** Web scraping and data processing patterns specific to the CreamPie project\n**Scope:** HTML parsing, background tasks, data ordering, error handling, and file processing\n**Target Users:** AI assistants and developers implementing web scraping functionality\n**Last Updated:** Current\n\n**AI Context:** This guide provides the foundational patterns for all web scraping and data processing in the project. It ensures robust, maintainable, and error-resistant scraping implementations.\n\n## 1. HTML Structure Handling\n\n### Best Practices\n- **Use proper semantic HTML elements**: Leverage `<thead>`, `<tbody>` when parsing tables\n- **Write robust parsers**: Handle standard HTML table structures correctly\n- **Avoid assumptions about structure**: Don't assume HTML will always have a specific format\n- **Use semantic selectors**: Prefer direct, semantic selectors (e.g., `.table`) over complex ones\n\nThese practices will inform all HTML parsing implementation throughout the project.\n\nAll HTML parsing must follow these practices and include proper error handling.\n\n### CSS Selector Design\n- **Keep selectors simple and specific**: Avoid overly complex selectors that are fragile\n- **Use direct, semantic selectors**: Prefer `.table` over `.gridLayout > div:nth-child(2)`\n- **Make selectors resilient**: Choose selectors that are less likely to break with HTML changes\n- **Document selector assumptions**: Explain why specific selectors were chosen\n\nThis selector strategy will inform all CSS selector design and documentation.\n\nAll selectors must be documented with reasoning and tested for resilience.\n\n### HTML Parsing Patterns\n```python\nfrom bs4 import BeautifulSoup\nimport logging\nfrom typing import List, Dict, Optional\n\nlogger = logging.getLogger(__name__)\n\ndef parse_stock_table(html_content: str) -> List[Dict[str, str]]:\n    \"\"\"Parse stock data from HTML table.\n\n    Args:\n        html_content: Raw HTML content to parse\n\n    Returns:\n        List of dictionaries containing parsed stock data\n\n    Raises:\n        ValueError: If HTML structure is invalid\n        Exception: For other parsing errors\n    \"\"\"\n    try:\n        soup = BeautifulSoup(html_content, 'html.parser')\n\n        # Use semantic selectors - prefer direct table selection\n        table = soup.find('table', class_='stock-table')\n        if not table:\n            logger.warning(\"Stock table not found in HTML\")\n            return []\n\n        # Parse table structure with proper error handling\n        rows = table.find_all('tr')\n        if len(rows) < 2:  # Need header + at least one data row\n            logger.warning(\"Insufficient table rows found\")\n            return []\n\n        # Extract headers with validation\n        header_row = rows[0]\n        headers = []\n        for th in header_row.find_all('th'):\n            header_text = th.get_text(strip=True)\n            if header_text:  # Only include non-empty headers\n                headers.append(header_text)\n\n        if not headers:\n            logger.error(\"No valid headers found in table\")\n            return []\n\n        # Parse data rows with validation\n        data = []\n        for row in rows[1:]:\n            cells = row.find_all('td')\n            if len(cells) == len(headers):\n                row_data = {}\n                for header, cell in zip(headers, cells):\n                    cell_text = cell.get_text(strip=True)\n                    row_data[header] = cell_text\n                data.append(row_data)\n            else:\n                logger.warning(f\"Row has {len(cells)} cells, expected {len(headers)}\")\n\n        logger.info(f\"Successfully parsed {len(data)} rows from stock table\")\n        return data\n\n    except Exception as e:\n        logger.error(f\"Error parsing HTML: {e}\")\n        raise ValueError(f\"Failed to parse HTML table: {e}\")\n```\n\nThis parsing pattern will inform all HTML table parsing implementation with proper error handling and logging.\n\nAll HTML parsing must include comprehensive error handling, logging, and data validation.\n\n### Robust Selector Patterns\n```python\ndef find_table_with_fallback(soup: BeautifulSoup, selectors: List[str]) -> Optional[BeautifulSoup]:\n    \"\"\"Find table using multiple selector fallbacks.\n\n    Args:\n        soup: BeautifulSoup object\n        selectors: List of CSS selectors to try in order\n\n    Returns:\n        Table element if found, None otherwise\n    \"\"\"\n    for selector in selectors:\n        table = soup.select_one(selector)\n        if table:\n            logger.info(f\"Found table using selector: {selector}\")\n            return table\n\n    logger.warning(f\"No table found with any selector: {selectors}\")\n    return None\n\n# Usage example\ndef parse_stock_data_robust(html_content: str) -> List[Dict[str, str]]:\n    \"\"\"Parse stock data with multiple selector fallbacks.\"\"\"\n    soup = BeautifulSoup(html_content, 'html.parser')\n\n    # Try multiple selectors in order of preference\n    selectors = [\n        'table.stock-table',\n        'table.data-table',\n        'table',\n        '.stock-data table'\n    ]\n\n    table = find_table_with_fallback(soup, selectors)\n    if not table:\n        return []\n\n    # Continue with parsing logic...\n    return parse_table_data(table)\n```\n\nThis fallback pattern will inform robust selector implementation for handling HTML structure variations.\n\nAll selector implementations must include fallback strategies and proper logging.\n\n## 2. Background Task Integration\n\n### Task Organization\n- **Keep tasks independent and focused**: Separate different operations into independent tasks\n- **Use single responsibility principle**: Each task should have one clear purpose\n- **Implement proper task lifecycle**: Create tasks with clear start/stop conditions\n\nThis task organization will inform all background task implementation and lifecycle management.\n\nAll background tasks must follow single responsibility principle and include proper lifecycle management.\n\n### Task Implementation Patterns\n```python\nimport asyncio\nimport logging\nfrom typing import Optional\nfrom datetime import datetime\n\nlogger = logging.getLogger(__name__)\n\nclass TaskLifecycleManager:\n    \"\"\"Manages background task lifecycle and error handling.\"\"\"\n\n    def __init__(self, task_name: str):\n        self.task_name = task_name\n        self.is_running = False\n        self.last_run = None\n        self.error_count = 0\n        self.max_errors = 3\n\n    async def run_with_lifecycle(self, task_func, *args, **kwargs):\n        \"\"\"Run task with proper lifecycle management.\"\"\"\n        self.is_running = True\n        self.last_run = datetime.utcnow()\n\n        try:\n            logger.info(f\"Starting {self.task_name}\")\n            await task_func(*args, **kwargs)\n            self.error_count = 0  # Reset error count on success\n            logger.info(f\"Completed {self.task_name}\")\n\n        except Exception as e:\n            self.error_count += 1\n            logger.error(f\"Error in {self.task_name}: {e}\")\n\n            if self.error_count >= self.max_errors:\n                logger.critical(f\"Too many errors in {self.task_name}, stopping task\")\n                self.is_running = False\n                return False\n\n        return True\n\n# Good - independent, focused tasks with lifecycle management\nasync def run_periodic_updates() -> None:\n    \"\"\"Run periodic updates of tracked stocks.\"\"\"\n    lifecycle_manager = TaskLifecycleManager(\"stock_updates\")\n\n    while lifecycle_manager.is_running:\n        try:\n            success = await lifecycle_manager.run_with_lifecycle(update_all_tracked_stocks)\n            if not success:\n                break\n\n        except Exception as e:\n            logger.error(f\"Critical error in stock updates: {e}\")\n            break\n\n        await asyncio.sleep(RETRIEVAL_INTERVAL_SECONDS)\n\nasync def run_periodic_file_processing() -> None:\n    \"\"\"Run periodic processing of raw HTML files.\"\"\"\n    lifecycle_manager = TaskLifecycleManager(\"file_processing\")\n\n    while lifecycle_manager.is_running:\n        try:\n            success = await lifecycle_manager.run_with_lifecycle(process_raw_files_task)\n            if not success:\n                break\n\n        except RuntimeError as e:\n            logger.critical(f\"Critical error in file processing: {e}\")\n            break\n        except Exception as e:\n            logger.error(f\"Error in file processing: {e}\")\n            # Continue on non-critical errors\n\n        await asyncio.sleep(PROCESSING_INTERVAL_SECONDS)\n```\n\nThis lifecycle management pattern will inform all background task implementation with proper error handling and monitoring.\n\nAll background tasks must include lifecycle management and proper error handling strategies.\n\n### Error Handling Strategy\n- **Distinguish error types**: Use different exception types for different error categories\n- **Implement appropriate responses**: Handle critical errors (stop task) and recoverable errors (continue) differently\n- **Use proper logging levels**: Log critical errors with `logger.critical()` and regular errors with `logger.error()`\n- **Provide clear error context**: Include relevant information in error messages\n\nThis error handling strategy will inform all background task error handling implementation.\n\nAll background tasks must implement proper error categorization and appropriate responses.\n\n### Task Health Monitoring\n```python\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nfrom typing import Dict, Any\n\n@dataclass\nclass TaskHealth:\n    \"\"\"Task health monitoring data.\"\"\"\n    task_name: str\n    is_running: bool\n    last_run: Optional[datetime]\n    error_count: int\n    success_count: int\n    avg_duration: float\n\n    def is_healthy(self) -> bool:\n        \"\"\"Check if task is healthy.\"\"\"\n        if not self.is_running:\n            return False\n\n        # Check if task has run recently (within expected interval)\n        if self.last_run and datetime.utcnow() - self.last_run > timedelta(minutes=30):\n            return False\n\n        # Check error rate\n        total_runs = self.success_count + self.error_count\n        if total_runs > 0 and self.error_count / total_runs > 0.5:\n            return False\n\n        return True\n\nclass TaskMonitor:\n    \"\"\"Monitor background task health.\"\"\"\n\n    def __init__(self):\n        self.tasks: Dict[str, TaskHealth] = {}\n\n    def register_task(self, task_name: str) -> TaskHealth:\n        \"\"\"Register a new task for monitoring.\"\"\"\n        health = TaskHealth(\n            task_name=task_name,\n            is_running=True,\n            last_run=None,\n            error_count=0,\n            success_count=0,\n            avg_duration=0.0\n        )\n        self.tasks[task_name] = health\n        return health\n\n    def update_task_health(self, task_name: str, success: bool, duration: float):\n        \"\"\"Update task health metrics.\"\"\"\n        if task_name not in self.tasks:\n            return\n\n        health = self.tasks[task_name]\n        health.last_run = datetime.utcnow()\n\n        if success:\n            health.success_count += 1\n        else:\n            health.error_count += 1\n\n        # Update average duration\n        total_runs = health.success_count + health.error_count\n        health.avg_duration = (health.avg_duration * (total_runs - 1) + duration) / total_runs\n\n    def get_unhealthy_tasks(self) -> List[str]:\n        \"\"\"Get list of unhealthy tasks.\"\"\"\n        return [name for name, health in self.tasks.items() if not health.is_healthy()]\n```\n\nThis monitoring pattern will inform all background task health monitoring implementation.\n\nAll background tasks must include health monitoring and metrics collection.\n\n## 3. Data Ordering and Time Series\n\n### Best Practices\n- **Make data ordering explicit**: When dealing with time-series data, always consider the order\n- **Add explicit sorting**: Use `sort_values(\"date\")` to ensure consistent data order\n- **Document ordering requirements**: Explain why specific ordering is important\n- **Test ordering edge cases**: Verify behavior with different data orderings\n\nThese practices will inform all time series data processing implementation.\n\nAll time series data must include explicit ordering and proper validation.\n\n### Time Series Data Patterns\n```python\nimport pandas as pd\nfrom datetime import datetime\nfrom typing import List, Dict, Any\n\ndef process_time_series_data(data: List[Dict[str, Any]]) -> pd.DataFrame:\n    \"\"\"Process time series data with proper ordering and validation.\n\n    Args:\n        data: List of dictionaries containing time series data\n\n    Returns:\n        Processed DataFrame with proper time ordering\n\n    Raises:\n        ValueError: If data is invalid or missing required fields\n    \"\"\"\n    if not data:\n        raise ValueError(\"No data provided for processing\")\n\n    df = pd.DataFrame(data)\n\n    # Validate required columns\n    required_columns = ['date', 'symbol']\n    missing_columns = [col for col in required_columns if col not in df.columns]\n    if missing_columns:\n        raise ValueError(f\"Missing required columns: {missing_columns}\")\n\n    # Ensure date column exists and is properly formatted\n    try:\n        df['date'] = pd.to_datetime(df['date'])\n    except Exception as e:\n        raise ValueError(f\"Invalid date format: {e}\")\n\n    # Validate date range\n    current_time = pd.Timestamp.now()\n    future_dates = df[df['date'] > current_time]\n    if not future_dates.empty:\n        logger.warning(f\"Found {len(future_dates)} records with future dates\")\n        df = df[df['date'] <= current_time]\n\n    # Sort by date to ensure chronological order\n    df = df.sort_values('date')\n\n    # Reset index after sorting\n    df = df.reset_index(drop=True)\n\n    # Validate data integrity\n    if df.empty:\n        raise ValueError(\"No valid data after processing\")\n\n    # Check for duplicate date-symbol combinations\n    duplicates = df.duplicated(subset=['date', 'symbol']).sum()\n    if duplicates > 0:\n        logger.warning(f\"Found {duplicates} duplicate date-symbol combinations\")\n        df = df.drop_duplicates(subset=['date', 'symbol'], keep='last')\n\n    logger.info(f\"Processed {len(df)} time series records\")\n    return df\n```\n\nThis time series pattern will inform all time series data processing with proper validation and ordering.\n\nAll time series processing must include date validation, ordering, and duplicate handling.\n\n### Data Validation Patterns\n```python\nfrom pydantic import BaseModel, validator, Field\nfrom datetime import datetime\nfrom typing import List, Optional\nfrom enum import Enum\n\nclass DataQuality(Enum):\n    \"\"\"Data quality levels.\"\"\"\n    HIGH = \"high\"\n    MEDIUM = \"medium\"\n    LOW = \"low\"\n\nclass StockDataPoint(BaseModel):\n    \"\"\"Validated stock data point.\"\"\"\n    symbol: str = Field(..., min_length=1, max_length=10, regex=r'^[A-Z]+$')\n    date: datetime\n    price: float = Field(..., gt=0)\n    volume: Optional[int] = Field(None, ge=0)\n    quality: DataQuality = DataQuality.MEDIUM\n\n    @validator('price')\n    def validate_price(cls, v):\n        if v <= 0:\n            raise ValueError('Price must be positive')\n        if v > 1000000:  # Reasonable upper limit\n            raise ValueError('Price seems unreasonably high')\n        return v\n\n    @validator('date')\n    def validate_date(cls, v):\n        if v > datetime.now():\n            raise ValueError('Date cannot be in the future')\n        if v < datetime(1990, 1, 1):  # Reasonable lower limit\n            raise ValueError('Date seems unreasonably old')\n        return v\n\n    @validator('symbol')\n    def validate_symbol(cls, v):\n        return v.upper()\n\nclass StockDataValidator:\n    \"\"\"Validate stock data quality and consistency.\"\"\"\n\n    def __init__(self):\n        self.validation_errors = []\n\n    def validate_dataset(self, data: List[Dict[str, Any]]) -> List[StockDataPoint]:\n        \"\"\"Validate entire dataset.\"\"\"\n        validated_data = []\n\n        for i, item in enumerate(data):\n            try:\n                validated_item = StockDataPoint(**item)\n                validated_data.append(validated_item)\n            except Exception as e:\n                self.validation_errors.append(f\"Row {i}: {e}\")\n                logger.warning(f\"Validation error in row {i}: {e}\")\n\n        if self.validation_errors:\n            logger.warning(f\"Found {len(self.validation_errors)} validation errors\")\n\n        return validated_data\n\n    def get_validation_report(self) -> Dict[str, Any]:\n        \"\"\"Get validation report.\"\"\"\n        return {\n            \"total_errors\": len(self.validation_errors),\n            \"errors\": self.validation_errors,\n            \"error_rate\": len(self.validation_errors) / max(len(self.validation_errors), 1)\n        }\n```\n\nThis validation pattern will inform all data validation implementation with comprehensive error reporting.\n\nAll data validation must include comprehensive error reporting and quality assessment.\n\n## 4. File Processing Patterns\n\n### File Organization\n```python\nimport os\nfrom pathlib import Path\nfrom datetime import datetime\nfrom typing import List, Optional\n\nclass FileProcessor:\n    \"\"\"Manages file processing with proper organization and error handling.\"\"\"\n\n    def __init__(self, base_path: str):\n        self.base_path = Path(base_path)\n        self.raw_path = self.base_path / \"raw_responses\"\n        self.parsed_path = self.base_path / \"parsed_responses\"\n        self.processed_path = self.base_path / \"processed_data\"\n\n        # Ensure directories exist\n        for path in [self.raw_path, self.parsed_path, self.processed_path]:\n            path.mkdir(parents=True, exist_ok=True)\n\n    def get_raw_files(self, pattern: str = \"*.html\") -> List[Path]:\n        \"\"\"Get list of raw files to process.\"\"\"\n        return list(self.raw_path.glob(pattern))\n\n    def move_to_processed(self, file_path: Path, success: bool = True) -> None:\n        \"\"\"Move file to processed directory with status indication.\"\"\"\n        if not file_path.exists():\n            logger.warning(f\"File not found: {file_path}\")\n            return\n\n        # Create processed subdirectory based on success\n        status_dir = \"success\" if success else \"failed\"\n        processed_dir = self.processed_path / status_dir\n        processed_dir.mkdir(exist_ok=True)\n\n        # Move file with timestamp\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        new_name = f\"{file_path.stem}_{timestamp}{file_path.suffix}\"\n        new_path = processed_dir / new_name\n\n        try:\n            file_path.rename(new_path)\n            logger.info(f\"Moved {file_path} to {new_path}\")\n        except Exception as e:\n            logger.error(f\"Failed to move {file_path}: {e}\")\n\n    def save_parsed_data(self, data: List[Dict[str, Any]], filename: str) -> None:\n        \"\"\"Save parsed data to file.\"\"\"\n        import json\n\n        file_path = self.parsed_path / f\"{filename}.json\"\n        try:\n            with open(file_path, 'w') as f:\n                json.dump(data, f, indent=2, default=str)\n            logger.info(f\"Saved parsed data to {file_path}\")\n        except Exception as e:\n            logger.error(f\"Failed to save parsed data: {e}\")\n```\n\nThis file processing pattern will inform all file organization and processing implementation.\n\nAll file processing must include proper organization, error handling, and status tracking.\n\n### Batch Processing\n```python\nimport asyncio\nfrom typing import List, Callable, Any\n\nclass BatchProcessor:\n    \"\"\"Process files in batches with proper error handling.\"\"\"\n\n    def __init__(self, batch_size: int = 10, max_workers: int = 5):\n        self.batch_size = batch_size\n        self.max_workers = max_workers\n        self.processed_count = 0\n        self.error_count = 0\n\n    async def process_batch(\n        self,\n        files: List[Path],\n        processor_func: Callable[[Path], Any]\n    ) -> Dict[str, Any]:\n        \"\"\"Process a batch of files.\"\"\"\n        results = {\n            \"processed\": 0,\n            \"errors\": 0,\n            \"errors_details\": []\n        }\n\n        # Process files with semaphore to limit concurrency\n        semaphore = asyncio.Semaphore(self.max_workers)\n\n        async def process_single_file(file_path: Path):\n            async with semaphore:\n                try:\n                    await processor_func(file_path)\n                    results[\"processed\"] += 1\n                    self.processed_count += 1\n                except Exception as e:\n                    results[\"errors\"] += 1\n                    results[\"errors_details\"].append(f\"{file_path}: {e}\")\n                    self.error_count += 1\n                    logger.error(f\"Error processing {file_path}: {e}\")\n\n        # Create tasks for all files\n        tasks = [process_single_file(file) for file in files]\n\n        # Execute tasks\n        await asyncio.gather(*tasks, return_exceptions=True)\n\n        return results\n\n    async def process_all_files(\n        self,\n        file_list: List[Path],\n        processor_func: Callable[[Path], Any]\n    ) -> Dict[str, Any]:\n        \"\"\"Process all files in batches.\"\"\"\n        total_files = len(file_list)\n        logger.info(f\"Starting batch processing of {total_files} files\")\n\n        all_results = {\n            \"total_files\": total_files,\n            \"total_processed\": 0,\n            \"total_errors\": 0,\n            \"batch_results\": []\n        }\n\n        # Process in batches\n        for i in range(0, total_files, self.batch_size):\n            batch = file_list[i:i + self.batch_size]\n            batch_num = i // self.batch_size + 1\n\n            logger.info(f\"Processing batch {batch_num} ({len(batch)} files)\")\n\n            batch_results = await self.process_batch(batch, processor_func)\n            all_results[\"batch_results\"].append(batch_results)\n            all_results[\"total_processed\"] += batch_results[\"processed\"]\n            all_results[\"total_errors\"] += batch_results[\"errors\"]\n\n        logger.info(f\"Batch processing completed: {all_results['total_processed']} processed, {all_results['total_errors']} errors\")\n        return all_results\n```\n\nThis batch processing pattern will inform all batch file processing implementation with proper concurrency control.\n\nAll batch processing must include proper concurrency control, error handling, and progress tracking.\n\n## Implementation Guidelines\n\n### For AI Assistants\n1. **Follow these patterns** for all web scraping implementation\n2. **Use semantic selectors** and document selector reasoning\n3. **Implement proper error handling** with error categorization\n4. **Include comprehensive logging** at all stages\n5. **Validate data quality** and handle edge cases\n6. **Use background tasks** for long-running operations\n7. **Implement proper file organization** and status tracking\n8. **Test with various HTML structures** and error conditions\n\n### For Human Developers\n1. **Reference these patterns** when implementing web scraping\n2. **Use robust selectors** that can handle HTML changes\n3. **Implement comprehensive error handling** and logging\n4. **Validate data quality** before processing\n5. **Use background tasks** for performance\n6. **Organize files properly** with clear status tracking\n7. **Test thoroughly** with various scenarios\n\n## Quality Assurance\n\n### Scraping Quality Standards\n- All selectors must be documented with reasoning\n- Error handling must be comprehensive and categorized\n- Data validation must include quality assessment\n- File processing must include proper organization\n\n### Performance Standards\n- Batch processing must use appropriate concurrency limits\n- Background tasks must not block main application flow\n- File operations must be efficient and non-blocking\n- Memory usage must be optimized for large datasets\n\n### Reliability Standards\n- Scraping must handle HTML structure changes gracefully\n- Error recovery must be implemented for transient failures\n- Data integrity must be maintained throughout processing\n- File status must be properly tracked and reported\n\n### Monitoring Standards\n- Task health must be monitored continuously\n- Error rates must be tracked and reported\n- Performance metrics must be collected\n- Data quality metrics must be assessed\n\n---\n\n**AI Quality Checklist**: Before implementing web scraping functionality, ensure:\n- [x] Selectors are semantic and well-documented\n- [x] Error handling is comprehensive and categorized\n- [x] Data validation includes quality assessment\n- [x] Background tasks include proper lifecycle management\n- [x] File processing includes proper organization\n- [x] Batch processing includes concurrency control\n- [x] Monitoring and health checks are implemented\n- [x] Testing covers various scenarios and edge cases\n"
  },
  "cross_references": [],
  "code_generation_hints": [
    {
      "context": "general",
      "hint": "These practices will inform all HTML parsing implementation throughout the project.",
      "validation": ""
    },
    {
      "context": "general",
      "hint": "This selector strategy will inform all CSS selector design and documentation.",
      "validation": ""
    },
    {
      "context": "error handling",
      "hint": "This parsing pattern will inform all HTML table parsing implementation with proper error handling and logging.",
      "validation": ""
    },
    {
      "context": "general",
      "hint": "This fallback pattern will inform robust selector implementation for handling HTML structure variations.",
      "validation": ""
    },
    {
      "context": "general",
      "hint": "This task organization will inform all background task implementation and lifecycle management.",
      "validation": ""
    },
    {
      "context": "error handling",
      "hint": "This lifecycle management pattern will inform all background task implementation with proper error handling and monitoring.",
      "validation": ""
    },
    {
      "context": "error handling",
      "hint": "This error handling strategy will inform all background task error handling implementation.",
      "validation": ""
    },
    {
      "context": "general",
      "hint": "This monitoring pattern will inform all background task health monitoring implementation.",
      "validation": ""
    },
    {
      "context": "general",
      "hint": "These practices will inform all time series data processing implementation.",
      "validation": ""
    },
    {
      "context": "general",
      "hint": "This time series pattern will inform all time series data processing with proper validation and ordering.",
      "validation": ""
    },
    {
      "context": "error handling",
      "hint": "This validation pattern will inform all data validation implementation with comprehensive error reporting.",
      "validation": ""
    },
    {
      "context": "general",
      "hint": "This file processing pattern will inform all file organization and processing implementation.",
      "validation": ""
    },
    {
      "context": "general",
      "hint": "This batch processing pattern will inform all batch file processing implementation with proper concurrency control.",
      "validation": ""
    }
  ],
  "validation_rules": [
    "File operations must be efficient and non-blocking",
    "All selectors must be documented with reasoning",
    "Error handling must distinguish between critical and recoverable errors",
    "File status must be properly tracked and reported",
    "Task health must be monitored continuously",
    "Performance metrics must be collected",
    "Data validation must include quality assessment",
    "Scraping must handle HTML structure changes gracefully",
    "Background tasks must include proper lifecycle management",
    "Error recovery must be implemented for transient failures",
    "All HTML parsing must include comprehensive error handling, logging, and data validation",
    "Memory usage must be optimized for large datasets",
    "All data validation must include comprehensive error reporting and quality assessment",
    "**Use single responsibility principle**: Each task should have one clear purpose",
    "All HTML parsing must use semantic selectors and proper error handling",
    "All background tasks must include health monitoring and metrics collection",
    "All selectors must be documented with reasoning and tested for resilience",
    "Data quality metrics must be assessed",
    "Data ordering must be explicit and consistent",
    "All background tasks must follow single responsibility principle and include proper lifecycle management",
    "Data integrity must be maintained throughout processing",
    "All time series processing must include date validation, ordering, and duplicate handling",
    "Background tasks must not block main application flow",
    "File processing must include proper organization",
    "All selector implementations must include fallback strategies and proper logging",
    "Batch processing must use appropriate concurrency limits",
    "Error handling must be comprehensive and categorized",
    "All background tasks must include lifecycle management and proper error handling strategies",
    "All file processing must include proper organization, error handling, and status tracking",
    "All batch processing must include proper concurrency control, error handling, and progress tracking",
    "All HTML parsing must follow these practices and include proper error handling",
    "All time series data must include explicit ordering and proper validation",
    "All background tasks must implement proper error categorization and appropriate responses",
    "Error rates must be tracked and reported"
  ],
  "optimization": {
    "version": "1.0",
    "optimized_at": "2025-06-18T19:19:47.751597",
    "improvements": [
      "fixed_file_references",
      "extracted_ai_metadata",
      "structured_cross_references",
      "extracted_code_hints",
      "structured_validation_rules"
    ],
    "literal_strings_cleaned": true,
    "cleaned_at": "2025-06-18T19:30:00.000000"
  }
}