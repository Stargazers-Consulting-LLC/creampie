{
  "ai_metadata": {
    "purpose": "",
    "last_updated": "",
    "template_version": "2.1",
    "ai_tool_compatibility": "",
    "ai_processing_level": "High",
    "required_context": "Python testing, pytest, mocking, test data management",
    "validation_required": "Yes",
    "code_generation": "Supported",
    "cross_references": [
      "../Core%20Principles.json",
      "../Python%20Style%20Guide.json",
      "../Domain-Specific/Web%20Scraping%20Patterns.json",
      "../../project_context/Common%20Patterns.json",
      "../../project_context/Architecture%20Overview.json"
    ],
    "maintenance": ""
  },
  "file_info": {
    "file_path": "guide_docs/Language-Specific/Python Testing Guide.md",
    "original_format": "markdown",
    "converted_at": "2025-06-18T19:14:30.251248",
    "file_size": 21887,
    "line_count": 654,
    "optimized_at": "2025-06-18T19:19:47.745247",
    "optimization_version": "1.0"
  },
  "content": {
    "sections": [
      {
        "level": 1,
        "title": "Python Testing Guide",
        "content": "> This guide provides comprehensive testing patterns and best practices for Python development. Use these patterns to ensure robust, maintainable test suites.",
        "subsections": []
      },
      {
        "level": 2,
        "title": "AI Metadata",
        "content": "**Template Version:** 2.1\n**AI Processing Level:** High\n**Required Context:** Python testing, pytest, mocking, test data management\n**Validation Required:** Yes\n**Code Generation:** Supported\n\n**Dependencies:**\n- `../Core%20Principles.json.replace(\".json\", \".json\")` - Decision-making frameworks\n- `../Python%20Style%20Guide.json.replace(\".json\", \".json\")` - Python implementation patterns\n- `../Domain-Specific/Web%20Scraping%20Patterns.json.replace(\".json\", \".json\")` - File processing patterns\n- `../../project_context/Common%20Patterns.json.replace(\".json\", \".json\")` - Project-specific patterns\n- `../../project_context/Architecture%20Overview.json.replace(\".json\", \".json\")` - System architecture\n\n**Validation Rules:**\n- All tests must include proper error handling and edge cases\n- Test data must be isolated and not use production directories\n- Mocking must be used for external dependencies\n- Test organization must follow established patterns\n- Performance tests must include realistic measurements",
        "subsections": []
      },
      {
        "level": 2,
        "title": "Overview",
        "content": "**Document Purpose:** Comprehensive Python testing standards and best practices for the CreamPie project\n**Scope:** Unit testing, integration testing, test data management, and performance testing\n**Target Users:** AI assistants and developers implementing Python tests\n**Last Updated:** Current\n\n**AI Context:** This guide provides the foundational testing patterns that must be followed for all Python development in the project. It ensures comprehensive test coverage, proper isolation, and maintainable test suites.",
        "subsections": []
      },
      {
        "level": 2,
        "title": "1. Testing Fundamentals",
        "content": "",
        "subsections": []
      },
      {
        "level": 3,
        "title": "General Testing Guidelines",
        "content": "1. Write unit tests for all functions\n2. Use pytest for testing\n3. Use fixtures for test setup\n4. Use parametrize for multiple test cases\n5. Use mock for external dependencies\n6. Include both positive and negative test cases\n7. Mock external dependencies in tests\n\nThese guidelines will inform all test implementation throughout the project.\n\nAll tests must follow these guidelines and include comprehensive coverage.",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Test Directory Structure",
        "content": "```\ncream_api/\n├── tests/\n│   ├── conftest.py              # Shared test configuration\n│   ├── stock_data/\n│   │   ├── fixtures/           # Test-specific data files\n│   │   │   └── AAPL_2025-06-16.html\n│   │   ├── test_loader.py\n│   │   ├── test_parser.py\n│   │   └── test_constants.py\n│   ├── users/\n│   │   ├── test_auth.py\n│   │   └── test_models.py\n│   └── common/\n│       └── test_rate_limiting.py\n└── ...\n```\n\nThis structure will inform all test organization and file placement.\n\nAll test modules must follow this structure and naming conventions.",
        "subsections": []
      },
      {
        "level": 2,
        "title": "2. Test Data Management",
        "content": "",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Directory Isolation",
        "content": "- Production data directories (e.g., `raw_responses/`) should NEVER be used in tests\n- Each test module should have its own test-specific directories\n- Use temporary directories for file operations in tests\n\nThis isolation strategy will inform all test data management implementation.\n\nAll tests must use isolated test data and never reference production directories.",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Test Fixture Patterns",
        "content": "```python\nimport pytest\nimport os\nimport tempfile\nfrom pathlib import Path\nfrom unittest.mock import patch\nfrom typing import AsyncGenerator\n\n@pytest.fixture\ndef test_raw_responses_dir(tmp_path: Path) -> Path:\n    \"\"\"Create a temporary directory for test raw responses.\"\"\"\n    test_dir = tmp_path / \"test_raw_responses\"\n    test_dir.mkdir(exist_ok=True)\n    return test_dir\n\n@pytest.fixture\ndef sample_html_file(test_raw_responses_dir: Path) -> Path:\n    \"\"\"Create a sample HTML file for testing.\"\"\"\n    html_content = \"\"\"\n    <html>\n        <body>\n            <table class=\"stock-table\">\n                <tr><th>Symbol</th><th>Price</th></tr>\n                <tr><td>AAPL</td><td>150.00</td></tr>\n            </table>\n        </body>\n    </html>\n    \"\"\"\n    file_path = test_raw_responses_dir / \"test_stock_data.html\"\n    file_path.write_text(html_content)\n    return file_path\n\n@pytest.fixture\nasync def async_test_db():\n    \"\"\"Create async test database session.\"\"\"\n    from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine\n    from sqlalchemy.orm import sessionmaker\n\n    # Use in-memory SQLite for testing\n    engine = create_async_engine(\"sqlite+aiosqlite:///:memory:\", echo=False)\n    TestingSessionLocal = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)\n\n    async with engine.begin() as conn:\n        # Create tables\n        from cream_api.models import Base\n        await conn.run_sync(Base.metadata.create_all)\n\n    async with TestingSessionLocal() as session:\n        yield session\n\n    await engine.dispose()\n\n@pytest.fixture\ndef mock_config():\n    \"\"\"Mock configuration for testing.\"\"\"\n    with patch(\"cream_api.settings.app_settings\") as mock_settings:\n        mock_settings.HTML_RAW_RESPONSES_DIR = Path(\"/test/raw\")\n        mock_settings.HTML_PARSED_RESPONSES_DIR = Path(\"/test/parsed\")\n        mock_settings.DATABASE_URL = \"sqlite+aiosqlite:///:memory:\"\n        yield mock_settings\n```\n\nThese fixture patterns will inform all test setup and data management implementation.\n\nAll tests must use proper fixtures for data setup and cleanup.",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Patch Production Paths",
        "content": "```python\n@pytest.fixture\nasync def loader_with_mock_paths(session: AsyncSession, test_raw_responses_dir: Path) -> StockDataLoader:\n    \"\"\"Create loader with mocked production paths.\"\"\"\n    with patch(\"cream_api.settings.app_settings.HTML_RAW_RESPONSES_DIR\", test_raw_responses_dir):\n        with patch(\"cream_api.settings.app_settings.HTML_PARSED_RESPONSES_DIR\", test_raw_responses_dir / \"parsed\"):\n            return StockDataLoader(session)\n\n@pytest.fixture\ndef mock_external_api():\n    \"\"\"Mock external API calls.\"\"\"\n    with patch(\"cream_api.stock_data.retriever.requests.get\") as mock_get:\n        mock_get.return_value.status_code = 200\n        mock_get.return_value.text = \"<html>Mock response</html>\"\n        yield mock_get\n```\n\nThis patching pattern will inform all external dependency mocking implementation.\n\nAll external dependencies must be properly mocked in tests.",
        "subsections": []
      },
      {
        "level": 2,
        "title": "3. Test Organization Patterns",
        "content": "",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Test Class Structure",
        "content": "```python\nimport pytest\nfrom unittest.mock import Mock, patch\nfrom typing import List, Dict, Any\n\nclass TestStockDataProcessor:\n    \"\"\"Test suite for StockDataProcessor.\"\"\"\n\n    @pytest.fixture(autouse=True)\n    def setup(self, async_test_db, mock_config):\n        \"\"\"Setup test environment.\"\"\"\n        self.db = async_test_db\n        self.config = mock_config\n        self.processor = StockDataProcessor(self.db)\n\n    def test_process_valid_data(self, sample_html_file: Path):\n        \"\"\"Test processing valid HTML data.\"\"\"\n        # Arrange\n        expected_data = [\n            {\"symbol\": \"AAPL\", \"price\": \"150.00\"}\n        ]\n\n        # Act\n        result = self.processor.process_file(sample_html_file)\n\n        # Assert\n        assert result == expected_data\n        assert len(result) == 1\n\n    def test_process_invalid_file(self, test_raw_responses_dir: Path):\n        \"\"\"Test processing invalid file.\"\"\"\n        # Arrange\n        invalid_file = test_raw_responses_dir / \"invalid.txt\"\n        invalid_file.write_text(\"Not HTML content\")\n\n        # Act & Assert\n        with pytest.raises(ValueError, match=\"Invalid HTML file\"):\n            self.processor.process_file(invalid_file)\n\n    @pytest.mark.parametrize(\"html_content,expected_count\", [\n        (\"<table><tr><td>AAPL</td></tr></table>\", 1),\n        (\"<table><tr><td>AAPL</td></tr><tr><td>TSLA</td></tr></table>\", 2),\n        (\"<div>No table</div>\", 0),\n    ])\n    def test_process_various_content(self, html_content: str, expected_count: int, test_raw_responses_dir: Path):\n        \"\"\"Test processing various HTML content.\"\"\"\n        # Arrange\n        test_file = test_raw_responses_dir / \"test.html\"\n        test_file.write_text(html_content)\n\n        # Act\n        result = self.processor.process_file(test_file)\n\n        # Assert\n        assert len(result) == expected_count\n```\n\nThis test class pattern will inform all test organization and structure implementation.\n\nAll test suites must follow this structure with proper setup, teardown, and parametrization.",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Async Test Patterns",
        "content": "```python\nimport pytest\nimport asyncio\nfrom unittest.mock import AsyncMock\n\nclass TestAsyncStockDataProcessor:\n    \"\"\"Test suite for async stock data processing.\"\"\"\n\n    @pytest.fixture(autouse=True)\n    async def setup(self, async_test_db):\n        \"\"\"Setup async test environment.\"\"\"\n        self.db = async_test_db\n        self.processor = AsyncStockDataProcessor(self.db)\n\n    @pytest.mark.asyncio\n    async def test_async_process_files(self, sample_html_file: Path):\n        \"\"\"Test async file processing.\"\"\"\n        # Arrange\n        files = [sample_html_file]\n\n        # Act\n        results = await self.processor.process_files(files)\n\n        # Assert\n        assert len(results) == 1\n        assert results[0][\"success\"] is True\n\n    @pytest.mark.asyncio\n    async def test_async_process_with_error(self, test_raw_responses_dir: Path):\n        \"\"\"Test async processing with error handling.\"\"\"\n        # Arrange\n        invalid_file = test_raw_responses_dir / \"invalid.html\"\n        invalid_file.write_text(\"Invalid content\")\n\n        # Act\n        results = await self.processor.process_files([invalid_file])\n\n        # Assert\n        assert len(results) == 1\n        assert results[0][\"success\"] is False\n        assert \"error\" in results[0]\n\n    @pytest.mark.asyncio\n    async def test_concurrent_processing(self, sample_html_file: Path):\n        \"\"\"Test concurrent file processing.\"\"\"\n        # Arrange\n        files = [sample_html_file] * 5  # Process same file 5 times\n\n        # Act\n        start_time = asyncio.get_event_loop().time()\n        results = await self.processor.process_files_concurrent(files)\n        end_time = asyncio.get_event_loop().time()\n\n        # Assert\n        assert len(results) == 5\n        assert all(r[\"success\"] for r in results)\n        assert end_time - start_time < 2.0  # Should complete quickly\n```\n\nThis async test pattern will inform all asynchronous test implementation.\n\nAll async tests must use proper async fixtures and error handling.",
        "subsections": []
      },
      {
        "level": 2,
        "title": "4. Mocking Patterns",
        "content": "",
        "subsections": []
      },
      {
        "level": 3,
        "title": "External API Mocking",
        "content": "```python\nimport pytest\nfrom unittest.mock import Mock, patch, AsyncMock\nfrom httpx import AsyncClient\n\nclass TestExternalAPIIntegration:\n    \"\"\"Test external API integration.\"\"\"\n\n    @pytest.fixture\n    def mock_stock_api(self):\n        \"\"\"Mock stock API responses.\"\"\"\n        with patch(\"cream_api.stock_data.retriever.AsyncClient\") as mock_client:\n            mock_response = Mock()\n            mock_response.status_code = 200\n            mock_response.text = \"<html>Stock data</html>\"\n\n            mock_client_instance = AsyncMock()\n            mock_client_instance.get.return_value = mock_response\n            mock_client.return_value.__aenter__.return_value = mock_client_instance\n\n            yield mock_client_instance\n\n    @pytest.mark.asyncio\n    async def test_fetch_stock_data_success(self, mock_stock_api):\n        \"\"\"Test successful stock data fetching.\"\"\"\n        # Arrange\n        symbol = \"AAPL\"\n\n        # Act\n        result = await fetch_stock_data(symbol)\n\n        # Assert\n        assert result is not None\n        mock_stock_api.get.assert_called_once()\n\n    @pytest.mark.asyncio\n    async def test_fetch_stock_data_failure(self, mock_stock_api):\n        \"\"\"Test stock data fetching failure.\"\"\"\n        # Arrange\n        mock_stock_api.get.side_effect = Exception(\"API Error\")\n        symbol = \"INVALID\"\n\n        # Act & Assert\n        with pytest.raises(Exception, match=\"API Error\"):\n            await fetch_stock_data(symbol)\n```\n\nThis mocking pattern will inform all external API test implementation.\n\nAll external API tests must include proper mocking and error scenarios.",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Database Mocking",
        "content": "```python\nimport pytest\nfrom unittest.mock import Mock, patch\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nclass TestDatabaseOperations:\n    \"\"\"Test database operations.\"\"\"\n\n    @pytest.fixture\n    def mock_session(self):\n        \"\"\"Mock database session.\"\"\"\n        session = Mock(spec=AsyncSession)\n        session.commit = AsyncMock()\n        session.rollback = AsyncMock()\n        session.close = AsyncMock()\n        return session\n\n    @pytest.mark.asyncio\n    async def test_save_stock_data(self, mock_session):\n        \"\"\"Test saving stock data to database.\"\"\"\n        # Arrange\n        stock_data = {\"symbol\": \"AAPL\", \"price\": 150.00}\n\n        # Act\n        result = await save_stock_data(mock_session, stock_data)\n\n        # Assert\n        assert result is True\n        mock_session.add.assert_called_once()\n        mock_session.commit.assert_called_once()\n\n    @pytest.mark.asyncio\n    async def test_save_stock_data_error(self, mock_session):\n        \"\"\"Test database error handling.\"\"\"\n        # Arrange\n        mock_session.commit.side_effect = Exception(\"Database error\")\n        stock_data = {\"symbol\": \"AAPL\", \"price\": 150.00}\n\n        # Act & Assert\n        with pytest.raises(Exception, match=\"Database error\"):\n            await save_stock_data(mock_session, stock_data)\n\n        mock_session.rollback.assert_called_once()\n```\n\nThis database mocking pattern will inform all database test implementation.\n\nAll database tests must include proper session mocking and error handling.",
        "subsections": []
      },
      {
        "level": 2,
        "title": "5. Performance Testing",
        "content": "",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Performance Test Patterns",
        "content": "```python\nimport pytest\nimport time\nimport asyncio\nfrom typing import List\n\nclass TestPerformance:\n    \"\"\"Performance test suite.\"\"\"\n\n    # Performance constants\n    MAX_PROCESSING_TIME_SECONDS = 5.0\n    MAX_MEMORY_USAGE_MB = 100\n    MIN_THROUGHPUT_RECORDS_PER_SECOND = 10\n\n    def test_processing_performance(self, large_dataset: List[Dict]):\n        \"\"\"Test processing performance with large dataset.\"\"\"\n        # Arrange\n        start_time = time.time()\n\n        # Act\n        result = process_large_dataset(large_dataset)\n        end_time = time.time()\n\n        # Assert\n        processing_time = end_time - start_time\n        assert processing_time < self.MAX_PROCESSING_TIME_SECONDS, \\\n            f\"Processing took {processing_time}s, expected <{self.MAX_PROCESSING_TIME_SECONDS}s\"\n\n        assert len(result) == len(large_dataset)\n\n    @pytest.mark.asyncio\n    async def test_concurrent_processing_performance(self, sample_html_file: Path):\n        \"\"\"Test concurrent processing performance.\"\"\"\n        # Arrange\n        files = [sample_html_file] * 50  # Process 50 files concurrently\n        start_time = asyncio.get_event_loop().time()\n\n        # Act\n        results = await process_files_concurrent(files)\n        end_time = asyncio.get_event_loop().time()\n\n        # Assert\n        processing_time = end_time - start_time\n        throughput = len(files) / processing_time\n\n        assert throughput >= self.MIN_THROUGHPUT_RECORDS_PER_SECOND, \\\n            f\"Throughput {throughput:.2f} records/sec, expected >= {self.MIN_THROUGHPUT_RECORDS_PER_SECOND}\"\n\n        assert all(r[\"success\"] for r in results)\n\n    def test_memory_usage(self, large_dataset: List[Dict]):\n        \"\"\"Test memory usage during processing.\"\"\"\n        import psutil\n        import os\n\n        # Arrange\n        process = psutil.Process(os.getpid())\n        initial_memory = process.memory_info().rss / 1024 / 1024  # MB\n\n        # Act\n        result = process_large_dataset(large_dataset)\n        final_memory = process.memory_info().rss / 1024 / 1024  # MB\n\n        # Assert\n        memory_increase = final_memory - initial_memory\n        assert memory_increase < self.MAX_MEMORY_USAGE_MB, \\\n            f\"Memory increase {memory_increase:.2f}MB, expected <{self.MAX_MEMORY_USAGE_MB}MB\"\n```\n\nThis performance test pattern will inform all performance testing implementation.\n\nAll performance tests must include realistic measurements and clear assertions.",
        "subsections": []
      },
      {
        "level": 2,
        "title": "6. Integration Testing",
        "content": "",
        "subsections": []
      },
      {
        "level": 3,
        "title": "API Integration Testing",
        "content": "```python\nimport pytest\nfrom fastapi.testclient import TestClient\nfrom httpx import AsyncClient\n\nclass TestAPIEndpoints:\n    \"\"\"Test API endpoint integration.\"\"\"\n\n    @pytest.fixture\n    def client(self, test_app):\n        \"\"\"Create test client.\"\"\"\n        return TestClient(test_app)\n\n    def test_get_stock_data_endpoint(self, client: TestClient):\n        \"\"\"Test GET /stock-data endpoint.\"\"\"\n        # Act\n        response = client.get(\"/stock-data/AAPL\")\n\n        # Assert\n        assert response.status_code == 200\n        data = response.json()\n        assert \"symbol\" in data\n        assert \"price\" in data\n\n    def test_post_stock_tracking(self, client: TestClient):\n        \"\"\"Test POST /stock-data/track endpoint.\"\"\"\n        # Arrange\n        payload = {\"symbol\": \"TSLA\"}\n\n        # Act\n        response = client.post(\"/stock-data/track\", json=payload)\n\n        # Assert\n        assert response.status_code == 200\n        data = response.json()\n        assert data[\"symbol\"] == \"TSLA\"\n        assert data[\"is_active\"] is True\n\n    @pytest.mark.asyncio\n    async def test_async_api_endpoint(self, async_client: AsyncClient):\n        \"\"\"Test async API endpoint.\"\"\"\n        # Act\n        response = await async_client.get(\"/stock-data/AAPL\")\n\n        # Assert\n        assert response.status_code == 200\n        data = response.json()\n        assert \"symbol\" in data\n```\n\nThis integration test pattern will inform all API integration testing implementation.\n\nAll integration tests must include proper client setup and response validation.",
        "subsections": []
      },
      {
        "level": 2,
        "title": "7. Test Data Management",
        "content": "",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Test Data Factory Patterns",
        "content": "```python\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Any\nfrom datetime import datetime\n\n@dataclass\nclass TestStockData:\n    \"\"\"Test stock data factory.\"\"\"\n    symbol: str = \"AAPL\"\n    price: float = 150.00\n    volume: int = 1000000\n    date: datetime = None\n\n    def __post_init__(self):\n        if self.date is None:\n            self.date = datetime.utcnow()\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary.\"\"\"\n        return {\n            \"symbol\": self.symbol,\n            \"price\": self.price,\n            \"volume\": self.volume,\n            \"date\": self.date.isoformat()\n        }\n\n    @classmethod\n    def create_batch(cls, count: int) -> List[Dict[str, Any]]:\n        \"\"\"Create batch of test data.\"\"\"\n        symbols = [\"AAPL\", \"TSLA\", \"GOOGL\", \"MSFT\", \"AMZN\"]\n        return [\n            cls(symbol=symbols[i % len(symbols)], price=100 + i * 10).to_dict()\n            for i in range(count)\n        ]\n\n@pytest.fixture\ndef sample_stock_data() -> TestStockData:\n    \"\"\"Create sample stock data.\"\"\"\n    return TestStockData()\n\n@pytest.fixture\ndef large_dataset() -> List[Dict[str, Any]]:\n    \"\"\"Create large dataset for performance testing.\"\"\"\n    return TestStockData.create_batch(1000)\n```\n\nThis test data factory pattern will inform all test data creation implementation.\n\nAll test data must be created using proper factories and fixtures.",
        "subsections": []
      },
      {
        "level": 2,
        "title": "Implementation Guidelines",
        "content": "",
        "subsections": []
      },
      {
        "level": 3,
        "title": "For AI Assistants",
        "content": "1. **Follow these patterns** for all test implementation\n2. **Use proper fixtures** for test setup and teardown\n3. **Mock external dependencies** to ensure test isolation\n4. **Include comprehensive error cases** in test coverage\n5. **Use parametrization** for multiple test scenarios\n6. **Implement performance tests** for critical operations\n7. **Follow test organization** patterns for maintainability\n8. **Use proper assertions** with descriptive messages",
        "subsections": []
      },
      {
        "level": 3,
        "title": "For Human Developers",
        "content": "1. **Reference these patterns** when writing tests\n2. **Use isolated test data** and never reference production\n3. **Mock external dependencies** for reliable tests\n4. **Include both success and error cases** in test coverage\n5. **Use descriptive test names** and assertions\n6. **Follow established patterns** for consistency\n7. **Test performance** for critical operations",
        "subsections": []
      },
      {
        "level": 2,
        "title": "Quality Assurance",
        "content": "",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Test Coverage Standards",
        "content": "- All functions must have unit tests\n- All API endpoints must have integration tests\n- All error paths must be tested\n- Performance tests must be included for critical operations\n- Test data must be properly isolated",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Test Quality Standards",
        "content": "- Tests must be independent and repeatable\n- Test names must be descriptive and clear\n- Assertions must include descriptive messages\n- Test data must be minimal and focused\n- Error handling must be comprehensive",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Performance Standards",
        "content": "- Unit tests must complete in under 1 second\n- Integration tests must complete in under 5 seconds\n- Performance tests must include realistic measurements\n- Memory usage must be monitored and controlled\n- Concurrent operations must be properly tested",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Maintenance Standards",
        "content": "- Tests must be updated with code changes\n- Test data must be kept current and relevant\n- Mock responses must reflect actual API behavior\n- Performance benchmarks must be regularly updated\n- Test documentation must be maintained\n\n---\n\n**AI Quality Checklist**: Before implementing tests, ensure:\n- [x] Test data is properly isolated from production\n- [x] External dependencies are properly mocked\n- [x] Both success and error cases are covered\n- [x] Performance tests include realistic measurements\n- [x] Test organization follows established patterns\n- [x] Assertions include descriptive messages\n- [x] Test data factories are used for consistency\n- [x] Async tests use proper async patterns",
        "subsections": []
      }
    ],
    "code_blocks": [
      {
        "language": "text",
        "code": "cream_api/\n├── tests/\n│   ├── conftest.py              # Shared test configuration\n│   ├── stock_data/\n│   │   ├── fixtures/           # Test-specific data files\n│   │   │   └── AAPL_2025-06-16.html\n│   │   ├── test_loader.py\n│   │   ├── test_parser.py\n│   │   └── test_constants.py\n│   ├── users/\n│   │   ├── test_auth.py\n│   │   └── test_models.py\n│   └── common/\n│       └── test_rate_limiting.py\n└── ..."
      },
      {
        "language": "python",
        "code": "import pytest\nimport os\nimport tempfile\nfrom pathlib import Path\nfrom unittest.mock import patch\nfrom typing import AsyncGenerator\n\n@pytest.fixture\ndef test_raw_responses_dir(tmp_path: Path) -> Path:\n    \"\"\"Create a temporary directory for test raw responses.\"\"\"\n    test_dir = tmp_path / \"test_raw_responses\"\n    test_dir.mkdir(exist_ok=True)\n    return test_dir\n\n@pytest.fixture\ndef sample_html_file(test_raw_responses_dir: Path) -> Path:\n    \"\"\"Create a sample HTML file for testing.\"\"\"\n    html_content = \"\"\"\n    <html>\n        <body>\n            <table class=\"stock-table\">\n                <tr><th>Symbol</th><th>Price</th></tr>\n                <tr><td>AAPL</td><td>150.00</td></tr>\n            </table>\n        </body>\n    </html>\n    \"\"\"\n    file_path = test_raw_responses_dir / \"test_stock_data.html\"\n    file_path.write_text(html_content)\n    return file_path\n\n@pytest.fixture\nasync def async_test_db():\n    \"\"\"Create async test database session.\"\"\"\n    from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine\n    from sqlalchemy.orm import sessionmaker\n\n    # Use in-memory SQLite for testing\n    engine = create_async_engine(\"sqlite+aiosqlite:///:memory:\", echo=False)\n    TestingSessionLocal = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)\n\n    async with engine.begin() as conn:\n        # Create tables\n        from cream_api.models import Base\n        await conn.run_sync(Base.metadata.create_all)\n\n    async with TestingSessionLocal() as session:\n        yield session\n\n    await engine.dispose()\n\n@pytest.fixture\ndef mock_config():\n    \"\"\"Mock configuration for testing.\"\"\"\n    with patch(\"cream_api.settings.app_settings\") as mock_settings:\n        mock_settings.HTML_RAW_RESPONSES_DIR = Path(\"/test/raw\")\n        mock_settings.HTML_PARSED_RESPONSES_DIR = Path(\"/test/parsed\")\n        mock_settings.DATABASE_URL = \"sqlite+aiosqlite:///:memory:\"\n        yield mock_settings"
      },
      {
        "language": "python",
        "code": "@pytest.fixture\nasync def loader_with_mock_paths(session: AsyncSession, test_raw_responses_dir: Path) -> StockDataLoader:\n    \"\"\"Create loader with mocked production paths.\"\"\"\n    with patch(\"cream_api.settings.app_settings.HTML_RAW_RESPONSES_DIR\", test_raw_responses_dir):\n        with patch(\"cream_api.settings.app_settings.HTML_PARSED_RESPONSES_DIR\", test_raw_responses_dir / \"parsed\"):\n            return StockDataLoader(session)\n\n@pytest.fixture\ndef mock_external_api():\n    \"\"\"Mock external API calls.\"\"\"\n    with patch(\"cream_api.stock_data.retriever.requests.get\") as mock_get:\n        mock_get.return_value.status_code = 200\n        mock_get.return_value.text = \"<html>Mock response</html>\"\n        yield mock_get"
      },
      {
        "language": "python",
        "code": "import pytest\nfrom unittest.mock import Mock, patch\nfrom typing import List, Dict, Any\n\nclass TestStockDataProcessor:\n    \"\"\"Test suite for StockDataProcessor.\"\"\"\n\n    @pytest.fixture(autouse=True)\n    def setup(self, async_test_db, mock_config):\n        \"\"\"Setup test environment.\"\"\"\n        self.db = async_test_db\n        self.config = mock_config\n        self.processor = StockDataProcessor(self.db)\n\n    def test_process_valid_data(self, sample_html_file: Path):\n        \"\"\"Test processing valid HTML data.\"\"\"\n        # Arrange\n        expected_data = [\n            {\"symbol\": \"AAPL\", \"price\": \"150.00\"}\n        ]\n\n        # Act\n        result = self.processor.process_file(sample_html_file)\n\n        # Assert\n        assert result == expected_data\n        assert len(result) == 1\n\n    def test_process_invalid_file(self, test_raw_responses_dir: Path):\n        \"\"\"Test processing invalid file.\"\"\"\n        # Arrange\n        invalid_file = test_raw_responses_dir / \"invalid.txt\"\n        invalid_file.write_text(\"Not HTML content\")\n\n        # Act & Assert\n        with pytest.raises(ValueError, match=\"Invalid HTML file\"):\n            self.processor.process_file(invalid_file)\n\n    @pytest.mark.parametrize(\"html_content,expected_count\", [\n        (\"<table><tr><td>AAPL</td></tr></table>\", 1),\n        (\"<table><tr><td>AAPL</td></tr><tr><td>TSLA</td></tr></table>\", 2),\n        (\"<div>No table</div>\", 0),\n    ])\n    def test_process_various_content(self, html_content: str, expected_count: int, test_raw_responses_dir: Path):\n        \"\"\"Test processing various HTML content.\"\"\"\n        # Arrange\n        test_file = test_raw_responses_dir / \"test.html\"\n        test_file.write_text(html_content)\n\n        # Act\n        result = self.processor.process_file(test_file)\n\n        # Assert\n        assert len(result) == expected_count"
      },
      {
        "language": "python",
        "code": "import pytest\nimport asyncio\nfrom unittest.mock import AsyncMock\n\nclass TestAsyncStockDataProcessor:\n    \"\"\"Test suite for async stock data processing.\"\"\"\n\n    @pytest.fixture(autouse=True)\n    async def setup(self, async_test_db):\n        \"\"\"Setup async test environment.\"\"\"\n        self.db = async_test_db\n        self.processor = AsyncStockDataProcessor(self.db)\n\n    @pytest.mark.asyncio\n    async def test_async_process_files(self, sample_html_file: Path):\n        \"\"\"Test async file processing.\"\"\"\n        # Arrange\n        files = [sample_html_file]\n\n        # Act\n        results = await self.processor.process_files(files)\n\n        # Assert\n        assert len(results) == 1\n        assert results[0][\"success\"] is True\n\n    @pytest.mark.asyncio\n    async def test_async_process_with_error(self, test_raw_responses_dir: Path):\n        \"\"\"Test async processing with error handling.\"\"\"\n        # Arrange\n        invalid_file = test_raw_responses_dir / \"invalid.html\"\n        invalid_file.write_text(\"Invalid content\")\n\n        # Act\n        results = await self.processor.process_files([invalid_file])\n\n        # Assert\n        assert len(results) == 1\n        assert results[0][\"success\"] is False\n        assert \"error\" in results[0]\n\n    @pytest.mark.asyncio\n    async def test_concurrent_processing(self, sample_html_file: Path):\n        \"\"\"Test concurrent file processing.\"\"\"\n        # Arrange\n        files = [sample_html_file] * 5  # Process same file 5 times\n\n        # Act\n        start_time = asyncio.get_event_loop().time()\n        results = await self.processor.process_files_concurrent(files)\n        end_time = asyncio.get_event_loop().time()\n\n        # Assert\n        assert len(results) == 5\n        assert all(r[\"success\"] for r in results)\n        assert end_time - start_time < 2.0  # Should complete quickly"
      },
      {
        "language": "python",
        "code": "import pytest\nfrom unittest.mock import Mock, patch, AsyncMock\nfrom httpx import AsyncClient\n\nclass TestExternalAPIIntegration:\n    \"\"\"Test external API integration.\"\"\"\n\n    @pytest.fixture\n    def mock_stock_api(self):\n        \"\"\"Mock stock API responses.\"\"\"\n        with patch(\"cream_api.stock_data.retriever.AsyncClient\") as mock_client:\n            mock_response = Mock()\n            mock_response.status_code = 200\n            mock_response.text = \"<html>Stock data</html>\"\n\n            mock_client_instance = AsyncMock()\n            mock_client_instance.get.return_value = mock_response\n            mock_client.return_value.__aenter__.return_value = mock_client_instance\n\n            yield mock_client_instance\n\n    @pytest.mark.asyncio\n    async def test_fetch_stock_data_success(self, mock_stock_api):\n        \"\"\"Test successful stock data fetching.\"\"\"\n        # Arrange\n        symbol = \"AAPL\"\n\n        # Act\n        result = await fetch_stock_data(symbol)\n\n        # Assert\n        assert result is not None\n        mock_stock_api.get.assert_called_once()\n\n    @pytest.mark.asyncio\n    async def test_fetch_stock_data_failure(self, mock_stock_api):\n        \"\"\"Test stock data fetching failure.\"\"\"\n        # Arrange\n        mock_stock_api.get.side_effect = Exception(\"API Error\")\n        symbol = \"INVALID\"\n\n        # Act & Assert\n        with pytest.raises(Exception, match=\"API Error\"):\n            await fetch_stock_data(symbol)"
      },
      {
        "language": "python",
        "code": "import pytest\nfrom unittest.mock import Mock, patch\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nclass TestDatabaseOperations:\n    \"\"\"Test database operations.\"\"\"\n\n    @pytest.fixture\n    def mock_session(self):\n        \"\"\"Mock database session.\"\"\"\n        session = Mock(spec=AsyncSession)\n        session.commit = AsyncMock()\n        session.rollback = AsyncMock()\n        session.close = AsyncMock()\n        return session\n\n    @pytest.mark.asyncio\n    async def test_save_stock_data(self, mock_session):\n        \"\"\"Test saving stock data to database.\"\"\"\n        # Arrange\n        stock_data = {\"symbol\": \"AAPL\", \"price\": 150.00}\n\n        # Act\n        result = await save_stock_data(mock_session, stock_data)\n\n        # Assert\n        assert result is True\n        mock_session.add.assert_called_once()\n        mock_session.commit.assert_called_once()\n\n    @pytest.mark.asyncio\n    async def test_save_stock_data_error(self, mock_session):\n        \"\"\"Test database error handling.\"\"\"\n        # Arrange\n        mock_session.commit.side_effect = Exception(\"Database error\")\n        stock_data = {\"symbol\": \"AAPL\", \"price\": 150.00}\n\n        # Act & Assert\n        with pytest.raises(Exception, match=\"Database error\"):\n            await save_stock_data(mock_session, stock_data)\n\n        mock_session.rollback.assert_called_once()"
      },
      {
        "language": "python",
        "code": "import pytest\nimport time\nimport asyncio\nfrom typing import List\n\nclass TestPerformance:\n    \"\"\"Performance test suite.\"\"\"\n\n    # Performance constants\n    MAX_PROCESSING_TIME_SECONDS = 5.0\n    MAX_MEMORY_USAGE_MB = 100\n    MIN_THROUGHPUT_RECORDS_PER_SECOND = 10\n\n    def test_processing_performance(self, large_dataset: List[Dict]):\n        \"\"\"Test processing performance with large dataset.\"\"\"\n        # Arrange\n        start_time = time.time()\n\n        # Act\n        result = process_large_dataset(large_dataset)\n        end_time = time.time()\n\n        # Assert\n        processing_time = end_time - start_time\n        assert processing_time < self.MAX_PROCESSING_TIME_SECONDS, \\\n            f\"Processing took {processing_time}s, expected <{self.MAX_PROCESSING_TIME_SECONDS}s\"\n\n        assert len(result) == len(large_dataset)\n\n    @pytest.mark.asyncio\n    async def test_concurrent_processing_performance(self, sample_html_file: Path):\n        \"\"\"Test concurrent processing performance.\"\"\"\n        # Arrange\n        files = [sample_html_file] * 50  # Process 50 files concurrently\n        start_time = asyncio.get_event_loop().time()\n\n        # Act\n        results = await process_files_concurrent(files)\n        end_time = asyncio.get_event_loop().time()\n\n        # Assert\n        processing_time = end_time - start_time\n        throughput = len(files) / processing_time\n\n        assert throughput >= self.MIN_THROUGHPUT_RECORDS_PER_SECOND, \\\n            f\"Throughput {throughput:.2f} records/sec, expected >= {self.MIN_THROUGHPUT_RECORDS_PER_SECOND}\"\n\n        assert all(r[\"success\"] for r in results)\n\n    def test_memory_usage(self, large_dataset: List[Dict]):\n        \"\"\"Test memory usage during processing.\"\"\"\n        import psutil\n        import os\n\n        # Arrange\n        process = psutil.Process(os.getpid())\n        initial_memory = process.memory_info().rss / 1024 / 1024  # MB\n\n        # Act\n        result = process_large_dataset(large_dataset)\n        final_memory = process.memory_info().rss / 1024 / 1024  # MB\n\n        # Assert\n        memory_increase = final_memory - initial_memory\n        assert memory_increase < self.MAX_MEMORY_USAGE_MB, \\\n            f\"Memory increase {memory_increase:.2f}MB, expected <{self.MAX_MEMORY_USAGE_MB}MB\""
      },
      {
        "language": "python",
        "code": "import pytest\nfrom fastapi.testclient import TestClient\nfrom httpx import AsyncClient\n\nclass TestAPIEndpoints:\n    \"\"\"Test API endpoint integration.\"\"\"\n\n    @pytest.fixture\n    def client(self, test_app):\n        \"\"\"Create test client.\"\"\"\n        return TestClient(test_app)\n\n    def test_get_stock_data_endpoint(self, client: TestClient):\n        \"\"\"Test GET /stock-data endpoint.\"\"\"\n        # Act\n        response = client.get(\"/stock-data/AAPL\")\n\n        # Assert\n        assert response.status_code == 200\n        data = response.json()\n        assert \"symbol\" in data\n        assert \"price\" in data\n\n    def test_post_stock_tracking(self, client: TestClient):\n        \"\"\"Test POST /stock-data/track endpoint.\"\"\"\n        # Arrange\n        payload = {\"symbol\": \"TSLA\"}\n\n        # Act\n        response = client.post(\"/stock-data/track\", json=payload)\n\n        # Assert\n        assert response.status_code == 200\n        data = response.json()\n        assert data[\"symbol\"] == \"TSLA\"\n        assert data[\"is_active\"] is True\n\n    @pytest.mark.asyncio\n    async def test_async_api_endpoint(self, async_client: AsyncClient):\n        \"\"\"Test async API endpoint.\"\"\"\n        # Act\n        response = await async_client.get(\"/stock-data/AAPL\")\n\n        # Assert\n        assert response.status_code == 200\n        data = response.json()\n        assert \"symbol\" in data"
      },
      {
        "language": "python",
        "code": "from dataclasses import dataclass\nfrom typing import List, Dict, Any\nfrom datetime import datetime\n\n@dataclass\nclass TestStockData:\n    \"\"\"Test stock data factory.\"\"\"\n    symbol: str = \"AAPL\"\n    price: float = 150.00\n    volume: int = 1000000\n    date: datetime = None\n\n    def __post_init__(self):\n        if self.date is None:\n            self.date = datetime.utcnow()\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary.\"\"\"\n        return {\n            \"symbol\": self.symbol,\n            \"price\": self.price,\n            \"volume\": self.volume,\n            \"date\": self.date.isoformat()\n        }\n\n    @classmethod\n    def create_batch(cls, count: int) -> List[Dict[str, Any]]:\n        \"\"\"Create batch of test data.\"\"\"\n        symbols = [\"AAPL\", \"TSLA\", \"GOOGL\", \"MSFT\", \"AMZN\"]\n        return [\n            cls(symbol=symbols[i % len(symbols)], price=100 + i * 10).to_dict()\n            for i in range(count)\n        ]\n\n@pytest.fixture\ndef sample_stock_data() -> TestStockData:\n    \"\"\"Create sample stock data.\"\"\"\n    return TestStockData()\n\n@pytest.fixture\ndef large_dataset() -> List[Dict[str, Any]]:\n    \"\"\"Create large dataset for performance testing.\"\"\"\n    return TestStockData.create_batch(1000)"
      }
    ],
    "links": [
      {
        "type": "code_reference",
        "text": "../Core%20Principles.md"
      },
      {
        "type": "code_reference",
        "text": "../Python%20Style%20Guide.md"
      },
      {
        "type": "code_reference",
        "text": "../Domain-Specific/Web%20Scraping%20Patterns.md"
      },
      {
        "type": "code_reference",
        "text": "../../project_context/Common%20Patterns.md"
      },
      {
        "type": "code_reference",
        "text": "../../project_context/Architecture%20Overview.md"
      },
      {
        "type": "code_reference",
        "text": "\ncream_api/\n├── tests/\n│   ├── conftest.py              # Shared test configuration\n│   ├── stock_data/\n│   │   ├── fixtures/           # Test-specific data files\n│   │   │   └── AAPL_2025-06-16.html\n│   │   ├── test_loader.py\n│   │   ├── test_parser.py\n│   │   └── test_constants.py\n│   ├── users/\n│   │   ├── test_auth.py\n│   │   └── test_models.py\n│   └── common/\n│       └── test_rate_limiting.py\n└── ...\n"
      },
      {
        "type": "code_reference",
        "text": "\n\nThis structure will inform all test organization and file placement.\n\nAll test modules must follow this structure and naming conventions.\n\n## 2. Test Data Management\n\n### Directory Isolation\n- Production data directories (e.g., "
      },
      {
        "type": "code_reference",
        "text": ") should NEVER be used in tests\n- Each test module should have its own test-specific directories\n- Use temporary directories for file operations in tests\n\nThis isolation strategy will inform all test data management implementation.\n\nAll tests must use isolated test data and never reference production directories.\n\n### Test Fixture Patterns\n"
      },
      {
        "type": "code_reference",
        "text": "python\nimport pytest\nimport os\nimport tempfile\nfrom pathlib import Path\nfrom unittest.mock import patch\nfrom typing import AsyncGenerator\n\n@pytest.fixture\ndef test_raw_responses_dir(tmp_path: Path) -> Path:\n    \"\"\"Create a temporary directory for test raw responses.\"\"\"\n    test_dir = tmp_path / \"test_raw_responses\"\n    test_dir.mkdir(exist_ok=True)\n    return test_dir\n\n@pytest.fixture\ndef sample_html_file(test_raw_responses_dir: Path) -> Path:\n    \"\"\"Create a sample HTML file for testing.\"\"\"\n    html_content = \"\"\"\n    <html>\n        <body>\n            <table class=\"stock-table\">\n                <tr><th>Symbol</th><th>Price</th></tr>\n                <tr><td>AAPL</td><td>150.00</td></tr>\n            </table>\n        </body>\n    </html>\n    \"\"\"\n    file_path = test_raw_responses_dir / \"test_stock_data.html\"\n    file_path.write_text(html_content)\n    return file_path\n\n@pytest.fixture\nasync def async_test_db():\n    \"\"\"Create async test database session.\"\"\"\n    from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine\n    from sqlalchemy.orm import sessionmaker\n\n    # Use in-memory SQLite for testing\n    engine = create_async_engine(\"sqlite+aiosqlite:///:memory:\", echo=False)\n    TestingSessionLocal = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)\n\n    async with engine.begin() as conn:\n        # Create tables\n        from cream_api.models import Base\n        await conn.run_sync(Base.metadata.create_all)\n\n    async with TestingSessionLocal() as session:\n        yield session\n\n    await engine.dispose()\n\n@pytest.fixture\ndef mock_config():\n    \"\"\"Mock configuration for testing.\"\"\"\n    with patch(\"cream_api.settings.app_settings\") as mock_settings:\n        mock_settings.HTML_RAW_RESPONSES_DIR = Path(\"/test/raw\")\n        mock_settings.HTML_PARSED_RESPONSES_DIR = Path(\"/test/parsed\")\n        mock_settings.DATABASE_URL = \"sqlite+aiosqlite:///:memory:\"\n        yield mock_settings\n"
      },
      {
        "type": "code_reference",
        "text": "\n\nThese fixture patterns will inform all test setup and data management implementation.\n\nAll tests must use proper fixtures for data setup and cleanup.\n\n### Patch Production Paths\n"
      },
      {
        "type": "code_reference",
        "text": "python\n@pytest.fixture\nasync def loader_with_mock_paths(session: AsyncSession, test_raw_responses_dir: Path) -> StockDataLoader:\n    \"\"\"Create loader with mocked production paths.\"\"\"\n    with patch(\"cream_api.settings.app_settings.HTML_RAW_RESPONSES_DIR\", test_raw_responses_dir):\n        with patch(\"cream_api.settings.app_settings.HTML_PARSED_RESPONSES_DIR\", test_raw_responses_dir / \"parsed\"):\n            return StockDataLoader(session)\n\n@pytest.fixture\ndef mock_external_api():\n    \"\"\"Mock external API calls.\"\"\"\n    with patch(\"cream_api.stock_data.retriever.requests.get\") as mock_get:\n        mock_get.return_value.status_code = 200\n        mock_get.return_value.text = \"<html>Mock response</html>\"\n        yield mock_get\n"
      },
      {
        "type": "code_reference",
        "text": "\n\nThis patching pattern will inform all external dependency mocking implementation.\n\nAll external dependencies must be properly mocked in tests.\n\n## 3. Test Organization Patterns\n\n### Test Class Structure\n"
      },
      {
        "type": "code_reference",
        "text": "python\nimport pytest\nfrom unittest.mock import Mock, patch\nfrom typing import List, Dict, Any\n\nclass TestStockDataProcessor:\n    \"\"\"Test suite for StockDataProcessor.\"\"\"\n\n    @pytest.fixture(autouse=True)\n    def setup(self, async_test_db, mock_config):\n        \"\"\"Setup test environment.\"\"\"\n        self.db = async_test_db\n        self.config = mock_config\n        self.processor = StockDataProcessor(self.db)\n\n    def test_process_valid_data(self, sample_html_file: Path):\n        \"\"\"Test processing valid HTML data.\"\"\"\n        # Arrange\n        expected_data = [\n            {\"symbol\": \"AAPL\", \"price\": \"150.00\"}\n        ]\n\n        # Act\n        result = self.processor.process_file(sample_html_file)\n\n        # Assert\n        assert result == expected_data\n        assert len(result) == 1\n\n    def test_process_invalid_file(self, test_raw_responses_dir: Path):\n        \"\"\"Test processing invalid file.\"\"\"\n        # Arrange\n        invalid_file = test_raw_responses_dir / \"invalid.txt\"\n        invalid_file.write_text(\"Not HTML content\")\n\n        # Act & Assert\n        with pytest.raises(ValueError, match=\"Invalid HTML file\"):\n            self.processor.process_file(invalid_file)\n\n    @pytest.mark.parametrize(\"html_content,expected_count\", [\n        (\"<table><tr><td>AAPL</td></tr></table>\", 1),\n        (\"<table><tr><td>AAPL</td></tr><tr><td>TSLA</td></tr></table>\", 2),\n        (\"<div>No table</div>\", 0),\n    ])\n    def test_process_various_content(self, html_content: str, expected_count: int, test_raw_responses_dir: Path):\n        \"\"\"Test processing various HTML content.\"\"\"\n        # Arrange\n        test_file = test_raw_responses_dir / \"test.html\"\n        test_file.write_text(html_content)\n\n        # Act\n        result = self.processor.process_file(test_file)\n\n        # Assert\n        assert len(result) == expected_count\n"
      },
      {
        "type": "code_reference",
        "text": "\n\nThis test class pattern will inform all test organization and structure implementation.\n\nAll test suites must follow this structure with proper setup, teardown, and parametrization.\n\n### Async Test Patterns\n"
      },
      {
        "type": "code_reference",
        "text": "python\nimport pytest\nimport asyncio\nfrom unittest.mock import AsyncMock\n\nclass TestAsyncStockDataProcessor:\n    \"\"\"Test suite for async stock data processing.\"\"\"\n\n    @pytest.fixture(autouse=True)\n    async def setup(self, async_test_db):\n        \"\"\"Setup async test environment.\"\"\"\n        self.db = async_test_db\n        self.processor = AsyncStockDataProcessor(self.db)\n\n    @pytest.mark.asyncio\n    async def test_async_process_files(self, sample_html_file: Path):\n        \"\"\"Test async file processing.\"\"\"\n        # Arrange\n        files = [sample_html_file]\n\n        # Act\n        results = await self.processor.process_files(files)\n\n        # Assert\n        assert len(results) == 1\n        assert results[0][\"success\"] is True\n\n    @pytest.mark.asyncio\n    async def test_async_process_with_error(self, test_raw_responses_dir: Path):\n        \"\"\"Test async processing with error handling.\"\"\"\n        # Arrange\n        invalid_file = test_raw_responses_dir / \"invalid.html\"\n        invalid_file.write_text(\"Invalid content\")\n\n        # Act\n        results = await self.processor.process_files([invalid_file])\n\n        # Assert\n        assert len(results) == 1\n        assert results[0][\"success\"] is False\n        assert \"error\" in results[0]\n\n    @pytest.mark.asyncio\n    async def test_concurrent_processing(self, sample_html_file: Path):\n        \"\"\"Test concurrent file processing.\"\"\"\n        # Arrange\n        files = [sample_html_file] * 5  # Process same file 5 times\n\n        # Act\n        start_time = asyncio.get_event_loop().time()\n        results = await self.processor.process_files_concurrent(files)\n        end_time = asyncio.get_event_loop().time()\n\n        # Assert\n        assert len(results) == 5\n        assert all(r[\"success\"] for r in results)\n        assert end_time - start_time < 2.0  # Should complete quickly\n"
      },
      {
        "type": "code_reference",
        "text": "\n\nThis async test pattern will inform all asynchronous test implementation.\n\nAll async tests must use proper async fixtures and error handling.\n\n## 4. Mocking Patterns\n\n### External API Mocking\n"
      },
      {
        "type": "code_reference",
        "text": "python\nimport pytest\nfrom unittest.mock import Mock, patch, AsyncMock\nfrom httpx import AsyncClient\n\nclass TestExternalAPIIntegration:\n    \"\"\"Test external API integration.\"\"\"\n\n    @pytest.fixture\n    def mock_stock_api(self):\n        \"\"\"Mock stock API responses.\"\"\"\n        with patch(\"cream_api.stock_data.retriever.AsyncClient\") as mock_client:\n            mock_response = Mock()\n            mock_response.status_code = 200\n            mock_response.text = \"<html>Stock data</html>\"\n\n            mock_client_instance = AsyncMock()\n            mock_client_instance.get.return_value = mock_response\n            mock_client.return_value.__aenter__.return_value = mock_client_instance\n\n            yield mock_client_instance\n\n    @pytest.mark.asyncio\n    async def test_fetch_stock_data_success(self, mock_stock_api):\n        \"\"\"Test successful stock data fetching.\"\"\"\n        # Arrange\n        symbol = \"AAPL\"\n\n        # Act\n        result = await fetch_stock_data(symbol)\n\n        # Assert\n        assert result is not None\n        mock_stock_api.get.assert_called_once()\n\n    @pytest.mark.asyncio\n    async def test_fetch_stock_data_failure(self, mock_stock_api):\n        \"\"\"Test stock data fetching failure.\"\"\"\n        # Arrange\n        mock_stock_api.get.side_effect = Exception(\"API Error\")\n        symbol = \"INVALID\"\n\n        # Act & Assert\n        with pytest.raises(Exception, match=\"API Error\"):\n            await fetch_stock_data(symbol)\n"
      },
      {
        "type": "code_reference",
        "text": "\n\nThis mocking pattern will inform all external API test implementation.\n\nAll external API tests must include proper mocking and error scenarios.\n\n### Database Mocking\n"
      },
      {
        "type": "code_reference",
        "text": "python\nimport pytest\nfrom unittest.mock import Mock, patch\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nclass TestDatabaseOperations:\n    \"\"\"Test database operations.\"\"\"\n\n    @pytest.fixture\n    def mock_session(self):\n        \"\"\"Mock database session.\"\"\"\n        session = Mock(spec=AsyncSession)\n        session.commit = AsyncMock()\n        session.rollback = AsyncMock()\n        session.close = AsyncMock()\n        return session\n\n    @pytest.mark.asyncio\n    async def test_save_stock_data(self, mock_session):\n        \"\"\"Test saving stock data to database.\"\"\"\n        # Arrange\n        stock_data = {\"symbol\": \"AAPL\", \"price\": 150.00}\n\n        # Act\n        result = await save_stock_data(mock_session, stock_data)\n\n        # Assert\n        assert result is True\n        mock_session.add.assert_called_once()\n        mock_session.commit.assert_called_once()\n\n    @pytest.mark.asyncio\n    async def test_save_stock_data_error(self, mock_session):\n        \"\"\"Test database error handling.\"\"\"\n        # Arrange\n        mock_session.commit.side_effect = Exception(\"Database error\")\n        stock_data = {\"symbol\": \"AAPL\", \"price\": 150.00}\n\n        # Act & Assert\n        with pytest.raises(Exception, match=\"Database error\"):\n            await save_stock_data(mock_session, stock_data)\n\n        mock_session.rollback.assert_called_once()\n"
      },
      {
        "type": "code_reference",
        "text": "\n\nThis database mocking pattern will inform all database test implementation.\n\nAll database tests must include proper session mocking and error handling.\n\n## 5. Performance Testing\n\n### Performance Test Patterns\n"
      },
      {
        "type": "code_reference",
        "text": "python\nimport pytest\nimport time\nimport asyncio\nfrom typing import List\n\nclass TestPerformance:\n    \"\"\"Performance test suite.\"\"\"\n\n    # Performance constants\n    MAX_PROCESSING_TIME_SECONDS = 5.0\n    MAX_MEMORY_USAGE_MB = 100\n    MIN_THROUGHPUT_RECORDS_PER_SECOND = 10\n\n    def test_processing_performance(self, large_dataset: List[Dict]):\n        \"\"\"Test processing performance with large dataset.\"\"\"\n        # Arrange\n        start_time = time.time()\n\n        # Act\n        result = process_large_dataset(large_dataset)\n        end_time = time.time()\n\n        # Assert\n        processing_time = end_time - start_time\n        assert processing_time < self.MAX_PROCESSING_TIME_SECONDS, \\\n            f\"Processing took {processing_time}s, expected <{self.MAX_PROCESSING_TIME_SECONDS}s\"\n\n        assert len(result) == len(large_dataset)\n\n    @pytest.mark.asyncio\n    async def test_concurrent_processing_performance(self, sample_html_file: Path):\n        \"\"\"Test concurrent processing performance.\"\"\"\n        # Arrange\n        files = [sample_html_file] * 50  # Process 50 files concurrently\n        start_time = asyncio.get_event_loop().time()\n\n        # Act\n        results = await process_files_concurrent(files)\n        end_time = asyncio.get_event_loop().time()\n\n        # Assert\n        processing_time = end_time - start_time\n        throughput = len(files) / processing_time\n\n        assert throughput >= self.MIN_THROUGHPUT_RECORDS_PER_SECOND, \\\n            f\"Throughput {throughput:.2f} records/sec, expected >= {self.MIN_THROUGHPUT_RECORDS_PER_SECOND}\"\n\n        assert all(r[\"success\"] for r in results)\n\n    def test_memory_usage(self, large_dataset: List[Dict]):\n        \"\"\"Test memory usage during processing.\"\"\"\n        import psutil\n        import os\n\n        # Arrange\n        process = psutil.Process(os.getpid())\n        initial_memory = process.memory_info().rss / 1024 / 1024  # MB\n\n        # Act\n        result = process_large_dataset(large_dataset)\n        final_memory = process.memory_info().rss / 1024 / 1024  # MB\n\n        # Assert\n        memory_increase = final_memory - initial_memory\n        assert memory_increase < self.MAX_MEMORY_USAGE_MB, \\\n            f\"Memory increase {memory_increase:.2f}MB, expected <{self.MAX_MEMORY_USAGE_MB}MB\"\n"
      },
      {
        "type": "code_reference",
        "text": "\n\nThis performance test pattern will inform all performance testing implementation.\n\nAll performance tests must include realistic measurements and clear assertions.\n\n## 6. Integration Testing\n\n### API Integration Testing\n"
      },
      {
        "type": "code_reference",
        "text": "python\nimport pytest\nfrom fastapi.testclient import TestClient\nfrom httpx import AsyncClient\n\nclass TestAPIEndpoints:\n    \"\"\"Test API endpoint integration.\"\"\"\n\n    @pytest.fixture\n    def client(self, test_app):\n        \"\"\"Create test client.\"\"\"\n        return TestClient(test_app)\n\n    def test_get_stock_data_endpoint(self, client: TestClient):\n        \"\"\"Test GET /stock-data endpoint.\"\"\"\n        # Act\n        response = client.get(\"/stock-data/AAPL\")\n\n        # Assert\n        assert response.status_code == 200\n        data = response.json()\n        assert \"symbol\" in data\n        assert \"price\" in data\n\n    def test_post_stock_tracking(self, client: TestClient):\n        \"\"\"Test POST /stock-data/track endpoint.\"\"\"\n        # Arrange\n        payload = {\"symbol\": \"TSLA\"}\n\n        # Act\n        response = client.post(\"/stock-data/track\", json=payload)\n\n        # Assert\n        assert response.status_code == 200\n        data = response.json()\n        assert data[\"symbol\"] == \"TSLA\"\n        assert data[\"is_active\"] is True\n\n    @pytest.mark.asyncio\n    async def test_async_api_endpoint(self, async_client: AsyncClient):\n        \"\"\"Test async API endpoint.\"\"\"\n        # Act\n        response = await async_client.get(\"/stock-data/AAPL\")\n\n        # Assert\n        assert response.status_code == 200\n        data = response.json()\n        assert \"symbol\" in data\n"
      },
      {
        "type": "code_reference",
        "text": "\n\nThis integration test pattern will inform all API integration testing implementation.\n\nAll integration tests must include proper client setup and response validation.\n\n## 7. Test Data Management\n\n### Test Data Factory Patterns\n"
      },
      {
        "type": "code_reference",
        "text": "python\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Any\nfrom datetime import datetime\n\n@dataclass\nclass TestStockData:\n    \"\"\"Test stock data factory.\"\"\"\n    symbol: str = \"AAPL\"\n    price: float = 150.00\n    volume: int = 1000000\n    date: datetime = None\n\n    def __post_init__(self):\n        if self.date is None:\n            self.date = datetime.utcnow()\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary.\"\"\"\n        return {\n            \"symbol\": self.symbol,\n            \"price\": self.price,\n            \"volume\": self.volume,\n            \"date\": self.date.isoformat()\n        }\n\n    @classmethod\n    def create_batch(cls, count: int) -> List[Dict[str, Any]]:\n        \"\"\"Create batch of test data.\"\"\"\n        symbols = [\"AAPL\", \"TSLA\", \"GOOGL\", \"MSFT\", \"AMZN\"]\n        return [\n            cls(symbol=symbols[i % len(symbols)], price=100 + i * 10).to_dict()\n            for i in range(count)\n        ]\n\n@pytest.fixture\ndef sample_stock_data() -> TestStockData:\n    \"\"\"Create sample stock data.\"\"\"\n    return TestStockData()\n\n@pytest.fixture\ndef large_dataset() -> List[Dict[str, Any]]:\n    \"\"\"Create large dataset for performance testing.\"\"\"\n    return TestStockData.create_batch(1000)\n"
      }
    ],
    "raw_content": "# Python Testing Guide\n\n> This guide provides comprehensive testing patterns and best practices for Python development. Use these patterns to ensure robust, maintainable test suites.\n\n## AI Metadata\n\n**Template Version:** 2.1\n**AI Processing Level:** High\n**Required Context:** Python testing, pytest, mocking, test data management\n**Validation Required:** Yes\n**Code Generation:** Supported\n\n**Dependencies:**\n- `../Core%20Principles.md` - Decision-making frameworks\n- `../Python%20Style%20Guide.md` - Python implementation patterns\n- `../Domain-Specific/Web%20Scraping%20Patterns.md` - File processing patterns\n- `../../project_context/Common%20Patterns.md` - Project-specific patterns\n- `../../project_context/Architecture%20Overview.md` - System architecture\n\n**Validation Rules:**\n- All tests must include proper error handling and edge cases\n- Test data must be isolated and not use production directories\n- Mocking must be used for external dependencies\n- Test organization must follow established patterns\n- Performance tests must include realistic measurements\n\n## Overview\n\n**Document Purpose:** Comprehensive Python testing standards and best practices for the CreamPie project\n**Scope:** Unit testing, integration testing, test data management, and performance testing\n**Target Users:** AI assistants and developers implementing Python tests\n**Last Updated:** Current\n\n**AI Context:** This guide provides the foundational testing patterns that must be followed for all Python development in the project. It ensures comprehensive test coverage, proper isolation, and maintainable test suites.\n\n## 1. Testing Fundamentals\n\n### General Testing Guidelines\n1. Write unit tests for all functions\n2. Use pytest for testing\n3. Use fixtures for test setup\n4. Use parametrize for multiple test cases\n5. Use mock for external dependencies\n6. Include both positive and negative test cases\n7. Mock external dependencies in tests\n\nThese guidelines will inform all test implementation throughout the project.\n\nAll tests must follow these guidelines and include comprehensive coverage.\n\n### Test Directory Structure\n```\ncream_api/\n├── tests/\n│   ├── conftest.py              # Shared test configuration\n│   ├── stock_data/\n│   │   ├── fixtures/           # Test-specific data files\n│   │   │   └── AAPL_2025-06-16.html\n│   │   ├── test_loader.py\n│   │   ├── test_parser.py\n│   │   └── test_constants.py\n│   ├── users/\n│   │   ├── test_auth.py\n│   │   └── test_models.py\n│   └── common/\n│       └── test_rate_limiting.py\n└── ...\n```\n\nThis structure will inform all test organization and file placement.\n\nAll test modules must follow this structure and naming conventions.\n\n## 2. Test Data Management\n\n### Directory Isolation\n- Production data directories (e.g., `raw_responses/`) should NEVER be used in tests\n- Each test module should have its own test-specific directories\n- Use temporary directories for file operations in tests\n\nThis isolation strategy will inform all test data management implementation.\n\nAll tests must use isolated test data and never reference production directories.\n\n### Test Fixture Patterns\n```python\nimport pytest\nimport os\nimport tempfile\nfrom pathlib import Path\nfrom unittest.mock import patch\nfrom typing import AsyncGenerator\n\n@pytest.fixture\ndef test_raw_responses_dir(tmp_path: Path) -> Path:\n    \"\"\"Create a temporary directory for test raw responses.\"\"\"\n    test_dir = tmp_path / \"test_raw_responses\"\n    test_dir.mkdir(exist_ok=True)\n    return test_dir\n\n@pytest.fixture\ndef sample_html_file(test_raw_responses_dir: Path) -> Path:\n    \"\"\"Create a sample HTML file for testing.\"\"\"\n    html_content = \"\"\"\n    <html>\n        <body>\n            <table class=\"stock-table\">\n                <tr><th>Symbol</th><th>Price</th></tr>\n                <tr><td>AAPL</td><td>150.00</td></tr>\n            </table>\n        </body>\n    </html>\n    \"\"\"\n    file_path = test_raw_responses_dir / \"test_stock_data.html\"\n    file_path.write_text(html_content)\n    return file_path\n\n@pytest.fixture\nasync def async_test_db():\n    \"\"\"Create async test database session.\"\"\"\n    from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine\n    from sqlalchemy.orm import sessionmaker\n\n    # Use in-memory SQLite for testing\n    engine = create_async_engine(\"sqlite+aiosqlite:///:memory:\", echo=False)\n    TestingSessionLocal = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)\n\n    async with engine.begin() as conn:\n        # Create tables\n        from cream_api.models import Base\n        await conn.run_sync(Base.metadata.create_all)\n\n    async with TestingSessionLocal() as session:\n        yield session\n\n    await engine.dispose()\n\n@pytest.fixture\ndef mock_config():\n    \"\"\"Mock configuration for testing.\"\"\"\n    with patch(\"cream_api.settings.app_settings\") as mock_settings:\n        mock_settings.HTML_RAW_RESPONSES_DIR = Path(\"/test/raw\")\n        mock_settings.HTML_PARSED_RESPONSES_DIR = Path(\"/test/parsed\")\n        mock_settings.DATABASE_URL = \"sqlite+aiosqlite:///:memory:\"\n        yield mock_settings\n```\n\nThese fixture patterns will inform all test setup and data management implementation.\n\nAll tests must use proper fixtures for data setup and cleanup.\n\n### Patch Production Paths\n```python\n@pytest.fixture\nasync def loader_with_mock_paths(session: AsyncSession, test_raw_responses_dir: Path) -> StockDataLoader:\n    \"\"\"Create loader with mocked production paths.\"\"\"\n    with patch(\"cream_api.settings.app_settings.HTML_RAW_RESPONSES_DIR\", test_raw_responses_dir):\n        with patch(\"cream_api.settings.app_settings.HTML_PARSED_RESPONSES_DIR\", test_raw_responses_dir / \"parsed\"):\n            return StockDataLoader(session)\n\n@pytest.fixture\ndef mock_external_api():\n    \"\"\"Mock external API calls.\"\"\"\n    with patch(\"cream_api.stock_data.retriever.requests.get\") as mock_get:\n        mock_get.return_value.status_code = 200\n        mock_get.return_value.text = \"<html>Mock response</html>\"\n        yield mock_get\n```\n\nThis patching pattern will inform all external dependency mocking implementation.\n\nAll external dependencies must be properly mocked in tests.\n\n## 3. Test Organization Patterns\n\n### Test Class Structure\n```python\nimport pytest\nfrom unittest.mock import Mock, patch\nfrom typing import List, Dict, Any\n\nclass TestStockDataProcessor:\n    \"\"\"Test suite for StockDataProcessor.\"\"\"\n\n    @pytest.fixture(autouse=True)\n    def setup(self, async_test_db, mock_config):\n        \"\"\"Setup test environment.\"\"\"\n        self.db = async_test_db\n        self.config = mock_config\n        self.processor = StockDataProcessor(self.db)\n\n    def test_process_valid_data(self, sample_html_file: Path):\n        \"\"\"Test processing valid HTML data.\"\"\"\n        # Arrange\n        expected_data = [\n            {\"symbol\": \"AAPL\", \"price\": \"150.00\"}\n        ]\n\n        # Act\n        result = self.processor.process_file(sample_html_file)\n\n        # Assert\n        assert result == expected_data\n        assert len(result) == 1\n\n    def test_process_invalid_file(self, test_raw_responses_dir: Path):\n        \"\"\"Test processing invalid file.\"\"\"\n        # Arrange\n        invalid_file = test_raw_responses_dir / \"invalid.txt\"\n        invalid_file.write_text(\"Not HTML content\")\n\n        # Act & Assert\n        with pytest.raises(ValueError, match=\"Invalid HTML file\"):\n            self.processor.process_file(invalid_file)\n\n    @pytest.mark.parametrize(\"html_content,expected_count\", [\n        (\"<table><tr><td>AAPL</td></tr></table>\", 1),\n        (\"<table><tr><td>AAPL</td></tr><tr><td>TSLA</td></tr></table>\", 2),\n        (\"<div>No table</div>\", 0),\n    ])\n    def test_process_various_content(self, html_content: str, expected_count: int, test_raw_responses_dir: Path):\n        \"\"\"Test processing various HTML content.\"\"\"\n        # Arrange\n        test_file = test_raw_responses_dir / \"test.html\"\n        test_file.write_text(html_content)\n\n        # Act\n        result = self.processor.process_file(test_file)\n\n        # Assert\n        assert len(result) == expected_count\n```\n\nThis test class pattern will inform all test organization and structure implementation.\n\nAll test suites must follow this structure with proper setup, teardown, and parametrization.\n\n### Async Test Patterns\n```python\nimport pytest\nimport asyncio\nfrom unittest.mock import AsyncMock\n\nclass TestAsyncStockDataProcessor:\n    \"\"\"Test suite for async stock data processing.\"\"\"\n\n    @pytest.fixture(autouse=True)\n    async def setup(self, async_test_db):\n        \"\"\"Setup async test environment.\"\"\"\n        self.db = async_test_db\n        self.processor = AsyncStockDataProcessor(self.db)\n\n    @pytest.mark.asyncio\n    async def test_async_process_files(self, sample_html_file: Path):\n        \"\"\"Test async file processing.\"\"\"\n        # Arrange\n        files = [sample_html_file]\n\n        # Act\n        results = await self.processor.process_files(files)\n\n        # Assert\n        assert len(results) == 1\n        assert results[0][\"success\"] is True\n\n    @pytest.mark.asyncio\n    async def test_async_process_with_error(self, test_raw_responses_dir: Path):\n        \"\"\"Test async processing with error handling.\"\"\"\n        # Arrange\n        invalid_file = test_raw_responses_dir / \"invalid.html\"\n        invalid_file.write_text(\"Invalid content\")\n\n        # Act\n        results = await self.processor.process_files([invalid_file])\n\n        # Assert\n        assert len(results) == 1\n        assert results[0][\"success\"] is False\n        assert \"error\" in results[0]\n\n    @pytest.mark.asyncio\n    async def test_concurrent_processing(self, sample_html_file: Path):\n        \"\"\"Test concurrent file processing.\"\"\"\n        # Arrange\n        files = [sample_html_file] * 5  # Process same file 5 times\n\n        # Act\n        start_time = asyncio.get_event_loop().time()\n        results = await self.processor.process_files_concurrent(files)\n        end_time = asyncio.get_event_loop().time()\n\n        # Assert\n        assert len(results) == 5\n        assert all(r[\"success\"] for r in results)\n        assert end_time - start_time < 2.0  # Should complete quickly\n```\n\nThis async test pattern will inform all asynchronous test implementation.\n\nAll async tests must use proper async fixtures and error handling.\n\n## 4. Mocking Patterns\n\n### External API Mocking\n```python\nimport pytest\nfrom unittest.mock import Mock, patch, AsyncMock\nfrom httpx import AsyncClient\n\nclass TestExternalAPIIntegration:\n    \"\"\"Test external API integration.\"\"\"\n\n    @pytest.fixture\n    def mock_stock_api(self):\n        \"\"\"Mock stock API responses.\"\"\"\n        with patch(\"cream_api.stock_data.retriever.AsyncClient\") as mock_client:\n            mock_response = Mock()\n            mock_response.status_code = 200\n            mock_response.text = \"<html>Stock data</html>\"\n\n            mock_client_instance = AsyncMock()\n            mock_client_instance.get.return_value = mock_response\n            mock_client.return_value.__aenter__.return_value = mock_client_instance\n\n            yield mock_client_instance\n\n    @pytest.mark.asyncio\n    async def test_fetch_stock_data_success(self, mock_stock_api):\n        \"\"\"Test successful stock data fetching.\"\"\"\n        # Arrange\n        symbol = \"AAPL\"\n\n        # Act\n        result = await fetch_stock_data(symbol)\n\n        # Assert\n        assert result is not None\n        mock_stock_api.get.assert_called_once()\n\n    @pytest.mark.asyncio\n    async def test_fetch_stock_data_failure(self, mock_stock_api):\n        \"\"\"Test stock data fetching failure.\"\"\"\n        # Arrange\n        mock_stock_api.get.side_effect = Exception(\"API Error\")\n        symbol = \"INVALID\"\n\n        # Act & Assert\n        with pytest.raises(Exception, match=\"API Error\"):\n            await fetch_stock_data(symbol)\n```\n\nThis mocking pattern will inform all external API test implementation.\n\nAll external API tests must include proper mocking and error scenarios.\n\n### Database Mocking\n```python\nimport pytest\nfrom unittest.mock import Mock, patch\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nclass TestDatabaseOperations:\n    \"\"\"Test database operations.\"\"\"\n\n    @pytest.fixture\n    def mock_session(self):\n        \"\"\"Mock database session.\"\"\"\n        session = Mock(spec=AsyncSession)\n        session.commit = AsyncMock()\n        session.rollback = AsyncMock()\n        session.close = AsyncMock()\n        return session\n\n    @pytest.mark.asyncio\n    async def test_save_stock_data(self, mock_session):\n        \"\"\"Test saving stock data to database.\"\"\"\n        # Arrange\n        stock_data = {\"symbol\": \"AAPL\", \"price\": 150.00}\n\n        # Act\n        result = await save_stock_data(mock_session, stock_data)\n\n        # Assert\n        assert result is True\n        mock_session.add.assert_called_once()\n        mock_session.commit.assert_called_once()\n\n    @pytest.mark.asyncio\n    async def test_save_stock_data_error(self, mock_session):\n        \"\"\"Test database error handling.\"\"\"\n        # Arrange\n        mock_session.commit.side_effect = Exception(\"Database error\")\n        stock_data = {\"symbol\": \"AAPL\", \"price\": 150.00}\n\n        # Act & Assert\n        with pytest.raises(Exception, match=\"Database error\"):\n            await save_stock_data(mock_session, stock_data)\n\n        mock_session.rollback.assert_called_once()\n```\n\nThis database mocking pattern will inform all database test implementation.\n\nAll database tests must include proper session mocking and error handling.\n\n## 5. Performance Testing\n\n### Performance Test Patterns\n```python\nimport pytest\nimport time\nimport asyncio\nfrom typing import List\n\nclass TestPerformance:\n    \"\"\"Performance test suite.\"\"\"\n\n    # Performance constants\n    MAX_PROCESSING_TIME_SECONDS = 5.0\n    MAX_MEMORY_USAGE_MB = 100\n    MIN_THROUGHPUT_RECORDS_PER_SECOND = 10\n\n    def test_processing_performance(self, large_dataset: List[Dict]):\n        \"\"\"Test processing performance with large dataset.\"\"\"\n        # Arrange\n        start_time = time.time()\n\n        # Act\n        result = process_large_dataset(large_dataset)\n        end_time = time.time()\n\n        # Assert\n        processing_time = end_time - start_time\n        assert processing_time < self.MAX_PROCESSING_TIME_SECONDS, \\\n            f\"Processing took {processing_time}s, expected <{self.MAX_PROCESSING_TIME_SECONDS}s\"\n\n        assert len(result) == len(large_dataset)\n\n    @pytest.mark.asyncio\n    async def test_concurrent_processing_performance(self, sample_html_file: Path):\n        \"\"\"Test concurrent processing performance.\"\"\"\n        # Arrange\n        files = [sample_html_file] * 50  # Process 50 files concurrently\n        start_time = asyncio.get_event_loop().time()\n\n        # Act\n        results = await process_files_concurrent(files)\n        end_time = asyncio.get_event_loop().time()\n\n        # Assert\n        processing_time = end_time - start_time\n        throughput = len(files) / processing_time\n\n        assert throughput >= self.MIN_THROUGHPUT_RECORDS_PER_SECOND, \\\n            f\"Throughput {throughput:.2f} records/sec, expected >= {self.MIN_THROUGHPUT_RECORDS_PER_SECOND}\"\n\n        assert all(r[\"success\"] for r in results)\n\n    def test_memory_usage(self, large_dataset: List[Dict]):\n        \"\"\"Test memory usage during processing.\"\"\"\n        import psutil\n        import os\n\n        # Arrange\n        process = psutil.Process(os.getpid())\n        initial_memory = process.memory_info().rss / 1024 / 1024  # MB\n\n        # Act\n        result = process_large_dataset(large_dataset)\n        final_memory = process.memory_info().rss / 1024 / 1024  # MB\n\n        # Assert\n        memory_increase = final_memory - initial_memory\n        assert memory_increase < self.MAX_MEMORY_USAGE_MB, \\\n            f\"Memory increase {memory_increase:.2f}MB, expected <{self.MAX_MEMORY_USAGE_MB}MB\"\n```\n\nThis performance test pattern will inform all performance testing implementation.\n\nAll performance tests must include realistic measurements and clear assertions.\n\n## 6. Integration Testing\n\n### API Integration Testing\n```python\nimport pytest\nfrom fastapi.testclient import TestClient\nfrom httpx import AsyncClient\n\nclass TestAPIEndpoints:\n    \"\"\"Test API endpoint integration.\"\"\"\n\n    @pytest.fixture\n    def client(self, test_app):\n        \"\"\"Create test client.\"\"\"\n        return TestClient(test_app)\n\n    def test_get_stock_data_endpoint(self, client: TestClient):\n        \"\"\"Test GET /stock-data endpoint.\"\"\"\n        # Act\n        response = client.get(\"/stock-data/AAPL\")\n\n        # Assert\n        assert response.status_code == 200\n        data = response.json()\n        assert \"symbol\" in data\n        assert \"price\" in data\n\n    def test_post_stock_tracking(self, client: TestClient):\n        \"\"\"Test POST /stock-data/track endpoint.\"\"\"\n        # Arrange\n        payload = {\"symbol\": \"TSLA\"}\n\n        # Act\n        response = client.post(\"/stock-data/track\", json=payload)\n\n        # Assert\n        assert response.status_code == 200\n        data = response.json()\n        assert data[\"symbol\"] == \"TSLA\"\n        assert data[\"is_active\"] is True\n\n    @pytest.mark.asyncio\n    async def test_async_api_endpoint(self, async_client: AsyncClient):\n        \"\"\"Test async API endpoint.\"\"\"\n        # Act\n        response = await async_client.get(\"/stock-data/AAPL\")\n\n        # Assert\n        assert response.status_code == 200\n        data = response.json()\n        assert \"symbol\" in data\n```\n\nThis integration test pattern will inform all API integration testing implementation.\n\nAll integration tests must include proper client setup and response validation.\n\n## 7. Test Data Management\n\n### Test Data Factory Patterns\n```python\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Any\nfrom datetime import datetime\n\n@dataclass\nclass TestStockData:\n    \"\"\"Test stock data factory.\"\"\"\n    symbol: str = \"AAPL\"\n    price: float = 150.00\n    volume: int = 1000000\n    date: datetime = None\n\n    def __post_init__(self):\n        if self.date is None:\n            self.date = datetime.utcnow()\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary.\"\"\"\n        return {\n            \"symbol\": self.symbol,\n            \"price\": self.price,\n            \"volume\": self.volume,\n            \"date\": self.date.isoformat()\n        }\n\n    @classmethod\n    def create_batch(cls, count: int) -> List[Dict[str, Any]]:\n        \"\"\"Create batch of test data.\"\"\"\n        symbols = [\"AAPL\", \"TSLA\", \"GOOGL\", \"MSFT\", \"AMZN\"]\n        return [\n            cls(symbol=symbols[i % len(symbols)], price=100 + i * 10).to_dict()\n            for i in range(count)\n        ]\n\n@pytest.fixture\ndef sample_stock_data() -> TestStockData:\n    \"\"\"Create sample stock data.\"\"\"\n    return TestStockData()\n\n@pytest.fixture\ndef large_dataset() -> List[Dict[str, Any]]:\n    \"\"\"Create large dataset for performance testing.\"\"\"\n    return TestStockData.create_batch(1000)\n```\n\nThis test data factory pattern will inform all test data creation implementation.\n\nAll test data must be created using proper factories and fixtures.\n\n## Implementation Guidelines\n\n### For AI Assistants\n1. **Follow these patterns** for all test implementation\n2. **Use proper fixtures** for test setup and teardown\n3. **Mock external dependencies** to ensure test isolation\n4. **Include comprehensive error cases** in test coverage\n5. **Use parametrization** for multiple test scenarios\n6. **Implement performance tests** for critical operations\n7. **Follow test organization** patterns for maintainability\n8. **Use proper assertions** with descriptive messages\n\n### For Human Developers\n1. **Reference these patterns** when writing tests\n2. **Use isolated test data** and never reference production\n3. **Mock external dependencies** for reliable tests\n4. **Include both success and error cases** in test coverage\n5. **Use descriptive test names** and assertions\n6. **Follow established patterns** for consistency\n7. **Test performance** for critical operations\n\n## Quality Assurance\n\n### Test Coverage Standards\n- All functions must have unit tests\n- All API endpoints must have integration tests\n- All error paths must be tested\n- Performance tests must be included for critical operations\n- Test data must be properly isolated\n\n### Test Quality Standards\n- Tests must be independent and repeatable\n- Test names must be descriptive and clear\n- Assertions must include descriptive messages\n- Test data must be minimal and focused\n- Error handling must be comprehensive\n\n### Performance Standards\n- Unit tests must complete in under 1 second\n- Integration tests must complete in under 5 seconds\n- Performance tests must include realistic measurements\n- Memory usage must be monitored and controlled\n- Concurrent operations must be properly tested\n\n### Maintenance Standards\n- Tests must be updated with code changes\n- Test data must be kept current and relevant\n- Mock responses must reflect actual API behavior\n- Performance benchmarks must be regularly updated\n- Test documentation must be maintained\n\n---\n\n**AI Quality Checklist**: Before implementing tests, ensure:\n- [x] Test data is properly isolated from production\n- [x] External dependencies are properly mocked\n- [x] Both success and error cases are covered\n- [x] Performance tests include realistic measurements\n- [x] Test organization follows established patterns\n- [x] Assertions include descriptive messages\n- [x] Test data factories are used for consistency\n- [x] Async tests use proper async patterns\n"
  },
  "cross_references": [],
  "code_generation_hints": [
    {
      "context": "testing",
      "hint": "These guidelines will inform all test implementation throughout the project.",
      "validation": ""
    },
    {
      "context": "testing",
      "hint": "This structure will inform all test organization and file placement.",
      "validation": ""
    },
    {
      "context": "testing",
      "hint": "This isolation strategy will inform all test data management implementation.",
      "validation": ""
    },
    {
      "context": "testing",
      "hint": "These fixture patterns will inform all test setup and data management implementation.",
      "validation": ""
    },
    {
      "context": "general",
      "hint": "This patching pattern will inform all external dependency mocking implementation.",
      "validation": ""
    },
    {
      "context": "testing",
      "hint": "This test class pattern will inform all test organization and structure implementation.",
      "validation": ""
    },
    {
      "context": "testing",
      "hint": "This async test pattern will inform all asynchronous test implementation.",
      "validation": ""
    },
    {
      "context": "testing",
      "hint": "This mocking pattern will inform all external API test implementation.",
      "validation": ""
    },
    {
      "context": "testing",
      "hint": "This database mocking pattern will inform all database test implementation.",
      "validation": ""
    },
    {
      "context": "testing",
      "hint": "This performance test pattern will inform all performance testing implementation.",
      "validation": ""
    },
    {
      "context": "testing",
      "hint": "This integration test pattern will inform all API integration testing implementation.",
      "validation": ""
    },
    {
      "context": "testing",
      "hint": "This test data factory pattern will inform all test data creation implementation.",
      "validation": ""
    }
  ],
  "validation_rules": [
    "Tests must be independent and repeatable",
    "Test organization must follow established patterns",
    "Concurrent operations must be properly tested",
    "All test modules must follow this structure and naming conventions",
    "Test data must be isolated and not use production directories",
    "All API endpoints must have integration tests",
    "Each test module should have its own test-specific directories",
    "Performance tests must be included for critical operations",
    "All test data must be created using proper factories and fixtures",
    "Production data directories (e.g., `raw_responses/`) should NEVER be used in tests",
    "All async tests must use proper async fixtures and error handling",
    "All tests must include proper error handling and edge cases",
    "Tests must be updated with code changes",
    "All performance tests must include realistic measurements and clear assertions",
    "Assertions must include descriptive messages",
    "Test data must be minimal and focused",
    "All error paths must be tested",
    "Integration tests must complete in under 5 seconds",
    "Mock responses must reflect actual API behavior",
    "All functions must have unit tests",
    "Test data must be properly isolated",
    "Error handling must be comprehensive",
    "Performance benchmarks must be regularly updated",
    "start_time < 2.0  # Should complete quickly\n```",
    "Test names must be descriptive and clear",
    "Test data must be kept current and relevant",
    "All tests must use proper fixtures for data setup and cleanup",
    "Performance tests must include realistic measurements",
    "Mocking must be used for external dependencies",
    "Unit tests must complete in under 1 second",
    "All database tests must include proper session mocking and error handling",
    "All test suites must follow this structure with proper setup, teardown, and parametrization",
    "All tests must use isolated test data and never reference production directories",
    "Test documentation must be maintained",
    "Memory usage must be monitored and controlled",
    "All external dependencies must be properly mocked in tests",
    "All integration tests must include proper client setup and response validation",
    "All tests must follow these guidelines and include comprehensive coverage",
    "All external API tests must include proper mocking and error scenarios"
  ],
  "optimization": {
    "version": "1.0",
    "optimized_at": "2025-06-18T19:19:47.745273",
    "improvements": [
      "fixed_file_references",
      "extracted_ai_metadata",
      "structured_cross_references",
      "extracted_code_hints",
      "structured_validation_rules"
    ],
    "literal_strings_cleaned": true,
    "cleaned_at": "2025-06-18T19:30:00.000000"
  }
}