#!/usr/bin/env python3

import json
import os
from datetime import datetime


def test_paths():
    """Test path resolution for the health check script."""

    # Get the script directory and AI folder path
    script_dir = os.path.dirname(__file__)
    ai_folder = os.path.dirname(script_dir)

    # Collect test results
    results = {
        "script_directory": script_dir,
        "ai_folder": ai_folder,
        "current_working_directory": os.getcwd(),
        "config_file_path": os.path.join(ai_folder, "ai_config.json"),
        "config_exists": False,
        "config_readable": False,
        "config_keys": [],
        "ai_folder_contents": [],
        "errors": [],
        "timestamp": datetime.now().isoformat(),
    }

    print(f"Script directory: {results['script_directory']}")
    print(f"AI folder: {results['ai_folder']}")
    print(f"Current working directory: {results['current_working_directory']}")

    # Test if ai_config.json exists
    print(f"Looking for config at: {results['config_file_path']}")
    results["config_exists"] = os.path.exists(results["config_file_path"])
    print(f"Config file exists: {results['config_exists']}")

    # List all files in the AI folder
    print("\nFiles in AI folder:")
    try:
        for item in os.listdir(ai_folder):
            item_path = os.path.join(ai_folder, item)
            item_info = {"name": item, "is_file": os.path.isfile(item_path), "is_directory": os.path.isdir(item_path)}
            results["ai_folder_contents"].append(item_info)

            if item_info["is_file"]:
                print(f"  📄 {item}")
            else:
                print(f"  📁 {item}/")
    except Exception as e:
        error_msg = f"Error listing AI folder contents: {e!s}"
        results["errors"].append(error_msg)
        print(f"❌ {error_msg}")

    # Test reading the config file
    if results["config_exists"]:
        try:
            with open(results["config_file_path"]) as f:
                config = json.load(f)
            results["config_readable"] = True
            results["config_keys"] = list(config.keys())
            print("\n✅ Successfully read config file")
            print(f"Config keys: {results['config_keys']}")
        except Exception as e:
            error_msg = f"Error reading config: {e!s}"
            results["errors"].append(error_msg)
            print(f"\n❌ {error_msg}")
    else:
        print("\n❌ Config file not found")

    # Generate and save report
    generate_report(results, ai_folder)

    return results


def generate_report(results, ai_folder):
    """Generate a structured report for AI consumption."""

    # Create output directory
    output_dir = os.path.join(ai_folder, "outputs", "test_results")
    os.makedirs(output_dir, exist_ok=True)

    # Generate report content
    report_content = f"""# AI Script Path Test Results

> **AI Assistant**:
This file contains the results of path resolution tests for AI scripts.
It provides insights into the file system structure and configuration accessibility.

## AI Metadata

- **Purpose**: Path resolution test results and system structure validation
- **Last Updated**: {datetime.now().strftime("%Y-%m-%d")}
- **Template Version**: 1.0
- **AI Tool Compatibility**: High
- **AI Processing Level**: High
- **Required Context**: AI documentation structure and file system layout
- **Validation Required**: Yes
- **Code Generation**: Not applicable
- **Cross-References**:
  - `../../scripts/test_paths.py` - Test script
  - `../../scripts/health_check.py` - Health check script
  - `../../scripts/update_documentation.py` - Update script
  - `../../ai_config.json` - AI configuration
- **Maintenance**: Auto-generated by test script

---

## Test Results

### Path Resolution
- **Script Directory**: `{results["script_directory"]}`
- **AI Folder**: `{results["ai_folder"]}`
- **Current Working Directory**: `{results["current_working_directory"]}`

### Configuration File
- **Config File Path**: `{results["config_file_path"]}`
- **Config File Exists**: {"✅ Yes" if results["config_exists"] else "❌ No"}
- **Config File Readable**: {"✅ Yes" if results["config_readable"] else "❌ No"}

### Configuration Structure
"""

    if results["config_keys"]:
        report_content += f"- **Config Keys**: {', '.join(results['config_keys'])}\n"
    else:
        report_content += "- **Config Keys**: None (file not readable or empty)\n"

    report_content += """
### AI Folder Contents
"""

    for item in results["ai_folder_contents"]:
        icon = "📄" if item["is_file"] else "📁"
        report_content += f"- {icon} {item['name']}{'/' if item['is_directory'] else ''}\n"

    if results["errors"]:
        report_content += """
### Errors Encountered
"""
        for error in results["errors"]:
            report_content += f"- ❌ {error}\n"

    report_content += f"""
### Test Summary
- **Total Items in AI Folder**: {len(results["ai_folder_contents"])}
- **Files**: {len([item for item in results["ai_folder_contents"] if item["is_file"]])}
- **Directories**: {len([item for item in results["ai_folder_contents"] if item["is_directory"]])}
- **Errors**: {len(results["errors"])}
- **Test Status**: {"✅ PASSED" if len(results["errors"]) == 0 else "❌ FAILED"}

### Timestamp
Test completed at: {results["timestamp"]}
"""

    # Save report
    report_file = os.path.join(output_dir, "path-test-results.md")
    with open(report_file, "w", encoding="utf-8") as f:
        f.write(report_content)

    print(f"\n📄 Test results saved to: {report_file}")

    # Also save JSON for programmatic access
    json_file = os.path.join(output_dir, "path-test-results.json")
    with open(json_file, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    print(f"📄 JSON results saved to: {json_file}")


if __name__ == "__main__":
    test_paths()
