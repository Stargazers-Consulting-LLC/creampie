{
  "ai_metadata": {
    "purpose": "",
    "last_updated": "",
    "template_version": "2.1",
    "ai_tool_compatibility": "",
    "ai_processing_level": "High",
    "required_context": "Project architecture, existing codebase, implementation requirements",
    "validation_required": "Yes",
    "code_generation": "Supported",
    "cross_references": [
      "../guide_docs/Core%20Principles.json",
      "Architecture%20Overview.json",
      "Development%20Workflow.json",
      "../guide_docs/Language-Specific/Python%20Style%20Guide.json",
      "../guide_docs/Language-Specific/FastAPI%20Development%20Guide.json"
    ],
    "maintenance": ""
  },
  "file_info": {
    "file_path": "project_context/Common Patterns.md",
    "original_format": "markdown",
    "converted_at": "2025-06-18T19:14:30.267087",
    "file_size": 22253,
    "line_count": 689,
    "optimized_at": "2025-06-18T19:19:47.763267",
    "optimization_version": "1.0"
  },
  "content": {
    "sections": [
      {
        "level": 1,
        "title": "Common Development Patterns",
        "content": "> This document outlines common patterns and practices used throughout the project. Use these patterns to ensure consistency and follow established conventions.",
        "subsections": []
      },
      {
        "level": 2,
        "title": "AI Metadata",
        "content": "**Template Version:** 2.1\n**AI Processing Level:** High\n**Required Context:** Project architecture, existing codebase, implementation requirements\n**Validation Required:** Yes\n**Code Generation:** Supported\n\n**Dependencies:**\n- `../guide_docs/Core%20Principles.json.replace(\".json\", \".json\")` - Decision-making frameworks\n- `Architecture%20Overview.json.replace(\".json\", \".json\")` - System architecture\n- `Development%20Workflow.json.replace(\".json\", \".json\")` - Development process\n- `../guide_docs/Language-Specific/Python%20Style%20Guide.json.replace(\".json\", \".json\")` - Python implementation patterns\n- `../guide_docs/Language-Specific/FastAPI%20Development%20Guide.json.replace(\".json\", \".json\")` - API development patterns\n\n**Validation Rules:**\n- All patterns must reference actual project structure and files\n- Code examples must follow established project conventions\n- Pattern application must be consistent across the codebase\n- Implementation must align with project architecture\n- Error handling must follow established patterns",
        "subsections": []
      },
      {
        "level": 2,
        "title": "Overview",
        "content": "**Document Purpose:** Established patterns and conventions for the CreamPie project\n**Scope:** All code organization, database operations, API development, and background tasks\n**Target Users:** AI assistants and developers implementing project features\n**Last Updated:** Current\n\n**AI Context:** This document provides the foundational patterns that must be followed when implementing any feature in the CreamPie project. It ensures consistency, maintainability, and alignment with the established architecture.",
        "subsections": []
      },
      {
        "level": 2,
        "title": "Code Organization Patterns",
        "content": "",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Module Structure",
        "content": "```python",
        "subsections": []
      },
      {
        "level": 1,
        "title": "Standard module organization",
        "content": "\"\"\"\nModule docstring explaining purpose and usage.\n\"\"\"",
        "subsections": []
      },
      {
        "level": 1,
        "title": "Imports organized by type",
        "content": "import os\nimport logging\nfrom typing import Optional\n\nimport aiohttp\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nfrom .config import get_module_config\nfrom .exceptions import ModuleSpecificException",
        "subsections": []
      },
      {
        "level": 1,
        "title": "Module-level configuration",
        "content": "config = get_module_config()\nlogger = logging.getLogger(__name__)",
        "subsections": []
      },
      {
        "level": 1,
        "title": "Constants",
        "content": "DEFAULT_TIMEOUT = 30\nMAX_RETRIES = 3",
        "subsections": []
      },
      {
        "level": 1,
        "title": "Main classes and functions",
        "content": "class MainClass:\n    \"\"\"Primary class for this module.\"\"\"\n\n    def __init__(self, config: dict):\n        self.config = config\n\n    async def main_method(self) -> dict:\n        \"\"\"Main method with proper error handling.\"\"\"\n        try:\n            result = await self._internal_method()\n            return result\n        except Exception as e:\n            logger.error(f\"Main method failed: {e}\")\n            raise ModuleSpecificException(\"User-friendly message\") from e\n\n    async def _internal_method(self) -> dict:\n        \"\"\"Internal method for implementation details.\"\"\"\n        pass\n```\n\nThis pattern will inform the structure of all new modules and classes in the project.\n\nModule structure must follow this exact organization with proper imports, configuration, and error handling.",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Configuration Pattern",
        "content": "```python",
        "subsections": []
      },
      {
        "level": 1,
        "title": "Load configuration once at module level",
        "content": "config = get_module_config()\n\ndef some_function():\n    # Use module-level config instead of loading again\n    processor = DataProcessor(config=config)\n```\n\nThis pattern will inform how to handle configuration loading and usage throughout the project.\n\nConfiguration must be loaded once at module level and reused, not loaded multiple times.",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Error Handling Pattern",
        "content": "```python\ntry:\n    # Operation that might fail\n    result = await some_operation()\nexcept SpecificException as e:\n    logger.error(f\"Operation failed: {e}\")\n    raise CustomException(\"User-friendly message\") from e\nexcept Exception as e:\n    logger.error(f\"Unexpected error: {e}\")\n    raise CustomException(\"An unexpected error occurred\") from e\n```\n\nThis pattern will inform error handling implementation for all async operations and external calls.\n\nError handling must include specific exception types, proper logging, and user-friendly error messages.",
        "subsections": []
      },
      {
        "level": 2,
        "title": "Database Patterns",
        "content": "",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Session Management",
        "content": "```python\nasync with AsyncSessionLocal() as session:\n    async with session.begin():\n        # Database operations\n        session.add(model)\n        await session.commit()\n```\n\nThis pattern will inform database session management for all database operations.\n\nDatabase sessions must use proper async context managers and transaction management.",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Model Definition",
        "content": "```python\nfrom sqlalchemy import Column, Integer, String, DateTime\nfrom sqlalchemy.ext.declarative import declarative_base\n\nBase = declarative_base()\n\nclass StockData(Base):\n    __tablename__ = \"stock_data\"\n\n    id = Column(Integer, primary_key=True, index=True)\n    symbol = Column(String, index=True, nullable=False)\n    date = Column(DateTime, nullable=False)\n    price = Column(Integer, nullable=False)\n```\n\nThis pattern will inform SQLAlchemy model definitions with proper indexing and constraints.\n\nModels must include proper indexes, constraints, and follow naming conventions.",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Migration Pattern",
        "content": "```python\n\"\"\"Add stock data table\n\nRevision ID: abc123def456\nRevises: previous_revision\nCreate Date: 2024-01-01 12:00:00.000000\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa\n\ndef upgrade():\n    op.create_table(\n        'stock_data',\n        sa.Column('id', sa.Integer(), nullable=False),\n        sa.Column('symbol', sa.String(), nullable=False),\n        sa.Column('date', sa.DateTime(), nullable=False),\n        sa.Column('price', sa.Integer(), nullable=False),\n        sa.PrimaryKeyConstraint('id')\n    )\n    op.create_index(op.f('ix_stock_data_symbol'), 'stock_data', ['symbol'], unique=False)\n\ndef downgrade():\n    op.drop_index(op.f('ix_stock_data_symbol'), table_name='stock_data')\n    op.drop_table('stock_data')\n```\n\nThis pattern will inform Alembic migration file structure and database schema changes.\n\nMigrations must include proper upgrade and downgrade functions with descriptive docstrings.",
        "subsections": []
      },
      {
        "level": 2,
        "title": "API Patterns",
        "content": "",
        "subsections": []
      },
      {
        "level": 3,
        "title": "FastAPI Endpoint",
        "content": "```python\nfrom fastapi import APIRouter, Depends, HTTPException\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nfrom ..db import get_session\nfrom ..models import StockData\nfrom ..schemas import StockDataResponse\n\nrouter = APIRouter()\n\n@router.get(\"/stock-data/{symbol}\", response_model=StockDataResponse)\nasync def get_stock_data(\n    symbol: str,\n    session: AsyncSession = Depends(get_session)\n) -> StockDataResponse:\n    \"\"\"Get stock data for a specific symbol.\"\"\"\n    try:\n        # Business logic here\n        data = await get_stock_data_from_db(session, symbol)\n        return StockDataResponse.from_orm(data)\n    except ValueError as e:\n        raise HTTPException(status_code=400, detail=str(e))\n    except Exception as e:\n        logger.error(f\"Failed to get stock data: {e}\")\n        raise HTTPException(status_code=500, detail=\"Internal server error\")\n```\n\nThis pattern will inform FastAPI endpoint implementation with proper error handling and response models.\n\nEndpoints must include proper error handling, logging, and response model validation.",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Pydantic Schema",
        "content": "```python\nfrom pydantic import BaseModel, Field\nfrom datetime import datetime\nfrom typing import Optional\n\nclass StockDataResponse(BaseModel):\n    id: int\n    symbol: str = Field(..., description=\"Stock symbol\")\n    date: datetime\n    price: int = Field(..., description=\"Price in cents\")\n\n    class Config:\n        from_attributes = True\n```\n\nThis pattern will inform Pydantic schema definitions with proper field validation and descriptions.\n\nSchemas must include proper field types, descriptions, and configuration for ORM compatibility.",
        "subsections": []
      },
      {
        "level": 2,
        "title": "Background Task Patterns",
        "content": "",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Task Definition",
        "content": "```python\nfrom .background_tasks import background_task\n\n@background_task\nasync def process_stock_data():\n    \"\"\"Background task for processing stock data.\"\"\"\n    try:\n        config = get_stock_data_config()\n        processor = StockDataProcessor(config)\n        await processor.process_all_files()\n    except Exception as e:\n        logger.error(f\"Stock data processing failed: {e}\")\n        # Don't re-raise - background tasks should handle errors gracefully\n```\n\nThis pattern will inform background task implementation with proper error handling and logging.\n\nBackground tasks must handle errors gracefully and not re-raise exceptions.",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Task Scheduling",
        "content": "```python",
        "subsections": []
      },
      {
        "level": 1,
        "title": "In main.py or appropriate startup location",
        "content": "from .background_tasks import schedule_periodic_task",
        "subsections": []
      },
      {
        "level": 1,
        "title": "Schedule periodic tasks",
        "content": "schedule_periodic_task(\n    task_name=\"process_stock_data\",\n    interval_seconds=300,  # 5 minutes\n    task_function=process_stock_data\n)\n```\n\nThis pattern will inform how to schedule and configure background tasks.\n\nTask scheduling must include proper intervals and error handling.",
        "subsections": []
      },
      {
        "level": 2,
        "title": "File Processing Patterns",
        "content": "",
        "subsections": []
      },
      {
        "level": 3,
        "title": "File Organization",
        "content": "```python",
        "subsections": []
      },
      {
        "level": 1,
        "title": "Standard file organization structure",
        "content": "cream_api/\n├── files/\n│   ├── raw_responses/      # Raw data files\n│   └── parsed_responses/   # Processed data files\n├── stock_data/\n│   ├── config.py          # Configuration\n│   ├── models.py          # Database models\n│   ├── retriever.py       # Data retrieval\n│   ├── parser.py          # Data parsing\n│   ├── loader.py          # Data loading\n│   ├── processor.py       # File processing\n│   ├── tasks.py           # Background tasks\n│   └── api.py             # API endpoints\n```\n\nThis pattern will inform file organization and module structure for new features.\n\nFile organization must follow this structure for consistency and maintainability.",
        "subsections": []
      },
      {
        "level": 3,
        "title": "File Processing Workflow",
        "content": "```python\nclass FileProcessor:\n    \"\"\"Orchestrates file processing workflow.\"\"\"\n\n    def __init__(self, config: dict):\n        self.config = config\n        self.parser = DataParser(config)\n        self.loader = DataLoader(config)\n\n    async def process_file(self, file_path: str) -> bool:\n        \"\"\"Process a single file through the complete pipeline.\"\"\"\n        try:\n            # Parse the file\n            data = await self.parser.parse_file(file_path)\n\n            # Load data to database\n            await self.loader.load_data(data)\n\n            # Move file to processed directory\n            await self._move_to_processed(file_path)\n\n            return True\n        except Exception as e:\n            logger.error(f\"File processing failed for {file_path}: {e}\")\n            return False\n```\n\nThis pattern will inform file processing workflow implementation with proper error handling.\n\nFile processing must include proper error handling, logging, and file movement operations.",
        "subsections": []
      },
      {
        "level": 2,
        "title": "Testing Patterns",
        "content": "",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Unit Test Structure",
        "content": "```python\nimport pytest\nfrom unittest.mock import AsyncMock, patch\nfrom .models import StockData\nfrom .processor import FileProcessor\n\nclass TestFileProcessor:\n    \"\"\"Test cases for FileProcessor class.\"\"\"\n\n    @pytest.fixture\n    def processor(self):\n        \"\"\"Create processor instance for testing.\"\"\"\n        config = {\"test\": True}\n        return FileProcessor(config)\n\n    @pytest.mark.asyncio\n    async def test_process_file_success(self, processor):\n        \"\"\"Test successful file processing.\"\"\"\n        with patch.object(processor.parser, 'parse_file') as mock_parse:\n            mock_parse.return_value = [{\"symbol\": \"AAPL\", \"price\": 150}]\n\n            result = await processor.process_file(\"test_file.txt\")\n\n            assert result is True\n            mock_parse.assert_called_once_with(\"test_file.txt\")\n\n    @pytest.mark.asyncio\n    async def test_process_file_failure(self, processor):\n        \"\"\"Test file processing failure.\"\"\"\n        with patch.object(processor.parser, 'parse_file') as mock_parse:\n            mock_parse.side_effect = Exception(\"Parse failed\")\n\n            result = await processor.process_file(\"test_file.txt\")\n\n            assert result is False\n```\n\nThis pattern will inform unit test structure with proper fixtures and async testing.\n\nTests must include proper fixtures, async testing, and comprehensive error case coverage.",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Integration Test Structure",
        "content": "```python\nimport pytest\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom .models import StockData\nfrom .api import get_stock_data\n\n@pytest.mark.asyncio\nasync def test_get_stock_data_integration(async_session: AsyncSession):\n    \"\"\"Test stock data retrieval integration.\"\"\"\n    # Setup test data\n    test_data = StockData(\n        symbol=\"AAPL\",\n        date=datetime.now(),\n        price=15000\n    )\n    async_session.add(test_data)\n    await async_session.commit()\n\n    # Test the endpoint\n    result = await get_stock_data(\"AAPL\", async_session)\n\n    assert result.symbol == \"AAPL\"\n    assert result.price == 15000\n```\n\nThis pattern will inform integration test structure with database setup and teardown.\n\nIntegration tests must include proper database setup, test data creation, and cleanup.",
        "subsections": []
      },
      {
        "level": 2,
        "title": "Configuration Patterns",
        "content": "",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Environment-Based Configuration",
        "content": "```python\nimport os\nfrom pydantic import BaseSettings\n\nclass StockDataConfig(BaseSettings):\n    \"\"\"Configuration for stock data processing.\"\"\"\n\n    # File paths\n    raw_responses_dir: str = \"cream_api/files/raw_responses\"\n    parsed_responses_dir: str = \"cream_api/files/parsed_responses\"\n\n    # HTTP settings\n    timeout_seconds: int = 30\n    max_retries: int = 3\n    retry_delay_seconds: int = 5\n\n    # Processing settings\n    batch_size: int = 100\n    max_concurrent_files: int = 5\n\n    class Config:\n        env_prefix = \"STOCK_DATA_\"\n```\n\nThis pattern will inform configuration class structure with environment variable support.\n\nConfiguration must use Pydantic for validation and support environment variable overrides.",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Configuration Loading",
        "content": "```python\ndef get_stock_data_config() -> StockDataConfig:\n    \"\"\"Get stock data configuration.\"\"\"\n    return StockDataConfig()",
        "subsections": []
      },
      {
        "level": 1,
        "title": "Load once at module level",
        "content": "config = get_stock_data_config()\n```\n\nThis pattern will inform configuration loading and usage throughout modules.\n\nConfiguration must be loaded once at module level and reused, not loaded multiple times.",
        "subsections": []
      },
      {
        "level": 2,
        "title": "Logging Patterns",
        "content": "",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Structured Logging",
        "content": "```python\nimport logging\nimport json\nfrom datetime import datetime\n\nclass StructuredFormatter(logging.Formatter):\n    \"\"\"Structured JSON logging formatter.\"\"\"\n\n    def format(self, record):\n        log_entry = {\n            \"timestamp\": datetime.utcnow().isoformat(),\n            \"level\": record.levelname,\n            \"module\": record.module,\n            \"function\": record.funcName,\n            \"line\": record.lineno,\n            \"message\": record.getMessage()\n        }\n\n        if hasattr(record, 'extra_fields'):\n            log_entry.update(record.extra_fields)\n\n        return json.dumps(log_entry)",
        "subsections": []
      },
      {
        "level": 1,
        "title": "Configure logging",
        "content": "logger = logging.getLogger(__name__)\nhandler = logging.StreamHandler()\nhandler.setFormatter(StructuredFormatter())\nlogger.addHandler(handler)\nlogger.setLevel(logging.INFO)\n```\n\nThis pattern will inform structured logging implementation with JSON formatting.\n\nLogging must include structured format with timestamps, levels, and contextual information.",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Logging Usage",
        "content": "```python",
        "subsections": []
      },
      {
        "level": 1,
        "title": "Standard logging patterns",
        "content": "logger.info(\"Processing started\", extra={\"file_count\": 10})\nlogger.warning(\"Retry attempt\", extra={\"attempt\": 3, \"max_retries\": 5})\nlogger.error(\"Processing failed\", extra={\"file\": \"data.txt\", \"error\": str(e)})\nlogger.debug(\"Debug information\", extra={\"data_size\": len(data)})\n```\n\nThis pattern will inform logging usage with proper levels and contextual information.\n\nLogging must use appropriate levels and include relevant contextual information.",
        "subsections": []
      },
      {
        "level": 2,
        "title": "Error Handling Patterns",
        "content": "",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Custom Exceptions",
        "content": "```python\nclass StockDataException(Exception):\n    \"\"\"Base exception for stock data operations.\"\"\"\n    pass\n\nclass StockDataRetrievalError(StockDataException):\n    \"\"\"Raised when stock data retrieval fails.\"\"\"\n    pass\n\nclass StockDataParsingError(StockDataException):\n    \"\"\"Raised when stock data parsing fails.\"\"\"\n    pass\n\nclass StockDataLoadingError(StockDataException):\n    \"\"\"Raised when stock data loading fails.\"\"\"\n    pass\n```\n\nThis pattern will inform custom exception hierarchy for domain-specific error handling.\n\nCustom exceptions must inherit from appropriate base classes and provide meaningful error messages.",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Error Recovery",
        "content": "```python\nasync def process_with_retry(operation, max_retries: int = 3):\n    \"\"\"Execute operation with retry logic.\"\"\"\n    for attempt in range(max_retries):\n        try:\n            return await operation()\n        except Exception as e:\n            if attempt == max_retries - 1:\n                raise\n            logger.warning(f\"Attempt {attempt + 1} failed, retrying: {e}\")\n            await asyncio.sleep(2 ** attempt)  # Exponential backoff\n```\n\nThis pattern will inform retry logic implementation with exponential backoff.\n\nRetry logic must include proper backoff strategies and maximum retry limits.",
        "subsections": []
      },
      {
        "level": 2,
        "title": "Performance Patterns",
        "content": "",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Async Operations",
        "content": "```python\nasync def process_files_concurrently(files: list[str], max_concurrent: int = 5):\n    \"\"\"Process multiple files concurrently with semaphore limiting.\"\"\"\n    semaphore = asyncio.Semaphore(max_concurrent)\n\n    async def process_single_file(file_path: str):\n        async with semaphore:\n            return await process_file(file_path)\n\n    tasks = [process_single_file(file) for file in files]\n    results = await asyncio.gather(*tasks, return_exceptions=True)\n\n    return results\n```\n\nThis pattern will inform concurrent processing implementation with proper resource limiting.\n\nConcurrent operations must include proper resource limiting and error handling.",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Database Optimization",
        "content": "```python\nasync def batch_insert_data(session: AsyncSession, data: list[dict]):\n    \"\"\"Insert data in batches for better performance.\"\"\"\n    batch_size = 1000\n\n    for i in range(0, len(data), batch_size):\n        batch = data[i:i + batch_size]\n        models = [StockData(**item) for item in batch]\n        session.add_all(models)\n        await session.commit()\n```\n\nThis pattern will inform batch database operations for improved performance.\n\nBatch operations must include proper batch sizes and transaction management.",
        "subsections": []
      },
      {
        "level": 2,
        "title": "Security Patterns",
        "content": "",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Input Validation",
        "content": "```python\nfrom pydantic import BaseModel, validator\nimport re\n\nclass StockSymbolRequest(BaseModel):\n    symbol: str\n\n    @validator('symbol')\n    def validate_symbol(cls, v):\n        if not re.match(r'^[A-Z]{1,10}$', v):\n            raise ValueError('Symbol must be 1-10 uppercase letters')\n        return v.upper()\n```\n\nThis pattern will inform input validation implementation with Pydantic validators.\n\nInput validation must include proper regex patterns and meaningful error messages.",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Authentication and Authorization",
        "content": "```python\nfrom fastapi import Depends, HTTPException, status\nfrom fastapi.security import HTTPBearer\n\nsecurity = HTTPBearer()\n\nasync def get_current_user(token: str = Depends(security)):\n    \"\"\"Validate user authentication.\"\"\"\n    try:\n        # Validate token and return user\n        user = validate_token(token.credentials)\n        return user\n    except Exception:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Invalid authentication credentials\"\n        )\n\nasync def require_admin(user = Depends(get_current_user)):\n    \"\"\"Require admin privileges.\"\"\"\n    if not user.is_admin:\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=\"Admin privileges required\"\n        )\n    return user\n```\n\nThis pattern will inform authentication and authorization implementation.\n\nAuthentication must include proper token validation and role-based access control.",
        "subsections": []
      },
      {
        "level": 2,
        "title": "Implementation Guidelines",
        "content": "",
        "subsections": []
      },
      {
        "level": 3,
        "title": "For AI Assistants",
        "content": "1. **Reference this document** for all pattern implementations\n2. **Follow established conventions** for consistency\n3. **Apply patterns consistently** across similar functionality\n4. **Validate implementations** against pattern requirements\n5. **Update patterns** when new best practices emerge",
        "subsections": []
      },
      {
        "level": 3,
        "title": "For Human Developers",
        "content": "1. **Follow these patterns** for all new implementations\n2. **Reference patterns** when reviewing code\n3. **Suggest pattern improvements** when better approaches are found\n4. **Maintain consistency** with established conventions\n5. **Document deviations** when patterns cannot be followed",
        "subsections": []
      },
      {
        "level": 2,
        "title": "Quality Assurance",
        "content": "",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Pattern Compliance",
        "content": "- All implementations must follow established patterns\n- Deviations must be justified and documented\n- Pattern updates must be reviewed and approved\n- Consistency must be maintained across the codebase",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Code Quality",
        "content": "- All code must follow project style guidelines\n- Error handling must be comprehensive\n- Performance must meet established requirements\n- Security must follow established patterns",
        "subsections": []
      },
      {
        "level": 3,
        "title": "Documentation",
        "content": "- All patterns must be documented with examples\n- Implementation guidelines must be clear\n- Validation rules must be specific\n- Updates must be communicated to the team\n\n---\n\n**AI Quality Checklist**: Before implementing patterns, ensure:\n- [x] Pattern is appropriate for the specific use case\n- [x] Implementation follows established conventions\n- [x] Error handling is comprehensive\n- [x] Performance requirements are met\n- [x] Security patterns are followed\n- [x] Code quality standards are maintained\n- [x] Documentation is updated\n- [x] Tests are implemented",
        "subsections": []
      }
    ],
    "code_blocks": [
      {
        "language": "python",
        "code": "# Standard module organization\n\"\"\"\nModule docstring explaining purpose and usage.\n\"\"\"\n\n# Imports organized by type\nimport os\nimport logging\nfrom typing import Optional\n\nimport aiohttp\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nfrom .config import get_module_config\nfrom .exceptions import ModuleSpecificException\n\n# Module-level configuration\nconfig = get_module_config()\nlogger = logging.getLogger(__name__)\n\n# Constants\nDEFAULT_TIMEOUT = 30\nMAX_RETRIES = 3\n\n# Main classes and functions\nclass MainClass:\n    \"\"\"Primary class for this module.\"\"\"\n\n    def __init__(self, config: dict):\n        self.config = config\n\n    async def main_method(self) -> dict:\n        \"\"\"Main method with proper error handling.\"\"\"\n        try:\n            result = await self._internal_method()\n            return result\n        except Exception as e:\n            logger.error(f\"Main method failed: {e}\")\n            raise ModuleSpecificException(\"User-friendly message\") from e\n\n    async def _internal_method(self) -> dict:\n        \"\"\"Internal method for implementation details.\"\"\"\n        pass"
      },
      {
        "language": "python",
        "code": "# Load configuration once at module level\nconfig = get_module_config()\n\ndef some_function():\n    # Use module-level config instead of loading again\n    processor = DataProcessor(config=config)"
      },
      {
        "language": "python",
        "code": "try:\n    # Operation that might fail\n    result = await some_operation()\nexcept SpecificException as e:\n    logger.error(f\"Operation failed: {e}\")\n    raise CustomException(\"User-friendly message\") from e\nexcept Exception as e:\n    logger.error(f\"Unexpected error: {e}\")\n    raise CustomException(\"An unexpected error occurred\") from e"
      },
      {
        "language": "python",
        "code": "async with AsyncSessionLocal() as session:\n    async with session.begin():\n        # Database operations\n        session.add(model)\n        await session.commit()"
      },
      {
        "language": "python",
        "code": "from sqlalchemy import Column, Integer, String, DateTime\nfrom sqlalchemy.ext.declarative import declarative_base\n\nBase = declarative_base()\n\nclass StockData(Base):\n    __tablename__ = \"stock_data\"\n\n    id = Column(Integer, primary_key=True, index=True)\n    symbol = Column(String, index=True, nullable=False)\n    date = Column(DateTime, nullable=False)\n    price = Column(Integer, nullable=False)"
      },
      {
        "language": "python",
        "code": "\"\"\"Add stock data table\n\nRevision ID: abc123def456\nRevises: previous_revision\nCreate Date: 2024-01-01 12:00:00.000000\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa\n\ndef upgrade():\n    op.create_table(\n        'stock_data',\n        sa.Column('id', sa.Integer(), nullable=False),\n        sa.Column('symbol', sa.String(), nullable=False),\n        sa.Column('date', sa.DateTime(), nullable=False),\n        sa.Column('price', sa.Integer(), nullable=False),\n        sa.PrimaryKeyConstraint('id')\n    )\n    op.create_index(op.f('ix_stock_data_symbol'), 'stock_data', ['symbol'], unique=False)\n\ndef downgrade():\n    op.drop_index(op.f('ix_stock_data_symbol'), table_name='stock_data')\n    op.drop_table('stock_data')"
      },
      {
        "language": "python",
        "code": "from fastapi import APIRouter, Depends, HTTPException\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nfrom ..db import get_session\nfrom ..models import StockData\nfrom ..schemas import StockDataResponse\n\nrouter = APIRouter()\n\n@router.get(\"/stock-data/{symbol}\", response_model=StockDataResponse)\nasync def get_stock_data(\n    symbol: str,\n    session: AsyncSession = Depends(get_session)\n) -> StockDataResponse:\n    \"\"\"Get stock data for a specific symbol.\"\"\"\n    try:\n        # Business logic here\n        data = await get_stock_data_from_db(session, symbol)\n        return StockDataResponse.from_orm(data)\n    except ValueError as e:\n        raise HTTPException(status_code=400, detail=str(e))\n    except Exception as e:\n        logger.error(f\"Failed to get stock data: {e}\")\n        raise HTTPException(status_code=500, detail=\"Internal server error\")"
      },
      {
        "language": "python",
        "code": "from pydantic import BaseModel, Field\nfrom datetime import datetime\nfrom typing import Optional\n\nclass StockDataResponse(BaseModel):\n    id: int\n    symbol: str = Field(..., description=\"Stock symbol\")\n    date: datetime\n    price: int = Field(..., description=\"Price in cents\")\n\n    class Config:\n        from_attributes = True"
      },
      {
        "language": "python",
        "code": "from .background_tasks import background_task\n\n@background_task\nasync def process_stock_data():\n    \"\"\"Background task for processing stock data.\"\"\"\n    try:\n        config = get_stock_data_config()\n        processor = StockDataProcessor(config)\n        await processor.process_all_files()\n    except Exception as e:\n        logger.error(f\"Stock data processing failed: {e}\")\n        # Don't re-raise - background tasks should handle errors gracefully"
      },
      {
        "language": "python",
        "code": "# In main.py or appropriate startup location\nfrom .background_tasks import schedule_periodic_task\n\n# Schedule periodic tasks\nschedule_periodic_task(\n    task_name=\"process_stock_data\",\n    interval_seconds=300,  # 5 minutes\n    task_function=process_stock_data\n)"
      },
      {
        "language": "python",
        "code": "# Standard file organization structure\ncream_api/\n├── files/\n│   ├── raw_responses/      # Raw data files\n│   └── parsed_responses/   # Processed data files\n├── stock_data/\n│   ├── config.py          # Configuration\n│   ├── models.py          # Database models\n│   ├── retriever.py       # Data retrieval\n│   ├── parser.py          # Data parsing\n│   ├── loader.py          # Data loading\n│   ├── processor.py       # File processing\n│   ├── tasks.py           # Background tasks\n│   └── api.py             # API endpoints"
      },
      {
        "language": "python",
        "code": "class FileProcessor:\n    \"\"\"Orchestrates file processing workflow.\"\"\"\n\n    def __init__(self, config: dict):\n        self.config = config\n        self.parser = DataParser(config)\n        self.loader = DataLoader(config)\n\n    async def process_file(self, file_path: str) -> bool:\n        \"\"\"Process a single file through the complete pipeline.\"\"\"\n        try:\n            # Parse the file\n            data = await self.parser.parse_file(file_path)\n\n            # Load data to database\n            await self.loader.load_data(data)\n\n            # Move file to processed directory\n            await self._move_to_processed(file_path)\n\n            return True\n        except Exception as e:\n            logger.error(f\"File processing failed for {file_path}: {e}\")\n            return False"
      },
      {
        "language": "python",
        "code": "import pytest\nfrom unittest.mock import AsyncMock, patch\nfrom .models import StockData\nfrom .processor import FileProcessor\n\nclass TestFileProcessor:\n    \"\"\"Test cases for FileProcessor class.\"\"\"\n\n    @pytest.fixture\n    def processor(self):\n        \"\"\"Create processor instance for testing.\"\"\"\n        config = {\"test\": True}\n        return FileProcessor(config)\n\n    @pytest.mark.asyncio\n    async def test_process_file_success(self, processor):\n        \"\"\"Test successful file processing.\"\"\"\n        with patch.object(processor.parser, 'parse_file') as mock_parse:\n            mock_parse.return_value = [{\"symbol\": \"AAPL\", \"price\": 150}]\n\n            result = await processor.process_file(\"test_file.txt\")\n\n            assert result is True\n            mock_parse.assert_called_once_with(\"test_file.txt\")\n\n    @pytest.mark.asyncio\n    async def test_process_file_failure(self, processor):\n        \"\"\"Test file processing failure.\"\"\"\n        with patch.object(processor.parser, 'parse_file') as mock_parse:\n            mock_parse.side_effect = Exception(\"Parse failed\")\n\n            result = await processor.process_file(\"test_file.txt\")\n\n            assert result is False"
      },
      {
        "language": "python",
        "code": "import pytest\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom .models import StockData\nfrom .api import get_stock_data\n\n@pytest.mark.asyncio\nasync def test_get_stock_data_integration(async_session: AsyncSession):\n    \"\"\"Test stock data retrieval integration.\"\"\"\n    # Setup test data\n    test_data = StockData(\n        symbol=\"AAPL\",\n        date=datetime.now(),\n        price=15000\n    )\n    async_session.add(test_data)\n    await async_session.commit()\n\n    # Test the endpoint\n    result = await get_stock_data(\"AAPL\", async_session)\n\n    assert result.symbol == \"AAPL\"\n    assert result.price == 15000"
      },
      {
        "language": "python",
        "code": "import os\nfrom pydantic import BaseSettings\n\nclass StockDataConfig(BaseSettings):\n    \"\"\"Configuration for stock data processing.\"\"\"\n\n    # File paths\n    raw_responses_dir: str = \"cream_api/files/raw_responses\"\n    parsed_responses_dir: str = \"cream_api/files/parsed_responses\"\n\n    # HTTP settings\n    timeout_seconds: int = 30\n    max_retries: int = 3\n    retry_delay_seconds: int = 5\n\n    # Processing settings\n    batch_size: int = 100\n    max_concurrent_files: int = 5\n\n    class Config:\n        env_prefix = \"STOCK_DATA_\""
      },
      {
        "language": "python",
        "code": "def get_stock_data_config() -> StockDataConfig:\n    \"\"\"Get stock data configuration.\"\"\"\n    return StockDataConfig()\n\n# Load once at module level\nconfig = get_stock_data_config()"
      },
      {
        "language": "python",
        "code": "import logging\nimport json\nfrom datetime import datetime\n\nclass StructuredFormatter(logging.Formatter):\n    \"\"\"Structured JSON logging formatter.\"\"\"\n\n    def format(self, record):\n        log_entry = {\n            \"timestamp\": datetime.utcnow().isoformat(),\n            \"level\": record.levelname,\n            \"module\": record.module,\n            \"function\": record.funcName,\n            \"line\": record.lineno,\n            \"message\": record.getMessage()\n        }\n\n        if hasattr(record, 'extra_fields'):\n            log_entry.update(record.extra_fields)\n\n        return json.dumps(log_entry)\n\n# Configure logging\nlogger = logging.getLogger(__name__)\nhandler = logging.StreamHandler()\nhandler.setFormatter(StructuredFormatter())\nlogger.addHandler(handler)\nlogger.setLevel(logging.INFO)"
      },
      {
        "language": "python",
        "code": "# Standard logging patterns\nlogger.info(\"Processing started\", extra={\"file_count\": 10})\nlogger.warning(\"Retry attempt\", extra={\"attempt\": 3, \"max_retries\": 5})\nlogger.error(\"Processing failed\", extra={\"file\": \"data.txt\", \"error\": str(e)})\nlogger.debug(\"Debug information\", extra={\"data_size\": len(data)})"
      },
      {
        "language": "python",
        "code": "class StockDataException(Exception):\n    \"\"\"Base exception for stock data operations.\"\"\"\n    pass\n\nclass StockDataRetrievalError(StockDataException):\n    \"\"\"Raised when stock data retrieval fails.\"\"\"\n    pass\n\nclass StockDataParsingError(StockDataException):\n    \"\"\"Raised when stock data parsing fails.\"\"\"\n    pass\n\nclass StockDataLoadingError(StockDataException):\n    \"\"\"Raised when stock data loading fails.\"\"\"\n    pass"
      },
      {
        "language": "python",
        "code": "async def process_with_retry(operation, max_retries: int = 3):\n    \"\"\"Execute operation with retry logic.\"\"\"\n    for attempt in range(max_retries):\n        try:\n            return await operation()\n        except Exception as e:\n            if attempt == max_retries - 1:\n                raise\n            logger.warning(f\"Attempt {attempt + 1} failed, retrying: {e}\")\n            await asyncio.sleep(2 ** attempt)  # Exponential backoff"
      },
      {
        "language": "python",
        "code": "async def process_files_concurrently(files: list[str], max_concurrent: int = 5):\n    \"\"\"Process multiple files concurrently with semaphore limiting.\"\"\"\n    semaphore = asyncio.Semaphore(max_concurrent)\n\n    async def process_single_file(file_path: str):\n        async with semaphore:\n            return await process_file(file_path)\n\n    tasks = [process_single_file(file) for file in files]\n    results = await asyncio.gather(*tasks, return_exceptions=True)\n\n    return results"
      },
      {
        "language": "python",
        "code": "async def batch_insert_data(session: AsyncSession, data: list[dict]):\n    \"\"\"Insert data in batches for better performance.\"\"\"\n    batch_size = 1000\n\n    for i in range(0, len(data), batch_size):\n        batch = data[i:i + batch_size]\n        models = [StockData(**item) for item in batch]\n        session.add_all(models)\n        await session.commit()"
      },
      {
        "language": "python",
        "code": "from pydantic import BaseModel, validator\nimport re\n\nclass StockSymbolRequest(BaseModel):\n    symbol: str\n\n    @validator('symbol')\n    def validate_symbol(cls, v):\n        if not re.match(r'^[A-Z]{1,10}$', v):\n            raise ValueError('Symbol must be 1-10 uppercase letters')\n        return v.upper()"
      },
      {
        "language": "python",
        "code": "from fastapi import Depends, HTTPException, status\nfrom fastapi.security import HTTPBearer\n\nsecurity = HTTPBearer()\n\nasync def get_current_user(token: str = Depends(security)):\n    \"\"\"Validate user authentication.\"\"\"\n    try:\n        # Validate token and return user\n        user = validate_token(token.credentials)\n        return user\n    except Exception:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Invalid authentication credentials\"\n        )\n\nasync def require_admin(user = Depends(get_current_user)):\n    \"\"\"Require admin privileges.\"\"\"\n    if not user.is_admin:\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=\"Admin privileges required\"\n        )\n    return user"
      }
    ],
    "links": [
      {
        "type": "code_reference",
        "text": "../guide_docs/Core%20Principles.md"
      },
      {
        "type": "code_reference",
        "text": "Architecture%20Overview.md"
      },
      {
        "type": "code_reference",
        "text": "Development%20Workflow.md"
      },
      {
        "type": "code_reference",
        "text": "../guide_docs/Language-Specific/Python%20Style%20Guide.md"
      },
      {
        "type": "code_reference",
        "text": "../guide_docs/Language-Specific/FastAPI%20Development%20Guide.md"
      },
      {
        "type": "code_reference",
        "text": "python\n# Standard module organization\n\"\"\"\nModule docstring explaining purpose and usage.\n\"\"\"\n\n# Imports organized by type\nimport os\nimport logging\nfrom typing import Optional\n\nimport aiohttp\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nfrom .config import get_module_config\nfrom .exceptions import ModuleSpecificException\n\n# Module-level configuration\nconfig = get_module_config()\nlogger = logging.getLogger(__name__)\n\n# Constants\nDEFAULT_TIMEOUT = 30\nMAX_RETRIES = 3\n\n# Main classes and functions\nclass MainClass:\n    \"\"\"Primary class for this module.\"\"\"\n\n    def __init__(self, config: dict):\n        self.config = config\n\n    async def main_method(self) -> dict:\n        \"\"\"Main method with proper error handling.\"\"\"\n        try:\n            result = await self._internal_method()\n            return result\n        except Exception as e:\n            logger.error(f\"Main method failed: {e}\")\n            raise ModuleSpecificException(\"User-friendly message\") from e\n\n    async def _internal_method(self) -> dict:\n        \"\"\"Internal method for implementation details.\"\"\"\n        pass\n"
      },
      {
        "type": "code_reference",
        "text": "\n\nThis pattern will inform the structure of all new modules and classes in the project.\n\nModule structure must follow this exact organization with proper imports, configuration, and error handling.\n\n### Configuration Pattern\n"
      },
      {
        "type": "code_reference",
        "text": "\n\nThis pattern will inform how to handle configuration loading and usage throughout the project.\n\nConfiguration must be loaded once at module level and reused, not loaded multiple times.\n\n### Error Handling Pattern\n"
      },
      {
        "type": "code_reference",
        "text": "python\ntry:\n    # Operation that might fail\n    result = await some_operation()\nexcept SpecificException as e:\n    logger.error(f\"Operation failed: {e}\")\n    raise CustomException(\"User-friendly message\") from e\nexcept Exception as e:\n    logger.error(f\"Unexpected error: {e}\")\n    raise CustomException(\"An unexpected error occurred\") from e\n"
      },
      {
        "type": "code_reference",
        "text": "\n\nThis pattern will inform error handling implementation for all async operations and external calls.\n\nError handling must include specific exception types, proper logging, and user-friendly error messages.\n\n## Database Patterns\n\n### Session Management\n"
      },
      {
        "type": "code_reference",
        "text": "python\nasync with AsyncSessionLocal() as session:\n    async with session.begin():\n        # Database operations\n        session.add(model)\n        await session.commit()\n"
      },
      {
        "type": "code_reference",
        "text": "\n\nThis pattern will inform database session management for all database operations.\n\nDatabase sessions must use proper async context managers and transaction management.\n\n### Model Definition\n"
      },
      {
        "type": "code_reference",
        "text": "python\nfrom sqlalchemy import Column, Integer, String, DateTime\nfrom sqlalchemy.ext.declarative import declarative_base\n\nBase = declarative_base()\n\nclass StockData(Base):\n    __tablename__ = \"stock_data\"\n\n    id = Column(Integer, primary_key=True, index=True)\n    symbol = Column(String, index=True, nullable=False)\n    date = Column(DateTime, nullable=False)\n    price = Column(Integer, nullable=False)\n"
      },
      {
        "type": "code_reference",
        "text": "\n\nThis pattern will inform SQLAlchemy model definitions with proper indexing and constraints.\n\nModels must include proper indexes, constraints, and follow naming conventions.\n\n### Migration Pattern\n"
      },
      {
        "type": "code_reference",
        "text": "python\n\"\"\"Add stock data table\n\nRevision ID: abc123def456\nRevises: previous_revision\nCreate Date: 2024-01-01 12:00:00.000000\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa\n\ndef upgrade():\n    op.create_table(\n        'stock_data',\n        sa.Column('id', sa.Integer(), nullable=False),\n        sa.Column('symbol', sa.String(), nullable=False),\n        sa.Column('date', sa.DateTime(), nullable=False),\n        sa.Column('price', sa.Integer(), nullable=False),\n        sa.PrimaryKeyConstraint('id')\n    )\n    op.create_index(op.f('ix_stock_data_symbol'), 'stock_data', ['symbol'], unique=False)\n\ndef downgrade():\n    op.drop_index(op.f('ix_stock_data_symbol'), table_name='stock_data')\n    op.drop_table('stock_data')\n"
      },
      {
        "type": "code_reference",
        "text": "\n\nThis pattern will inform Alembic migration file structure and database schema changes.\n\nMigrations must include proper upgrade and downgrade functions with descriptive docstrings.\n\n## API Patterns\n\n### FastAPI Endpoint\n"
      },
      {
        "type": "code_reference",
        "text": "python\nfrom fastapi import APIRouter, Depends, HTTPException\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nfrom ..db import get_session\nfrom ..models import StockData\nfrom ..schemas import StockDataResponse\n\nrouter = APIRouter()\n\n@router.get(\"/stock-data/{symbol}\", response_model=StockDataResponse)\nasync def get_stock_data(\n    symbol: str,\n    session: AsyncSession = Depends(get_session)\n) -> StockDataResponse:\n    \"\"\"Get stock data for a specific symbol.\"\"\"\n    try:\n        # Business logic here\n        data = await get_stock_data_from_db(session, symbol)\n        return StockDataResponse.from_orm(data)\n    except ValueError as e:\n        raise HTTPException(status_code=400, detail=str(e))\n    except Exception as e:\n        logger.error(f\"Failed to get stock data: {e}\")\n        raise HTTPException(status_code=500, detail=\"Internal server error\")\n"
      },
      {
        "type": "code_reference",
        "text": "\n\nThis pattern will inform FastAPI endpoint implementation with proper error handling and response models.\n\nEndpoints must include proper error handling, logging, and response model validation.\n\n### Pydantic Schema\n"
      },
      {
        "type": "code_reference",
        "text": "python\nfrom pydantic import BaseModel, Field\nfrom datetime import datetime\nfrom typing import Optional\n\nclass StockDataResponse(BaseModel):\n    id: int\n    symbol: str = Field(..., description=\"Stock symbol\")\n    date: datetime\n    price: int = Field(..., description=\"Price in cents\")\n\n    class Config:\n        from_attributes = True\n"
      },
      {
        "type": "code_reference",
        "text": "\n\nThis pattern will inform Pydantic schema definitions with proper field validation and descriptions.\n\nSchemas must include proper field types, descriptions, and configuration for ORM compatibility.\n\n## Background Task Patterns\n\n### Task Definition\n"
      },
      {
        "type": "code_reference",
        "text": "python\nfrom .background_tasks import background_task\n\n@background_task\nasync def process_stock_data():\n    \"\"\"Background task for processing stock data.\"\"\"\n    try:\n        config = get_stock_data_config()\n        processor = StockDataProcessor(config)\n        await processor.process_all_files()\n    except Exception as e:\n        logger.error(f\"Stock data processing failed: {e}\")\n        # Don't re-raise - background tasks should handle errors gracefully\n"
      },
      {
        "type": "code_reference",
        "text": "\n\nThis pattern will inform background task implementation with proper error handling and logging.\n\nBackground tasks must handle errors gracefully and not re-raise exceptions.\n\n### Task Scheduling\n"
      },
      {
        "type": "code_reference",
        "text": "python\n# In main.py or appropriate startup location\nfrom .background_tasks import schedule_periodic_task\n\n# Schedule periodic tasks\nschedule_periodic_task(\n    task_name=\"process_stock_data\",\n    interval_seconds=300,  # 5 minutes\n    task_function=process_stock_data\n)\n"
      },
      {
        "type": "code_reference",
        "text": "\n\nThis pattern will inform how to schedule and configure background tasks.\n\nTask scheduling must include proper intervals and error handling.\n\n## File Processing Patterns\n\n### File Organization\n"
      },
      {
        "type": "code_reference",
        "text": "python\n# Standard file organization structure\ncream_api/\n├── files/\n│   ├── raw_responses/      # Raw data files\n│   └── parsed_responses/   # Processed data files\n├── stock_data/\n│   ├── config.py          # Configuration\n│   ├── models.py          # Database models\n│   ├── retriever.py       # Data retrieval\n│   ├── parser.py          # Data parsing\n│   ├── loader.py          # Data loading\n│   ├── processor.py       # File processing\n│   ├── tasks.py           # Background tasks\n│   └── api.py             # API endpoints\n"
      },
      {
        "type": "code_reference",
        "text": "\n\nThis pattern will inform file organization and module structure for new features.\n\nFile organization must follow this structure for consistency and maintainability.\n\n### File Processing Workflow\n"
      },
      {
        "type": "code_reference",
        "text": "python\nclass FileProcessor:\n    \"\"\"Orchestrates file processing workflow.\"\"\"\n\n    def __init__(self, config: dict):\n        self.config = config\n        self.parser = DataParser(config)\n        self.loader = DataLoader(config)\n\n    async def process_file(self, file_path: str) -> bool:\n        \"\"\"Process a single file through the complete pipeline.\"\"\"\n        try:\n            # Parse the file\n            data = await self.parser.parse_file(file_path)\n\n            # Load data to database\n            await self.loader.load_data(data)\n\n            # Move file to processed directory\n            await self._move_to_processed(file_path)\n\n            return True\n        except Exception as e:\n            logger.error(f\"File processing failed for {file_path}: {e}\")\n            return False\n"
      },
      {
        "type": "code_reference",
        "text": "\n\nThis pattern will inform file processing workflow implementation with proper error handling.\n\nFile processing must include proper error handling, logging, and file movement operations.\n\n## Testing Patterns\n\n### Unit Test Structure\n"
      },
      {
        "type": "code_reference",
        "text": "python\nimport pytest\nfrom unittest.mock import AsyncMock, patch\nfrom .models import StockData\nfrom .processor import FileProcessor\n\nclass TestFileProcessor:\n    \"\"\"Test cases for FileProcessor class.\"\"\"\n\n    @pytest.fixture\n    def processor(self):\n        \"\"\"Create processor instance for testing.\"\"\"\n        config = {\"test\": True}\n        return FileProcessor(config)\n\n    @pytest.mark.asyncio\n    async def test_process_file_success(self, processor):\n        \"\"\"Test successful file processing.\"\"\"\n        with patch.object(processor.parser, 'parse_file') as mock_parse:\n            mock_parse.return_value = [{\"symbol\": \"AAPL\", \"price\": 150}]\n\n            result = await processor.process_file(\"test_file.txt\")\n\n            assert result is True\n            mock_parse.assert_called_once_with(\"test_file.txt\")\n\n    @pytest.mark.asyncio\n    async def test_process_file_failure(self, processor):\n        \"\"\"Test file processing failure.\"\"\"\n        with patch.object(processor.parser, 'parse_file') as mock_parse:\n            mock_parse.side_effect = Exception(\"Parse failed\")\n\n            result = await processor.process_file(\"test_file.txt\")\n\n            assert result is False\n"
      },
      {
        "type": "code_reference",
        "text": "\n\nThis pattern will inform unit test structure with proper fixtures and async testing.\n\nTests must include proper fixtures, async testing, and comprehensive error case coverage.\n\n### Integration Test Structure\n"
      },
      {
        "type": "code_reference",
        "text": "python\nimport pytest\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom .models import StockData\nfrom .api import get_stock_data\n\n@pytest.mark.asyncio\nasync def test_get_stock_data_integration(async_session: AsyncSession):\n    \"\"\"Test stock data retrieval integration.\"\"\"\n    # Setup test data\n    test_data = StockData(\n        symbol=\"AAPL\",\n        date=datetime.now(),\n        price=15000\n    )\n    async_session.add(test_data)\n    await async_session.commit()\n\n    # Test the endpoint\n    result = await get_stock_data(\"AAPL\", async_session)\n\n    assert result.symbol == \"AAPL\"\n    assert result.price == 15000\n"
      },
      {
        "type": "code_reference",
        "text": "\n\nThis pattern will inform integration test structure with database setup and teardown.\n\nIntegration tests must include proper database setup, test data creation, and cleanup.\n\n## Configuration Patterns\n\n### Environment-Based Configuration\n"
      },
      {
        "type": "code_reference",
        "text": "python\nimport os\nfrom pydantic import BaseSettings\n\nclass StockDataConfig(BaseSettings):\n    \"\"\"Configuration for stock data processing.\"\"\"\n\n    # File paths\n    raw_responses_dir: str = \"cream_api/files/raw_responses\"\n    parsed_responses_dir: str = \"cream_api/files/parsed_responses\"\n\n    # HTTP settings\n    timeout_seconds: int = 30\n    max_retries: int = 3\n    retry_delay_seconds: int = 5\n\n    # Processing settings\n    batch_size: int = 100\n    max_concurrent_files: int = 5\n\n    class Config:\n        env_prefix = \"STOCK_DATA_\"\n"
      },
      {
        "type": "code_reference",
        "text": "\n\nThis pattern will inform configuration class structure with environment variable support.\n\nConfiguration must use Pydantic for validation and support environment variable overrides.\n\n### Configuration Loading\n"
      },
      {
        "type": "code_reference",
        "text": "python\ndef get_stock_data_config() -> StockDataConfig:\n    \"\"\"Get stock data configuration.\"\"\"\n    return StockDataConfig()\n\n# Load once at module level\nconfig = get_stock_data_config()\n"
      },
      {
        "type": "code_reference",
        "text": "\n\nThis pattern will inform configuration loading and usage throughout modules.\n\nConfiguration must be loaded once at module level and reused, not loaded multiple times.\n\n## Logging Patterns\n\n### Structured Logging\n"
      },
      {
        "type": "code_reference",
        "text": "python\nimport logging\nimport json\nfrom datetime import datetime\n\nclass StructuredFormatter(logging.Formatter):\n    \"\"\"Structured JSON logging formatter.\"\"\"\n\n    def format(self, record):\n        log_entry = {\n            \"timestamp\": datetime.utcnow().isoformat(),\n            \"level\": record.levelname,\n            \"module\": record.module,\n            \"function\": record.funcName,\n            \"line\": record.lineno,\n            \"message\": record.getMessage()\n        }\n\n        if hasattr(record, 'extra_fields'):\n            log_entry.update(record.extra_fields)\n\n        return json.dumps(log_entry)\n\n# Configure logging\nlogger = logging.getLogger(__name__)\nhandler = logging.StreamHandler()\nhandler.setFormatter(StructuredFormatter())\nlogger.addHandler(handler)\nlogger.setLevel(logging.INFO)\n"
      },
      {
        "type": "code_reference",
        "text": "\n\nThis pattern will inform structured logging implementation with JSON formatting.\n\nLogging must include structured format with timestamps, levels, and contextual information.\n\n### Logging Usage\n"
      },
      {
        "type": "code_reference",
        "text": "python\n# Standard logging patterns\nlogger.info(\"Processing started\", extra={\"file_count\": 10})\nlogger.warning(\"Retry attempt\", extra={\"attempt\": 3, \"max_retries\": 5})\nlogger.error(\"Processing failed\", extra={\"file\": \"data.txt\", \"error\": str(e)})\nlogger.debug(\"Debug information\", extra={\"data_size\": len(data)})\n"
      },
      {
        "type": "code_reference",
        "text": "\n\nThis pattern will inform logging usage with proper levels and contextual information.\n\nLogging must use appropriate levels and include relevant contextual information.\n\n## Error Handling Patterns\n\n### Custom Exceptions\n"
      },
      {
        "type": "code_reference",
        "text": "python\nclass StockDataException(Exception):\n    \"\"\"Base exception for stock data operations.\"\"\"\n    pass\n\nclass StockDataRetrievalError(StockDataException):\n    \"\"\"Raised when stock data retrieval fails.\"\"\"\n    pass\n\nclass StockDataParsingError(StockDataException):\n    \"\"\"Raised when stock data parsing fails.\"\"\"\n    pass\n\nclass StockDataLoadingError(StockDataException):\n    \"\"\"Raised when stock data loading fails.\"\"\"\n    pass\n"
      },
      {
        "type": "code_reference",
        "text": "\n\nThis pattern will inform custom exception hierarchy for domain-specific error handling.\n\nCustom exceptions must inherit from appropriate base classes and provide meaningful error messages.\n\n### Error Recovery\n"
      },
      {
        "type": "code_reference",
        "text": "python\nasync def process_with_retry(operation, max_retries: int = 3):\n    \"\"\"Execute operation with retry logic.\"\"\"\n    for attempt in range(max_retries):\n        try:\n            return await operation()\n        except Exception as e:\n            if attempt == max_retries - 1:\n                raise\n            logger.warning(f\"Attempt {attempt + 1} failed, retrying: {e}\")\n            await asyncio.sleep(2 ** attempt)  # Exponential backoff\n"
      },
      {
        "type": "code_reference",
        "text": "\n\nThis pattern will inform retry logic implementation with exponential backoff.\n\nRetry logic must include proper backoff strategies and maximum retry limits.\n\n## Performance Patterns\n\n### Async Operations\n"
      },
      {
        "type": "code_reference",
        "text": "python\nasync def process_files_concurrently(files: list[str], max_concurrent: int = 5):\n    \"\"\"Process multiple files concurrently with semaphore limiting.\"\"\"\n    semaphore = asyncio.Semaphore(max_concurrent)\n\n    async def process_single_file(file_path: str):\n        async with semaphore:\n            return await process_file(file_path)\n\n    tasks = [process_single_file(file) for file in files]\n    results = await asyncio.gather(*tasks, return_exceptions=True)\n\n    return results\n"
      },
      {
        "type": "code_reference",
        "text": "\n\nThis pattern will inform concurrent processing implementation with proper resource limiting.\n\nConcurrent operations must include proper resource limiting and error handling.\n\n### Database Optimization\n"
      },
      {
        "type": "code_reference",
        "text": "python\nasync def batch_insert_data(session: AsyncSession, data: list[dict]):\n    \"\"\"Insert data in batches for better performance.\"\"\"\n    batch_size = 1000\n\n    for i in range(0, len(data), batch_size):\n        batch = data[i:i + batch_size]\n        models = [StockData(**item) for item in batch]\n        session.add_all(models)\n        await session.commit()\n"
      },
      {
        "type": "code_reference",
        "text": "\n\nThis pattern will inform batch database operations for improved performance.\n\nBatch operations must include proper batch sizes and transaction management.\n\n## Security Patterns\n\n### Input Validation\n"
      },
      {
        "type": "code_reference",
        "text": "python\nfrom pydantic import BaseModel, validator\nimport re\n\nclass StockSymbolRequest(BaseModel):\n    symbol: str\n\n    @validator('symbol')\n    def validate_symbol(cls, v):\n        if not re.match(r'^[A-Z]{1,10}$', v):\n            raise ValueError('Symbol must be 1-10 uppercase letters')\n        return v.upper()\n"
      },
      {
        "type": "code_reference",
        "text": "\n\nThis pattern will inform input validation implementation with Pydantic validators.\n\nInput validation must include proper regex patterns and meaningful error messages.\n\n### Authentication and Authorization\n"
      },
      {
        "type": "code_reference",
        "text": "python\nfrom fastapi import Depends, HTTPException, status\nfrom fastapi.security import HTTPBearer\n\nsecurity = HTTPBearer()\n\nasync def get_current_user(token: str = Depends(security)):\n    \"\"\"Validate user authentication.\"\"\"\n    try:\n        # Validate token and return user\n        user = validate_token(token.credentials)\n        return user\n    except Exception:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Invalid authentication credentials\"\n        )\n\nasync def require_admin(user = Depends(get_current_user)):\n    \"\"\"Require admin privileges.\"\"\"\n    if not user.is_admin:\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=\"Admin privileges required\"\n        )\n    return user\n"
      }
    ],
    "raw_content": "# Common Development Patterns\n\n> This document outlines common patterns and practices used throughout the project. Use these patterns to ensure consistency and follow established conventions.\n\n## AI Metadata\n\n**Template Version:** 2.1\n**AI Processing Level:** High\n**Required Context:** Project architecture, existing codebase, implementation requirements\n**Validation Required:** Yes\n**Code Generation:** Supported\n\n**Dependencies:**\n- `../guide_docs/Core%20Principles.md` - Decision-making frameworks\n- `Architecture%20Overview.md` - System architecture\n- `Development%20Workflow.md` - Development process\n- `../guide_docs/Language-Specific/Python%20Style%20Guide.md` - Python implementation patterns\n- `../guide_docs/Language-Specific/FastAPI%20Development%20Guide.md` - API development patterns\n\n**Validation Rules:**\n- All patterns must reference actual project structure and files\n- Code examples must follow established project conventions\n- Pattern application must be consistent across the codebase\n- Implementation must align with project architecture\n- Error handling must follow established patterns\n\n## Overview\n\n**Document Purpose:** Established patterns and conventions for the CreamPie project\n**Scope:** All code organization, database operations, API development, and background tasks\n**Target Users:** AI assistants and developers implementing project features\n**Last Updated:** Current\n\n**AI Context:** This document provides the foundational patterns that must be followed when implementing any feature in the CreamPie project. It ensures consistency, maintainability, and alignment with the established architecture.\n\n## Code Organization Patterns\n\n### Module Structure\n```python\n# Standard module organization\n\"\"\"\nModule docstring explaining purpose and usage.\n\"\"\"\n\n# Imports organized by type\nimport os\nimport logging\nfrom typing import Optional\n\nimport aiohttp\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nfrom .config import get_module_config\nfrom .exceptions import ModuleSpecificException\n\n# Module-level configuration\nconfig = get_module_config()\nlogger = logging.getLogger(__name__)\n\n# Constants\nDEFAULT_TIMEOUT = 30\nMAX_RETRIES = 3\n\n# Main classes and functions\nclass MainClass:\n    \"\"\"Primary class for this module.\"\"\"\n\n    def __init__(self, config: dict):\n        self.config = config\n\n    async def main_method(self) -> dict:\n        \"\"\"Main method with proper error handling.\"\"\"\n        try:\n            result = await self._internal_method()\n            return result\n        except Exception as e:\n            logger.error(f\"Main method failed: {e}\")\n            raise ModuleSpecificException(\"User-friendly message\") from e\n\n    async def _internal_method(self) -> dict:\n        \"\"\"Internal method for implementation details.\"\"\"\n        pass\n```\n\nThis pattern will inform the structure of all new modules and classes in the project.\n\nModule structure must follow this exact organization with proper imports, configuration, and error handling.\n\n### Configuration Pattern\n```python\n# Load configuration once at module level\nconfig = get_module_config()\n\ndef some_function():\n    # Use module-level config instead of loading again\n    processor = DataProcessor(config=config)\n```\n\nThis pattern will inform how to handle configuration loading and usage throughout the project.\n\nConfiguration must be loaded once at module level and reused, not loaded multiple times.\n\n### Error Handling Pattern\n```python\ntry:\n    # Operation that might fail\n    result = await some_operation()\nexcept SpecificException as e:\n    logger.error(f\"Operation failed: {e}\")\n    raise CustomException(\"User-friendly message\") from e\nexcept Exception as e:\n    logger.error(f\"Unexpected error: {e}\")\n    raise CustomException(\"An unexpected error occurred\") from e\n```\n\nThis pattern will inform error handling implementation for all async operations and external calls.\n\nError handling must include specific exception types, proper logging, and user-friendly error messages.\n\n## Database Patterns\n\n### Session Management\n```python\nasync with AsyncSessionLocal() as session:\n    async with session.begin():\n        # Database operations\n        session.add(model)\n        await session.commit()\n```\n\nThis pattern will inform database session management for all database operations.\n\nDatabase sessions must use proper async context managers and transaction management.\n\n### Model Definition\n```python\nfrom sqlalchemy import Column, Integer, String, DateTime\nfrom sqlalchemy.ext.declarative import declarative_base\n\nBase = declarative_base()\n\nclass StockData(Base):\n    __tablename__ = \"stock_data\"\n\n    id = Column(Integer, primary_key=True, index=True)\n    symbol = Column(String, index=True, nullable=False)\n    date = Column(DateTime, nullable=False)\n    price = Column(Integer, nullable=False)\n```\n\nThis pattern will inform SQLAlchemy model definitions with proper indexing and constraints.\n\nModels must include proper indexes, constraints, and follow naming conventions.\n\n### Migration Pattern\n```python\n\"\"\"Add stock data table\n\nRevision ID: abc123def456\nRevises: previous_revision\nCreate Date: 2024-01-01 12:00:00.000000\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa\n\ndef upgrade():\n    op.create_table(\n        'stock_data',\n        sa.Column('id', sa.Integer(), nullable=False),\n        sa.Column('symbol', sa.String(), nullable=False),\n        sa.Column('date', sa.DateTime(), nullable=False),\n        sa.Column('price', sa.Integer(), nullable=False),\n        sa.PrimaryKeyConstraint('id')\n    )\n    op.create_index(op.f('ix_stock_data_symbol'), 'stock_data', ['symbol'], unique=False)\n\ndef downgrade():\n    op.drop_index(op.f('ix_stock_data_symbol'), table_name='stock_data')\n    op.drop_table('stock_data')\n```\n\nThis pattern will inform Alembic migration file structure and database schema changes.\n\nMigrations must include proper upgrade and downgrade functions with descriptive docstrings.\n\n## API Patterns\n\n### FastAPI Endpoint\n```python\nfrom fastapi import APIRouter, Depends, HTTPException\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nfrom ..db import get_session\nfrom ..models import StockData\nfrom ..schemas import StockDataResponse\n\nrouter = APIRouter()\n\n@router.get(\"/stock-data/{symbol}\", response_model=StockDataResponse)\nasync def get_stock_data(\n    symbol: str,\n    session: AsyncSession = Depends(get_session)\n) -> StockDataResponse:\n    \"\"\"Get stock data for a specific symbol.\"\"\"\n    try:\n        # Business logic here\n        data = await get_stock_data_from_db(session, symbol)\n        return StockDataResponse.from_orm(data)\n    except ValueError as e:\n        raise HTTPException(status_code=400, detail=str(e))\n    except Exception as e:\n        logger.error(f\"Failed to get stock data: {e}\")\n        raise HTTPException(status_code=500, detail=\"Internal server error\")\n```\n\nThis pattern will inform FastAPI endpoint implementation with proper error handling and response models.\n\nEndpoints must include proper error handling, logging, and response model validation.\n\n### Pydantic Schema\n```python\nfrom pydantic import BaseModel, Field\nfrom datetime import datetime\nfrom typing import Optional\n\nclass StockDataResponse(BaseModel):\n    id: int\n    symbol: str = Field(..., description=\"Stock symbol\")\n    date: datetime\n    price: int = Field(..., description=\"Price in cents\")\n\n    class Config:\n        from_attributes = True\n```\n\nThis pattern will inform Pydantic schema definitions with proper field validation and descriptions.\n\nSchemas must include proper field types, descriptions, and configuration for ORM compatibility.\n\n## Background Task Patterns\n\n### Task Definition\n```python\nfrom .background_tasks import background_task\n\n@background_task\nasync def process_stock_data():\n    \"\"\"Background task for processing stock data.\"\"\"\n    try:\n        config = get_stock_data_config()\n        processor = StockDataProcessor(config)\n        await processor.process_all_files()\n    except Exception as e:\n        logger.error(f\"Stock data processing failed: {e}\")\n        # Don't re-raise - background tasks should handle errors gracefully\n```\n\nThis pattern will inform background task implementation with proper error handling and logging.\n\nBackground tasks must handle errors gracefully and not re-raise exceptions.\n\n### Task Scheduling\n```python\n# In main.py or appropriate startup location\nfrom .background_tasks import schedule_periodic_task\n\n# Schedule periodic tasks\nschedule_periodic_task(\n    task_name=\"process_stock_data\",\n    interval_seconds=300,  # 5 minutes\n    task_function=process_stock_data\n)\n```\n\nThis pattern will inform how to schedule and configure background tasks.\n\nTask scheduling must include proper intervals and error handling.\n\n## File Processing Patterns\n\n### File Organization\n```python\n# Standard file organization structure\ncream_api/\n├── files/\n│   ├── raw_responses/      # Raw data files\n│   └── parsed_responses/   # Processed data files\n├── stock_data/\n│   ├── config.py          # Configuration\n│   ├── models.py          # Database models\n│   ├── retriever.py       # Data retrieval\n│   ├── parser.py          # Data parsing\n│   ├── loader.py          # Data loading\n│   ├── processor.py       # File processing\n│   ├── tasks.py           # Background tasks\n│   └── api.py             # API endpoints\n```\n\nThis pattern will inform file organization and module structure for new features.\n\nFile organization must follow this structure for consistency and maintainability.\n\n### File Processing Workflow\n```python\nclass FileProcessor:\n    \"\"\"Orchestrates file processing workflow.\"\"\"\n\n    def __init__(self, config: dict):\n        self.config = config\n        self.parser = DataParser(config)\n        self.loader = DataLoader(config)\n\n    async def process_file(self, file_path: str) -> bool:\n        \"\"\"Process a single file through the complete pipeline.\"\"\"\n        try:\n            # Parse the file\n            data = await self.parser.parse_file(file_path)\n\n            # Load data to database\n            await self.loader.load_data(data)\n\n            # Move file to processed directory\n            await self._move_to_processed(file_path)\n\n            return True\n        except Exception as e:\n            logger.error(f\"File processing failed for {file_path}: {e}\")\n            return False\n```\n\nThis pattern will inform file processing workflow implementation with proper error handling.\n\nFile processing must include proper error handling, logging, and file movement operations.\n\n## Testing Patterns\n\n### Unit Test Structure\n```python\nimport pytest\nfrom unittest.mock import AsyncMock, patch\nfrom .models import StockData\nfrom .processor import FileProcessor\n\nclass TestFileProcessor:\n    \"\"\"Test cases for FileProcessor class.\"\"\"\n\n    @pytest.fixture\n    def processor(self):\n        \"\"\"Create processor instance for testing.\"\"\"\n        config = {\"test\": True}\n        return FileProcessor(config)\n\n    @pytest.mark.asyncio\n    async def test_process_file_success(self, processor):\n        \"\"\"Test successful file processing.\"\"\"\n        with patch.object(processor.parser, 'parse_file') as mock_parse:\n            mock_parse.return_value = [{\"symbol\": \"AAPL\", \"price\": 150}]\n\n            result = await processor.process_file(\"test_file.txt\")\n\n            assert result is True\n            mock_parse.assert_called_once_with(\"test_file.txt\")\n\n    @pytest.mark.asyncio\n    async def test_process_file_failure(self, processor):\n        \"\"\"Test file processing failure.\"\"\"\n        with patch.object(processor.parser, 'parse_file') as mock_parse:\n            mock_parse.side_effect = Exception(\"Parse failed\")\n\n            result = await processor.process_file(\"test_file.txt\")\n\n            assert result is False\n```\n\nThis pattern will inform unit test structure with proper fixtures and async testing.\n\nTests must include proper fixtures, async testing, and comprehensive error case coverage.\n\n### Integration Test Structure\n```python\nimport pytest\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom .models import StockData\nfrom .api import get_stock_data\n\n@pytest.mark.asyncio\nasync def test_get_stock_data_integration(async_session: AsyncSession):\n    \"\"\"Test stock data retrieval integration.\"\"\"\n    # Setup test data\n    test_data = StockData(\n        symbol=\"AAPL\",\n        date=datetime.now(),\n        price=15000\n    )\n    async_session.add(test_data)\n    await async_session.commit()\n\n    # Test the endpoint\n    result = await get_stock_data(\"AAPL\", async_session)\n\n    assert result.symbol == \"AAPL\"\n    assert result.price == 15000\n```\n\nThis pattern will inform integration test structure with database setup and teardown.\n\nIntegration tests must include proper database setup, test data creation, and cleanup.\n\n## Configuration Patterns\n\n### Environment-Based Configuration\n```python\nimport os\nfrom pydantic import BaseSettings\n\nclass StockDataConfig(BaseSettings):\n    \"\"\"Configuration for stock data processing.\"\"\"\n\n    # File paths\n    raw_responses_dir: str = \"cream_api/files/raw_responses\"\n    parsed_responses_dir: str = \"cream_api/files/parsed_responses\"\n\n    # HTTP settings\n    timeout_seconds: int = 30\n    max_retries: int = 3\n    retry_delay_seconds: int = 5\n\n    # Processing settings\n    batch_size: int = 100\n    max_concurrent_files: int = 5\n\n    class Config:\n        env_prefix = \"STOCK_DATA_\"\n```\n\nThis pattern will inform configuration class structure with environment variable support.\n\nConfiguration must use Pydantic for validation and support environment variable overrides.\n\n### Configuration Loading\n```python\ndef get_stock_data_config() -> StockDataConfig:\n    \"\"\"Get stock data configuration.\"\"\"\n    return StockDataConfig()\n\n# Load once at module level\nconfig = get_stock_data_config()\n```\n\nThis pattern will inform configuration loading and usage throughout modules.\n\nConfiguration must be loaded once at module level and reused, not loaded multiple times.\n\n## Logging Patterns\n\n### Structured Logging\n```python\nimport logging\nimport json\nfrom datetime import datetime\n\nclass StructuredFormatter(logging.Formatter):\n    \"\"\"Structured JSON logging formatter.\"\"\"\n\n    def format(self, record):\n        log_entry = {\n            \"timestamp\": datetime.utcnow().isoformat(),\n            \"level\": record.levelname,\n            \"module\": record.module,\n            \"function\": record.funcName,\n            \"line\": record.lineno,\n            \"message\": record.getMessage()\n        }\n\n        if hasattr(record, 'extra_fields'):\n            log_entry.update(record.extra_fields)\n\n        return json.dumps(log_entry)\n\n# Configure logging\nlogger = logging.getLogger(__name__)\nhandler = logging.StreamHandler()\nhandler.setFormatter(StructuredFormatter())\nlogger.addHandler(handler)\nlogger.setLevel(logging.INFO)\n```\n\nThis pattern will inform structured logging implementation with JSON formatting.\n\nLogging must include structured format with timestamps, levels, and contextual information.\n\n### Logging Usage\n```python\n# Standard logging patterns\nlogger.info(\"Processing started\", extra={\"file_count\": 10})\nlogger.warning(\"Retry attempt\", extra={\"attempt\": 3, \"max_retries\": 5})\nlogger.error(\"Processing failed\", extra={\"file\": \"data.txt\", \"error\": str(e)})\nlogger.debug(\"Debug information\", extra={\"data_size\": len(data)})\n```\n\nThis pattern will inform logging usage with proper levels and contextual information.\n\nLogging must use appropriate levels and include relevant contextual information.\n\n## Error Handling Patterns\n\n### Custom Exceptions\n```python\nclass StockDataException(Exception):\n    \"\"\"Base exception for stock data operations.\"\"\"\n    pass\n\nclass StockDataRetrievalError(StockDataException):\n    \"\"\"Raised when stock data retrieval fails.\"\"\"\n    pass\n\nclass StockDataParsingError(StockDataException):\n    \"\"\"Raised when stock data parsing fails.\"\"\"\n    pass\n\nclass StockDataLoadingError(StockDataException):\n    \"\"\"Raised when stock data loading fails.\"\"\"\n    pass\n```\n\nThis pattern will inform custom exception hierarchy for domain-specific error handling.\n\nCustom exceptions must inherit from appropriate base classes and provide meaningful error messages.\n\n### Error Recovery\n```python\nasync def process_with_retry(operation, max_retries: int = 3):\n    \"\"\"Execute operation with retry logic.\"\"\"\n    for attempt in range(max_retries):\n        try:\n            return await operation()\n        except Exception as e:\n            if attempt == max_retries - 1:\n                raise\n            logger.warning(f\"Attempt {attempt + 1} failed, retrying: {e}\")\n            await asyncio.sleep(2 ** attempt)  # Exponential backoff\n```\n\nThis pattern will inform retry logic implementation with exponential backoff.\n\nRetry logic must include proper backoff strategies and maximum retry limits.\n\n## Performance Patterns\n\n### Async Operations\n```python\nasync def process_files_concurrently(files: list[str], max_concurrent: int = 5):\n    \"\"\"Process multiple files concurrently with semaphore limiting.\"\"\"\n    semaphore = asyncio.Semaphore(max_concurrent)\n\n    async def process_single_file(file_path: str):\n        async with semaphore:\n            return await process_file(file_path)\n\n    tasks = [process_single_file(file) for file in files]\n    results = await asyncio.gather(*tasks, return_exceptions=True)\n\n    return results\n```\n\nThis pattern will inform concurrent processing implementation with proper resource limiting.\n\nConcurrent operations must include proper resource limiting and error handling.\n\n### Database Optimization\n```python\nasync def batch_insert_data(session: AsyncSession, data: list[dict]):\n    \"\"\"Insert data in batches for better performance.\"\"\"\n    batch_size = 1000\n\n    for i in range(0, len(data), batch_size):\n        batch = data[i:i + batch_size]\n        models = [StockData(**item) for item in batch]\n        session.add_all(models)\n        await session.commit()\n```\n\nThis pattern will inform batch database operations for improved performance.\n\nBatch operations must include proper batch sizes and transaction management.\n\n## Security Patterns\n\n### Input Validation\n```python\nfrom pydantic import BaseModel, validator\nimport re\n\nclass StockSymbolRequest(BaseModel):\n    symbol: str\n\n    @validator('symbol')\n    def validate_symbol(cls, v):\n        if not re.match(r'^[A-Z]{1,10}$', v):\n            raise ValueError('Symbol must be 1-10 uppercase letters')\n        return v.upper()\n```\n\nThis pattern will inform input validation implementation with Pydantic validators.\n\nInput validation must include proper regex patterns and meaningful error messages.\n\n### Authentication and Authorization\n```python\nfrom fastapi import Depends, HTTPException, status\nfrom fastapi.security import HTTPBearer\n\nsecurity = HTTPBearer()\n\nasync def get_current_user(token: str = Depends(security)):\n    \"\"\"Validate user authentication.\"\"\"\n    try:\n        # Validate token and return user\n        user = validate_token(token.credentials)\n        return user\n    except Exception:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Invalid authentication credentials\"\n        )\n\nasync def require_admin(user = Depends(get_current_user)):\n    \"\"\"Require admin privileges.\"\"\"\n    if not user.is_admin:\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=\"Admin privileges required\"\n        )\n    return user\n```\n\nThis pattern will inform authentication and authorization implementation.\n\nAuthentication must include proper token validation and role-based access control.\n\n## Implementation Guidelines\n\n### For AI Assistants\n1. **Reference this document** for all pattern implementations\n2. **Follow established conventions** for consistency\n3. **Apply patterns consistently** across similar functionality\n4. **Validate implementations** against pattern requirements\n5. **Update patterns** when new best practices emerge\n\n### For Human Developers\n1. **Follow these patterns** for all new implementations\n2. **Reference patterns** when reviewing code\n3. **Suggest pattern improvements** when better approaches are found\n4. **Maintain consistency** with established conventions\n5. **Document deviations** when patterns cannot be followed\n\n## Quality Assurance\n\n### Pattern Compliance\n- All implementations must follow established patterns\n- Deviations must be justified and documented\n- Pattern updates must be reviewed and approved\n- Consistency must be maintained across the codebase\n\n### Code Quality\n- All code must follow project style guidelines\n- Error handling must be comprehensive\n- Performance must meet established requirements\n- Security must follow established patterns\n\n### Documentation\n- All patterns must be documented with examples\n- Implementation guidelines must be clear\n- Validation rules must be specific\n- Updates must be communicated to the team\n\n---\n\n**AI Quality Checklist**: Before implementing patterns, ensure:\n- [x] Pattern is appropriate for the specific use case\n- [x] Implementation follows established conventions\n- [x] Error handling is comprehensive\n- [x] Performance requirements are met\n- [x] Security patterns are followed\n- [x] Code quality standards are maintained\n- [x] Documentation is updated\n- [x] Tests are implemented\n"
  },
  "cross_references": [],
  "code_generation_hints": [
    {
      "context": "general",
      "hint": "This pattern will inform the structure of all new modules and classes in the project.",
      "validation": ""
    },
    {
      "context": "general",
      "hint": "This pattern will inform how to handle configuration loading and usage throughout the project.",
      "validation": ""
    },
    {
      "context": "error handling",
      "hint": "This pattern will inform error handling implementation for all async operations and external calls.",
      "validation": ""
    },
    {
      "context": "general",
      "hint": "This pattern will inform database session management for all database operations.",
      "validation": ""
    },
    {
      "context": "general",
      "hint": "This pattern will inform SQLAlchemy model definitions with proper indexing and constraints.",
      "validation": ""
    },
    {
      "context": "general",
      "hint": "This pattern will inform Alembic migration file structure and database schema changes.",
      "validation": ""
    },
    {
      "context": "error handling",
      "hint": "This pattern will inform FastAPI endpoint implementation with proper error handling and response models.",
      "validation": ""
    },
    {
      "context": "general",
      "hint": "This pattern will inform Pydantic schema definitions with proper field validation and descriptions.",
      "validation": ""
    },
    {
      "context": "error handling",
      "hint": "This pattern will inform background task implementation with proper error handling and logging.",
      "validation": ""
    },
    {
      "context": "general",
      "hint": "This pattern will inform how to schedule and configure background tasks.",
      "validation": ""
    },
    {
      "context": "general",
      "hint": "This pattern will inform file organization and module structure for new features.",
      "validation": ""
    },
    {
      "context": "error handling",
      "hint": "This pattern will inform file processing workflow implementation with proper error handling.",
      "validation": ""
    },
    {
      "context": "testing",
      "hint": "This pattern will inform unit test structure with proper fixtures and async testing.",
      "validation": ""
    },
    {
      "context": "testing",
      "hint": "This pattern will inform integration test structure with database setup and teardown.",
      "validation": ""
    },
    {
      "context": "general",
      "hint": "This pattern will inform configuration class structure with environment variable support.",
      "validation": ""
    },
    {
      "context": "general",
      "hint": "This pattern will inform configuration loading and usage throughout modules.",
      "validation": ""
    },
    {
      "context": "general",
      "hint": "This pattern will inform structured logging implementation with JSON formatting.",
      "validation": ""
    },
    {
      "context": "general",
      "hint": "This pattern will inform logging usage with proper levels and contextual information.",
      "validation": ""
    },
    {
      "context": "error handling",
      "hint": "This pattern will inform custom exception hierarchy for domain-specific error handling.",
      "validation": ""
    },
    {
      "context": "general",
      "hint": "This pattern will inform retry logic implementation with exponential backoff.",
      "validation": ""
    },
    {
      "context": "general",
      "hint": "This pattern will inform concurrent processing implementation with proper resource limiting.",
      "validation": ""
    },
    {
      "context": "general",
      "hint": "This pattern will inform batch database operations for improved performance.",
      "validation": ""
    },
    {
      "context": "general",
      "hint": "This pattern will inform input validation implementation with Pydantic validators.",
      "validation": ""
    },
    {
      "context": "general",
      "hint": "This pattern will inform authentication and authorization implementation.",
      "validation": ""
    }
  ],
  "validation_rules": [
    "All code must follow project style guidelines",
    "Consistency must be maintained across the codebase",
    "Retry logic must include proper backoff strategies and maximum retry limits",
    "Pattern application must be consistent across the codebase",
    "Security must follow established patterns",
    "Logging must use appropriate levels and include relevant contextual information",
    "Pattern updates must be reviewed and approved",
    "Error handling must include specific exception types, proper logging, and user-friendly error messages",
    "File organization must follow this structure for consistency and maintainability",
    "Z]{1,10}$', v):\n            raise ValueError('Symbol must be 1-10 uppercase letters')\n        return v.upper()\n```",
    "Validation rules must be specific",
    "Authentication must include proper token validation and role-based access control",
    "Implementation must align with project architecture",
    "Batch operations must include proper batch sizes and transaction management",
    "Integration tests must include proper database setup, test data creation, and cleanup",
    "Updates must be communicated to the team",
    "File processing must include proper error handling, logging, and file movement operations",
    "Implementation guidelines must be clear",
    "Code examples must follow established project conventions",
    "Configuration must be loaded once at module level and reused, not loaded multiple times",
    "Task scheduling must include proper intervals and error handling",
    "Background tasks must handle errors gracefully and not re-raise exceptions",
    "Models must include proper indexes, constraints, and follow naming conventions",
    "Error handling must be comprehensive",
    "Tests must include proper fixtures, async testing, and comprehensive error case coverage",
    "raise - background tasks should handle errors gracefully\n```",
    "Database sessions must use proper async context managers and transaction management",
    "Error handling must follow established patterns",
    "Migrations must include proper upgrade and downgrade functions with descriptive docstrings",
    "Custom exceptions must inherit from appropriate base classes and provide meaningful error messages",
    "Endpoints must include proper error handling, logging, and response model validation",
    "Module structure must follow this exact organization with proper imports, configuration, and error handling",
    "Configuration must use Pydantic for validation and support environment variable overrides",
    "All patterns must be documented with examples",
    "Input validation must include proper regex patterns and meaningful error messages",
    "Deviations must be justified and documented",
    "Logging must include structured format with timestamps, levels, and contextual information",
    "All patterns must reference actual project structure and files",
    "Performance must meet established requirements",
    "Schemas must include proper field types, descriptions, and configuration for ORM compatibility",
    "All implementations must follow established patterns",
    "Concurrent operations must include proper resource limiting and error handling"
  ],
  "optimization": {
    "version": "1.0",
    "optimized_at": "2025-06-18T19:19:47.763299",
    "improvements": [
      "fixed_file_references",
      "extracted_ai_metadata",
      "structured_cross_references",
      "extracted_code_hints",
      "structured_validation_rules"
    ],
    "literal_strings_cleaned": true,
    "cleaned_at": "2025-06-18T19:30:00.000000"
  }
}